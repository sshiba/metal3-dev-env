azureuser@sidneyshiba-capm3-vm:~/projects/metal3-dev-env$ make
./01_prepare_host.sh
+ source lib/logging.sh
+++ dirname ./01_prepare_host.sh
++ LOGDIR=./logs
++ '[' '!' -d ./logs ']'
++ mkdir -p ./logs
+++ basename ./01_prepare_host.sh .sh
+++ date +%F-%H%M%S
++ LOGFILE=./logs/01_prepare_host-2021-12-21-204135.log
++ echo 'Logging to ./logs/01_prepare_host-2021-12-21-204135.log'
Logging to ./logs/01_prepare_host-2021-12-21-204135.log
++ exec
+++ tee ./logs/01_prepare_host-2021-12-21-204135.log
+ source lib/common.sh
++ [[ :/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin
+++ go env
++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/azureuser/.cache/go-build"
GOENV="/home/azureuser/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/azureuser/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/azureuser/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build2393501010=/tmp/go-build -gno-record-gcc-switches"'
+++ GO111MODULE=
+++ GOARCH=amd64
+++ GOBIN=
+++ GOCACHE=/home/azureuser/.cache/go-build
+++ GOENV=/home/azureuser/.config/go/env
+++ GOEXE=
+++ GOFLAGS=
+++ GOHOSTARCH=amd64
+++ GOHOSTOS=linux
+++ GOINSECURE=
+++ GOMODCACHE=/home/azureuser/go/pkg/mod
+++ GONOPROXY=
+++ GONOSUMDB=
+++ GOOS=linux
+++ GOPATH=/home/azureuser/go
+++ GOPRIVATE=
+++ GOPROXY=https://proxy.golang.org,direct
+++ GOROOT=/usr/local/go
+++ GOSUMDB=sum.golang.org
+++ GOTMPDIR=
+++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
+++ GOVCS=
+++ GOVERSION=go1.16.7
+++ GCCGO=gccgo
+++ AR=ar
+++ CC=gcc
+++ CXX=g++
+++ CGO_ENABLED=1
+++ GOMOD=/dev/null
+++ CGO_CFLAGS='-g -O2'
+++ CGO_CPPFLAGS=
+++ CGO_CXXFLAGS='-g -O2'
+++ CGO_FFLAGS='-g -O2'
+++ CGO_LDFLAGS='-g -O2'
+++ PKG_CONFIG=pkg-config
+++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build2393501010=/tmp/go-build -gno-record-gcc-switches'
++ export GOPATH
++++ dirname lib/common.sh
+++ cd lib/..
+++ pwd
++ SCRIPTDIR=/home/azureuser/projects/metal3-dev-env
+++ whoami
++ USER=azureuser
++ export USER=azureuser
++ USER=azureuser
++ '[' -z '' ']'
++ '[' '!' -f /home/azureuser/projects/metal3-dev-env/config_azureuser.sh ']'
++ CONFIG=/home/azureuser/projects/metal3-dev-env/config_azureuser.sh
++ source /home/azureuser/projects/metal3-dev-env/config_azureuser.sh
+++ export KUBECONFIG=/home/azureuser/.kube/config
+++ KUBECONFIG=/home/azureuser/.kube/config
+++ export K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
+++ K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
+++ export IMAGE_OS=Ubuntu
+++ IMAGE_OS=Ubuntu
+++ export EPHEMERAL_CLUSTER=kind
+++ EPHEMERAL_CLUSTER=kind
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ export CAPM3_VERSION=v1beta1
+++ CAPM3_VERSION=v1beta1
+++ export CAPI_VERSION=v1beta1
+++ CAPI_VERSION=v1beta1
+++ export NUM_NODES=5
+++ NUM_NODES=5
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=2
+++ NUM_OF_WORKER_REPLICAS=2
+++ export KUBERNETES_VERSION=v1.21.1
+++ KUBERNETES_VERSION=v1.21.1
+++ export UPGRADED_K8S_VERSION=v1.22.2
+++ UPGRADED_K8S_VERSION=v1.22.2
++ export MARIADB_HOST=mariaDB
++ MARIADB_HOST=mariaDB
++ export MARIADB_HOST_IP=127.0.0.1
++ MARIADB_HOST_IP=127.0.0.1
++ ADDN_DNS=
++ EXT_IF=
++ PRO_IF=
++ MANAGE_BR_BRIDGE=y
++ MANAGE_PRO_BRIDGE=y
++ MANAGE_INT_BRIDGE=y
++ INT_IF=
++ ROOT_DISK_NAME=/dev/sda
++ NODE_HOSTNAME_FORMAT=node-%d
++ source /etc/os-release
+++ NAME=Ubuntu
+++ VERSION='20.04.3 LTS (Focal Fossa)'
+++ ID=ubuntu
+++ ID_LIKE=debian
+++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
+++ VERSION_ID=20.04
+++ HOME_URL=https://www.ubuntu.com/
+++ SUPPORT_URL=https://help.ubuntu.com/
+++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
+++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
+++ VERSION_CODENAME=focal
+++ UBUNTU_CODENAME=focal
++ export DISTRO=ubuntu20
++ DISTRO=ubuntu20
++ export OS=ubuntu
++ OS=ubuntu
++ export OS_VERSION_ID=20.04
++ OS_VERSION_ID=20.04
++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
++ export SUPPORTED_DISTROS
++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
++ [[ ubuntu == ubuntu ]]
++ export CONTAINER_RUNTIME=docker
++ CONTAINER_RUNTIME=docker
++ [[ docker == \p\o\d\m\a\n ]]
++ export POD_NAME=
++ POD_NAME=
++ export POD_NAME_INFRA=
++ POD_NAME_INFRA=
++ export SSH_KEY=/home/azureuser/.ssh/id_rsa
++ SSH_KEY=/home/azureuser/.ssh/id_rsa
++ export SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
++ SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
++ '[' '!' -f /home/azureuser/.ssh/id_rsa ']'
+++ dirname /home/azureuser/.ssh/id_rsa
++ mkdir -p /home/azureuser/.ssh
++ ssh-keygen -f /home/azureuser/.ssh/id_rsa -P ''
Generating public/private rsa key pair.
Your identification has been saved in /home/azureuser/.ssh/id_rsa
Your public key has been saved in /home/azureuser/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:kEE3FOAVEtiy+ejd5ABHmhrLhKCCMJteErqlTVcsQIA azureuser@sidneyshiba-capm3-vm
The key's randomart image is:
+---[RSA 3072]----+
|ooo. =*+Bo       |
|E.  +.=* .       |
|*=.  X+          |
|O.=.B ..         |
|oOo= =  S        |
|o.= . o .        |
|   . . =         |
|    . . o        |
|                 |
+----[SHA256]-----+
++ FILESYSTEM=/
++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
++ export CAPM3_VERSION=v1beta1
++ CAPM3_VERSION=v1beta1
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ '[' v1beta1 == v1beta1 ']'
++ export CAPI_VERSION=v1beta1
++ CAPI_VERSION=v1beta1
++ export M3PATH=/home/azureuser/go/src/github.com/metal3-io
++ M3PATH=/home/azureuser/go/src/github.com/metal3-io
++ export BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
++ BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
++ export RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ export CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ export IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
++ IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
++ export IPAM_BASE_URL=metal3-io/ip-address-manager
++ IPAM_BASE_URL=metal3-io/ip-address-manager
++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ '[' v1beta1 == v1alpha3 ']'
++ '[' v1beta1 == v1alpha4 ']'
++ IPAMBRANCH=main
++ IPA_DOWNLOAD_ENABLED=true
++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ CAPM3BRANCH=main
++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
++ BMOBRANCH=master
++ FORCE_REPO_UPDATE=true
++ BMOCOMMIT=HEAD
++ BMO_RUN_LOCAL=false
++ CAPM3_RUN_LOCAL=false
++ WORKING_DIR=/opt/metal3-dev-env
++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
++ NODES_PLATFORM=libvirt
++ export NAMESPACE=metal3
++ NAMESPACE=metal3
++ export NUM_NODES=5
++ NUM_NODES=5
++ export NUM_OF_MASTER_REPLICAS=3
++ NUM_OF_MASTER_REPLICAS=3
++ export NUM_OF_WORKER_REPLICAS=2
++ NUM_OF_WORKER_REPLICAS=2
++ export VM_EXTRADISKS=false
++ VM_EXTRADISKS=false
++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
++ VM_EXTRADISKS_FILE_SYSTEM=ext4
++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ export NODE_DRAIN_TIMEOUT=0s
++ NODE_DRAIN_TIMEOUT=0s
++ export MAX_SURGE_VALUE=1
++ MAX_SURGE_VALUE=1
++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ export CONTAINER_REGISTRY=quay.io
++ CONTAINER_REGISTRY=quay.io
++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
++ VBMC_IMAGE=quay.io/metal3-io/vbmc
++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ export IRONIC_TLS_SETUP=true
++ IRONIC_TLS_SETUP=true
++ export IRONIC_BASIC_AUTH=true
++ IRONIC_BASIC_AUTH=true
++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
++ IRONIC_IMAGE=quay.io/metal3-io/ironic
++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ '[' v1beta1 == v1alpha4 ']'
++ export IRONIC_NAMESPACE=baremetal-operator-system
++ IRONIC_NAMESPACE=baremetal-operator-system
++ export NAMEPREFIX=baremetal-operator
++ NAMEPREFIX=baremetal-operator
++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ export OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
++ OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ export DEFAULT_HOSTS_MEMORY=4096
++ DEFAULT_HOSTS_MEMORY=4096
++ export CLUSTER_NAME=test1
++ CLUSTER_NAME=test1
++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
++ CLUSTER_APIENDPOINT_IP=192.168.111.249
++ export KUBERNETES_VERSION=v1.21.1
++ KUBERNETES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_VERSION=v1.21.1
++ KUBERNETES_BINARIES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ '[' docker == docker ']'
++ export EPHEMERAL_CLUSTER=kind
++ EPHEMERAL_CLUSTER=kind
++ export KUSTOMIZE_VERSION=v4.1.3
++ KUSTOMIZE_VERSION=v4.1.3
++ export KIND_VERSION=v0.11.1
++ KIND_VERSION=v0.11.1
++ '[' v1.21.1 == v1.21.2 ']'
++ export KIND_NODE_IMAGE_VERSION=v1.22.2
++ KIND_NODE_IMAGE_VERSION=v1.22.2
++ export MINIKUBE_VERSION=v1.23.2
++ MINIKUBE_VERSION=v1.23.2
++ export ANSIBLE_VERSION=4.8.0
++ ANSIBLE_VERSION=4.8.0
++ SKIP_RETRIES=false
++ TEST_TIME_INTERVAL=10
++ TEST_MAX_TIME=240
++ FAILS=0
++ RESULT_STR=
++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ '[' 5 -lt 5 ']'
++ export LIBVIRT_DEFAULT_URI=qemu:///system
++ LIBVIRT_DEFAULT_URI=qemu:///system
++ '[' azureuser '!=' root ']'
++ '[' /run/user/1000 == /run/user/0 ']'
++ sudo -n uptime
++ export USE_FIREWALLD=False
++ USE_FIREWALLD=False
++ [[ ubuntu20 == \r\h\e\l\8 ]]
++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
+++ df / --output=fstype
+++ tail -n 1
++ FSTYPE=ext4
++ case ${FSTYPE} in
++ '[' '!' -d /opt/metal3-dev-env ']'
++ echo 'Creating Working Dir'
Creating Working Dir
++ sudo mkdir /opt/metal3-dev-env
++ sudo chown azureuser:azureuser /opt/metal3-dev-env
++ chmod 755 /opt/metal3-dev-env
++ id -u
+ [[ 1000 == 0 ]]
+ [[ ubuntu == ubuntu ]]
+ sudo apt-get update
Hit:1 http://azure.archive.ubuntu.com/ubuntu focal InRelease
Get:2 http://azure.archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]
Get:3 http://azure.archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]
Get:4 https://packages.microsoft.com/repos/azure-cli focal InRelease [10.4 kB]
Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]
Get:6 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [1400 kB]
Get:7 http://azure.archive.ubuntu.com/ubuntu focal-updates/main Translation-en [283 kB]
Get:8 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 c-n-f Metadata [14.7 kB]
Get:9 http://azure.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [616 kB]
Get:10 http://azure.archive.ubuntu.com/ubuntu focal-updates/restricted Translation-en [88.1 kB]
Get:11 http://azure.archive.ubuntu.com/ubuntu focal-updates/restricted amd64 c-n-f Metadata [528 B]
Get:12 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [884 kB]
Get:13 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe Translation-en [193 kB]
Get:14 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe amd64 c-n-f Metadata [19.9 kB]
Get:15 http://azure.archive.ubuntu.com/ubuntu focal-backports/main amd64 Packages [42.0 kB]
Get:16 http://azure.archive.ubuntu.com/ubuntu focal-backports/main Translation-en [10.0 kB]
Get:17 http://azure.archive.ubuntu.com/ubuntu focal-backports/main amd64 c-n-f Metadata [864 B]
Get:18 http://azure.archive.ubuntu.com/ubuntu focal-backports/universe amd64 Packages [18.9 kB]
Get:19 http://azure.archive.ubuntu.com/ubuntu focal-backports/universe Translation-en [7492 B]
Get:20 http://azure.archive.ubuntu.com/ubuntu focal-backports/universe amd64 c-n-f Metadata [636 B]
Get:21 https://packages.microsoft.com/repos/azure-cli focal/main amd64 Packages [7761 B]
Get:22 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [1069 kB]
Get:23 http://security.ubuntu.com/ubuntu focal-security/main Translation-en [197 kB]
Get:24 http://security.ubuntu.com/ubuntu focal-security/main amd64 c-n-f Metadata [9096 B]
Get:25 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [566 kB]
Get:26 http://security.ubuntu.com/ubuntu focal-security/restricted Translation-en [80.9 kB]
Get:27 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 c-n-f Metadata [528 B]
Get:28 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [668 kB]
Get:29 http://security.ubuntu.com/ubuntu focal-security/universe Translation-en [112 kB]
Get:30 http://security.ubuntu.com/ubuntu focal-security/universe amd64 c-n-f Metadata [13.0 kB]
Fetched 6648 kB in 2s (4421 kB/s)
Reading package lists...
+ sudo apt -y install python3-pip

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
Building dependency tree...
Reading state information...
The following additional packages will be installed:
  libexpat1-dev libpython3-dev libpython3.8 libpython3.8-dev
  libpython3.8-minimal libpython3.8-stdlib python-pip-whl python3-dev
  python3-wheel python3.8 python3.8-dev python3.8-minimal zlib1g-dev
Suggested packages:
  python3.8-venv python3.8-doc binfmt-support
The following NEW packages will be installed:
  libexpat1-dev libpython3-dev libpython3.8-dev python-pip-whl python3-dev
  python3-pip python3-wheel python3.8-dev zlib1g-dev
The following packages will be upgraded:
  libpython3.8 libpython3.8-minimal libpython3.8-stdlib python3.8
  python3.8-minimal
5 upgraded, 9 newly installed, 0 to remove and 23 not upgraded.
Need to get 13.1 MB of archives.
After this operation, 25.6 MB of additional disk space will be used.
Get:1 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8 amd64 3.8.10-0ubuntu1~20.04.2 [1625 kB]
Get:2 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8 amd64 3.8.10-0ubuntu1~20.04.2 [387 kB]
Get:3 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-stdlib amd64 3.8.10-0ubuntu1~20.04.2 [1675 kB]
Get:4 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8-minimal amd64 3.8.10-0ubuntu1~20.04.2 [1900 kB]
Get:5 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-minimal amd64 3.8.10-0ubuntu1~20.04.2 [717 kB]
Get:6 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libexpat1-dev amd64 2.2.9-1build1 [116 kB]
Get:7 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 libpython3.8-dev amd64 3.8.10-0ubuntu1~20.04.2 [3950 kB]
Get:8 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libpython3-dev amd64 3.8.2-0ubuntu2 [7236 B]
Get:9 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe amd64 python-pip-whl all 20.0.2-5ubuntu1.6 [1805 kB]
Get:10 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 zlib1g-dev amd64 1:1.2.11.dfsg-2ubuntu1.2 [155 kB]
Get:11 http://azure.archive.ubuntu.com/ubuntu focal-updates/main amd64 python3.8-dev amd64 3.8.10-0ubuntu1~20.04.2 [510 kB]
Get:12 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 python3-dev amd64 3.8.2-0ubuntu2 [1212 B]
Get:13 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 python3-wheel all 0.34.2-1 [23.8 kB]
Get:14 http://azure.archive.ubuntu.com/ubuntu focal-updates/universe amd64 python3-pip all 20.0.2-5ubuntu1.6 [231 kB]
Fetched 13.1 MB in 0s (57.9 MB/s)
(Reading database ... 121516 files and directories currently installed.)
Preparing to unpack .../00-libpython3.8_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking libpython3.8:amd64 (3.8.10-0ubuntu1~20.04.2) over (3.8.10-0ubuntu1~20.04.1) ...
Preparing to unpack .../01-python3.8_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking python3.8 (3.8.10-0ubuntu1~20.04.2) over (3.8.10-0ubuntu1~20.04.1) ...
Preparing to unpack .../02-libpython3.8-stdlib_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking libpython3.8-stdlib:amd64 (3.8.10-0ubuntu1~20.04.2) over (3.8.10-0ubuntu1~20.04.1) ...
Preparing to unpack .../03-python3.8-minimal_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking python3.8-minimal (3.8.10-0ubuntu1~20.04.2) over (3.8.10-0ubuntu1~20.04.1) ...
Preparing to unpack .../04-libpython3.8-minimal_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking libpython3.8-minimal:amd64 (3.8.10-0ubuntu1~20.04.2) over (3.8.10-0ubuntu1~20.04.1) ...
Selecting previously unselected package libexpat1-dev:amd64.
Preparing to unpack .../05-libexpat1-dev_2.2.9-1build1_amd64.deb ...
Unpacking libexpat1-dev:amd64 (2.2.9-1build1) ...
Selecting previously unselected package libpython3.8-dev:amd64.
Preparing to unpack .../06-libpython3.8-dev_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking libpython3.8-dev:amd64 (3.8.10-0ubuntu1~20.04.2) ...
Selecting previously unselected package libpython3-dev:amd64.
Preparing to unpack .../07-libpython3-dev_3.8.2-0ubuntu2_amd64.deb ...
Unpacking libpython3-dev:amd64 (3.8.2-0ubuntu2) ...
Selecting previously unselected package python-pip-whl.
Preparing to unpack .../08-python-pip-whl_20.0.2-5ubuntu1.6_all.deb ...
Unpacking python-pip-whl (20.0.2-5ubuntu1.6) ...
Selecting previously unselected package zlib1g-dev:amd64.
Preparing to unpack .../09-zlib1g-dev_1%3a1.2.11.dfsg-2ubuntu1.2_amd64.deb ...
Unpacking zlib1g-dev:amd64 (1:1.2.11.dfsg-2ubuntu1.2) ...
Selecting previously unselected package python3.8-dev.
Preparing to unpack .../10-python3.8-dev_3.8.10-0ubuntu1~20.04.2_amd64.deb ...
Unpacking python3.8-dev (3.8.10-0ubuntu1~20.04.2) ...
Selecting previously unselected package python3-dev.
Preparing to unpack .../11-python3-dev_3.8.2-0ubuntu2_amd64.deb ...
Unpacking python3-dev (3.8.2-0ubuntu2) ...
Selecting previously unselected package python3-wheel.
Preparing to unpack .../12-python3-wheel_0.34.2-1_all.deb ...
Unpacking python3-wheel (0.34.2-1) ...
Selecting previously unselected package python3-pip.
Preparing to unpack .../13-python3-pip_20.0.2-5ubuntu1.6_all.deb ...
Unpacking python3-pip (20.0.2-5ubuntu1.6) ...
Setting up libpython3.8-minimal:amd64 (3.8.10-0ubuntu1~20.04.2) ...
Setting up python3-wheel (0.34.2-1) ...
Setting up libexpat1-dev:amd64 (2.2.9-1build1) ...
Setting up zlib1g-dev:amd64 (1:1.2.11.dfsg-2ubuntu1.2) ...
Setting up python3.8-minimal (3.8.10-0ubuntu1~20.04.2) ...
Setting up python-pip-whl (20.0.2-5ubuntu1.6) ...
Setting up libpython3.8-stdlib:amd64 (3.8.10-0ubuntu1~20.04.2) ...
Setting up python3.8 (3.8.10-0ubuntu1~20.04.2) ...
Setting up libpython3.8:amd64 (3.8.10-0ubuntu1~20.04.2) ...
Setting up python3-pip (20.0.2-5ubuntu1.6) ...
Setting up libpython3.8-dev:amd64 (3.8.10-0ubuntu1~20.04.2) ...
Setting up python3.8-dev (3.8.10-0ubuntu1~20.04.2) ...
Setting up libpython3-dev:amd64 (3.8.2-0ubuntu2) ...
Setting up python3-dev (3.8.2-0ubuntu2) ...
Processing triggers for libc-bin (2.31-0ubuntu9.2) ...
Processing triggers for man-db (2.9.1-1) ...
Processing triggers for mime-support (3.64ubuntu1) ...
+ [[ ubuntu20 == \u\b\u\n\t\u\1\8 ]]
+ [[ ubuntu20 == \u\b\u\n\t\u\2\0 ]]
+ sudo update-alternatives --install /usr/bin/python python /usr/bin/python3.8 1
update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python (python) in auto mode
+ sudo pip3 install ansible==4.8.0
Collecting ansible==4.8.0
  Downloading ansible-4.8.0.tar.gz (36.1 MB)
Collecting ansible-core<2.12,>=2.11.6
  Downloading ansible-core-2.11.7.tar.gz (7.1 MB)
Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from ansible-core<2.12,>=2.11.6->ansible==4.8.0) (5.3.1)
Requirement already satisfied: cryptography in /usr/lib/python3/dist-packages (from ansible-core<2.12,>=2.11.6->ansible==4.8.0) (2.8)
Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from ansible-core<2.12,>=2.11.6->ansible==4.8.0) (2.10.1)
Collecting packaging
  Downloading packaging-21.3-py3-none-any.whl (40 kB)
Collecting resolvelib<0.6.0,>=0.5.3
  Downloading resolvelib-0.5.4-py2.py3-none-any.whl (12 kB)
Collecting pyparsing!=3.0.5,>=2.0.2
  Downloading pyparsing-3.0.6-py3-none-any.whl (97 kB)
Building wheels for collected packages: ansible, ansible-core
  Building wheel for ansible (setup.py): started
  Building wheel for ansible (setup.py): finished with status 'done'
  Created wheel for ansible: filename=ansible-4.8.0-py3-none-any.whl size=59454442 sha256=fe37eeb4cc1b2c72117de06dcfdefc3f0fb2c6352717c942addaac06eefe3a2d
  Stored in directory: /root/.cache/pip/wheels/3d/db/ed/6c8b0ffe3008371db04f2098374dfa21b22823bd7bf30fe5b1
  Building wheel for ansible-core (setup.py): started
  Building wheel for ansible-core (setup.py): finished with status 'done'
  Created wheel for ansible-core: filename=ansible_core-2.11.7-py3-none-any.whl size=1959478 sha256=2cc193de60937071472ac0b979b90b56033217eae01f0bec254761e034f62861
  Stored in directory: /root/.cache/pip/wheels/9c/a0/c8/9b4a0b374e96e5beb696b3224cf08ceceedb067df0b82d6fce
Successfully built ansible ansible-core
Installing collected packages: pyparsing, packaging, resolvelib, ansible-core, ansible
Successfully installed ansible-4.8.0 ansible-core-2.11.7 packaging-21.3 pyparsing-3.0.6 resolvelib-0.5.4
+ source lib/network.sh
++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ export POD_CIDR=192.168.0.0/18
++ POD_CIDR=192.168.0.0/18
++ PROVISIONING_IPV6=false
++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
++ [[ false == \t\r\u\e ]]
++ export BOOT_MODE=legacy
++ BOOT_MODE=legacy
++ export PROVISIONING_NETWORK=172.22.0.0/24
++ PROVISIONING_NETWORK=172.22.0.0/24
++ [[ legacy == \l\e\g\a\c\y ]]
++ export LIBVIRT_FIRMWARE=bios
++ LIBVIRT_FIRMWARE=bios
++ export LIBVIRT_SECURE_BOOT=false
++ LIBVIRT_SECURE_BOOT=false
++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
++ resultvar=PROVISIONING_CIDR
++ network=172.22.0.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
++ result=24
++ eval PROVISIONING_CIDR=24
+++ PROVISIONING_CIDR=24
++ export PROVISIONING_CIDR
++ export PROVISIONING_CIDR
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").netmask)'
++ export PROVISIONING_NETMASK=255.255.255.0
++ PROVISIONING_NETMASK=255.255.255.0
++ network_address PROVISIONING_IP 172.22.0.0/24 1
++ resultvar=PROVISIONING_IP
++ network=172.22.0.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
++ result=172.22.0.1
++ eval PROVISIONING_IP=172.22.0.1
+++ PROVISIONING_IP=172.22.0.1
++ export PROVISIONING_IP
++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
++ resultvar=CLUSTER_PROVISIONING_IP
++ network=172.22.0.0/24
++ record=2
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
++ result=172.22.0.2
++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
+++ CLUSTER_PROVISIONING_IP=172.22.0.2
++ export CLUSTER_PROVISIONING_IP
++ export PROVISIONING_IP
++ export CLUSTER_PROVISIONING_IP
++ [[ 172.22.0.1 == *\:* ]]
++ export PROVISIONING_URL_HOST=172.22.0.1
++ PROVISIONING_URL_HOST=172.22.0.1
++ export CLUSTER_URL_HOST=172.22.0.2
++ CLUSTER_URL_HOST=172.22.0.2
++ [[ 192.168.111.249 == *\:* ]]
++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ export CLUSTER_APIENDPOINT_PORT=6443
++ CLUSTER_APIENDPOINT_PORT=6443
++ network_address dhcp_range_start 172.22.0.0/24 10
++ resultvar=dhcp_range_start
++ network=172.22.0.0/24
++ record=10
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
++ result=172.22.0.10
++ eval dhcp_range_start=172.22.0.10
+++ dhcp_range_start=172.22.0.10
++ export dhcp_range_start
++ network_address dhcp_range_end 172.22.0.0/24 100
++ resultvar=dhcp_range_end
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval dhcp_range_end=172.22.0.100
+++ dhcp_range_end=172.22.0.100
++ export dhcp_range_end
++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
++ resultvar=PROVISIONING_POOL_RANGE_START
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ PROVISIONING_POOL_RANGE_START=172.22.0.100
++ export PROVISIONING_POOL_RANGE_START
++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
++ resultvar=PROVISIONING_POOL_RANGE_END
++ network=172.22.0.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
++ result=172.22.0.200
++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ PROVISIONING_POOL_RANGE_END=172.22.0.200
++ export PROVISIONING_POOL_RANGE_END
++ export PROVISIONING_POOL_RANGE_START
++ export PROVISIONING_POOL_RANGE_END
++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ EXTERNAL_SUBNET=
++ [[ -n '' ]]
++ export IP_STACK=v4
++ IP_STACK=v4
++ [[ v4 == \v\4 ]]
++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
++ EXTERNAL_SUBNET_V4=192.168.111.0/24
++ export EXTERNAL_SUBNET_V6=
++ EXTERNAL_SUBNET_V6=
++ [[ kind == \m\i\n\i\k\u\b\e ]]
++ [[ -n 192.168.111.0/24 ]]
++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
++ network=192.168.111.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
++ result=24
++ eval EXTERNAL_SUBNET_V4_PREFIX=24
+++ EXTERNAL_SUBNET_V4_PREFIX=24
++ export EXTERNAL_SUBNET_V4_PREFIX
++ export EXTERNAL_SUBNET_V4_PREFIX
++ [[ -z '' ]]
++ network_address EXTERNAL_SUBNET_V4_HOST 192.168.111.0/24 1
++ resultvar=EXTERNAL_SUBNET_V4_HOST
++ network=192.168.111.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 1 - 1, None)))'
++ result=192.168.111.1
++ eval EXTERNAL_SUBNET_V4_HOST=192.168.111.1
+++ EXTERNAL_SUBNET_V4_HOST=192.168.111.1
++ export EXTERNAL_SUBNET_V4_HOST
++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
++ resultvar=VIRSH_DHCP_V4_START
++ network=192.168.111.0/24
++ record=20
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
++ result=192.168.111.20
++ eval VIRSH_DHCP_V4_START=192.168.111.20
+++ VIRSH_DHCP_V4_START=192.168.111.20
++ export VIRSH_DHCP_V4_START
++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
++ resultvar=VIRSH_DHCP_V4_END
++ network=192.168.111.0/24
++ record=60
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
++ result=192.168.111.60
++ eval VIRSH_DHCP_V4_END=192.168.111.60
+++ VIRSH_DHCP_V4_END=192.168.111.60
++ export VIRSH_DHCP_V4_END
++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
++ resultvar=BAREMETALV4_POOL_RANGE_START
++ network=192.168.111.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
++ result=192.168.111.100
++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
++ export BAREMETALV4_POOL_RANGE_START
++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
++ resultvar=BAREMETALV4_POOL_RANGE_END
++ network=192.168.111.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
++ result=192.168.111.200
++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
++ export BAREMETALV4_POOL_RANGE_END
++ export VIRSH_DHCP_V4_START
++ export VIRSH_DHCP_V4_END
++ export BAREMETALV4_POOL_RANGE_START
++ export BAREMETALV4_POOL_RANGE_END
++ [[ -n '' ]]
++ export EXTERNAL_SUBNET_V6_HOST=
++ EXTERNAL_SUBNET_V6_HOST=
++ export EXTERNAL_SUBNET_V6_PREFIX=
++ EXTERNAL_SUBNET_V6_PREFIX=
++ export BAREMETALV6_POOL_RANGE_START=
++ BAREMETALV6_POOL_RANGE_START=
++ export BAREMETALV6_POOL_RANGE_END=
++ BAREMETALV6_POOL_RANGE_END=
++ export REGISTRY_PORT=5000
++ REGISTRY_PORT=5000
++ export HTTP_PORT=6180
++ HTTP_PORT=6180
++ export IRONIC_INSPECTOR_PORT=5050
++ IRONIC_INSPECTOR_PORT=5050
++ export IRONIC_API_PORT=6385
++ IRONIC_API_PORT=6385
++ [[ -n 192.168.111.1 ]]
++ export REGISTRY=192.168.111.1:5000
++ REGISTRY=192.168.111.1:5000
++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
++ resultvar=INITIAL_IRONICBRIDGE_IP
++ network=172.22.0.0/24
++ record=9
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
++ result=172.22.0.9
++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
++ export INITIAL_IRONICBRIDGE_IP
++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ '[' true == true ']'
++ export IRONIC_URL=https://172.22.0.2:6385/v1/
++ IRONIC_URL=https://172.22.0.2:6385/v1/
++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+ ansible-galaxy install -r vm-setup/requirements.yml
[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names 
to new standard, use callbacks_enabled instead. This feature will be removed 
from ansible-core in version 2.15. Deprecation warnings can be disabled by 
setting deprecation_warnings=False in ansible.cfg.
Starting galaxy role install process
- extracting fubarhouse.golang to /home/azureuser/.ansible/roles/fubarhouse.golang
- fubarhouse.golang (master) was installed successfully
Starting galaxy collection install process
Process install dependency map
Starting collection install process
Downloading https://galaxy.ansible.com/download/kubernetes-core-2.2.1.tar.gz to /home/azureuser/.ansible/tmp/ansible-local-2930xqn_4nq_/tmpq6ystq9x/kubernetes-core-2.2.1-chfkpr1_
Installing 'kubernetes.core:2.2.1' to '/home/azureuser/.ansible/collections/ansible_collections/kubernetes/core'
kubernetes.core:2.2.1 was installed successfully
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e working_dir=/opt/metal3-dev-env -e metal3_dir=/home/azureuser/projects/metal3-dev-env -e virthost=sidneyshiba-capm3-vm -i vm-setup/inventory.ini -b vm-setup/install-package-playbook.yml
[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names 
to new standard, use callbacks_enabled instead. This feature will be removed 
from ansible-core in version 2.15. Deprecation warnings can be disabled by 
setting deprecation_warnings=False in ansible.cfg.

PLAY [Install packages needed for the Dev-env] *********************************
Tuesday 21 December 2021  20:43:59 +0000 (0:00:00.014)       0:00:00.014 ****** 

TASK [Gathering Facts] *********************************************************
ok: [localhost]
Tuesday 21 December 2021  20:44:01 +0000 (0:00:01.583)       0:00:01.598 ****** 

TASK [packages_installation : Install required packages for Ubuntu] ************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/packages_installation/tasks/ubuntu_required_packages.yml for localhost
Tuesday 21 December 2021  20:44:01 +0000 (0:00:00.065)       0:00:01.663 ****** 

TASK [packages_installation : Update all packages to their latest version] *****
changed: [localhost]
Tuesday 21 December 2021  20:45:51 +0000 (0:01:50.219)       0:01:51.883 ****** 

TASK [packages_installation : Fetch yarn gpg key] ******************************
changed: [localhost]
Tuesday 21 December 2021  20:45:52 +0000 (0:00:01.686)       0:01:53.570 ****** 

TASK [packages_installation : Add yarn release key] ****************************
changed: [localhost]
Tuesday 21 December 2021  20:45:53 +0000 (0:00:00.384)       0:01:53.954 ****** 

TASK [packages_installation : Add OS release key] ******************************
changed: [localhost]
Tuesday 21 December 2021  20:45:53 +0000 (0:00:00.275)       0:01:54.230 ****** 

TASK [packages_installation : Fetch OS release key] ****************************
changed: [localhost]
Tuesday 21 December 2021  20:45:55 +0000 (0:00:01.743)       0:01:55.973 ****** 

TASK [packages_installation : Update all packages to their latest version] *****
ok: [localhost]
Tuesday 21 December 2021  20:45:58 +0000 (0:00:02.657)       0:01:58.630 ****** 
Tuesday 21 December 2021  20:45:58 +0000 (0:00:00.049)       0:01:58.680 ****** 
Tuesday 21 December 2021  20:45:58 +0000 (0:00:00.044)       0:01:58.724 ****** 

TASK [packages_installation : Add Dockerâ€™s GPG key] ****************************
ok: [localhost]
Tuesday 21 December 2021  20:45:58 +0000 (0:00:00.661)       0:01:59.386 ****** 

TASK [packages_installation : Add Docker Repository] ***************************
changed: [localhost]
Tuesday 21 December 2021  20:46:05 +0000 (0:00:06.461)       0:02:05.847 ****** 

TASK [packages_installation : Update all packages to their latest version] *****
changed: [localhost]
Tuesday 21 December 2021  20:46:25 +0000 (0:00:20.499)       0:02:26.346 ****** 

TASK [packages_installation : Install docker] **********************************
ok: [localhost] => (item=docker-ce)
ok: [localhost] => (item=docker-ce-cli)
ok: [localhost] => (item=containerd.io)
Tuesday 21 December 2021  20:46:33 +0000 (0:00:07.360)       0:02:33.707 ****** 

TASK [packages_installation : Template daemon.json to /etc/docker/daemon.json] ***
changed: [localhost]
Tuesday 21 December 2021  20:46:33 +0000 (0:00:00.788)       0:02:34.495 ****** 

TASK [packages_installation : Restart docker systemd service] ******************
changed: [localhost]
Tuesday 21 December 2021  20:46:35 +0000 (0:00:01.980)       0:02:36.476 ****** 

TASK [packages_installation : Add current user to the docker group] ************
ok: [localhost]
Tuesday 21 December 2021  20:46:36 +0000 (0:00:00.596)       0:02:37.073 ****** 

TASK [packages_installation : Install common packages using standard package manager for Ubuntu] ***
changed: [localhost]
Tuesday 21 December 2021  20:48:12 +0000 (0:01:35.747)       0:04:12.820 ****** 
Tuesday 21 December 2021  20:48:12 +0000 (0:00:00.049)       0:04:12.870 ****** 

TASK [packages_installation : Install packages using standard package manager for Ubuntu 20.04] ***
changed: [localhost]
Tuesday 21 December 2021  20:48:27 +0000 (0:00:15.046)       0:04:27.917 ****** 
Tuesday 21 December 2021  20:48:27 +0000 (0:00:00.050)       0:04:27.967 ****** 

TASK [packages_installation : Install packages using pip3] *********************
changed: [localhost]
Tuesday 21 December 2021  20:48:46 +0000 (0:00:19.390)       0:04:47.357 ****** 
Tuesday 21 December 2021  20:48:46 +0000 (0:00:00.042)       0:04:47.400 ****** 
Tuesday 21 December 2021  20:48:46 +0000 (0:00:00.040)       0:04:47.441 ****** 
Tuesday 21 December 2021  20:48:46 +0000 (0:00:00.045)       0:04:47.487 ****** 

TASK [fubarhouse.golang : Include tasks gathering system information] **********
included: /home/azureuser/.ansible/roles/fubarhouse.golang/tasks/setup.yml for localhost
Tuesday 21 December 2021  20:48:46 +0000 (0:00:00.055)       0:04:47.542 ****** 
Tuesday 21 December 2021  20:48:47 +0000 (0:00:00.041)       0:04:47.584 ****** 

TASK [fubarhouse.golang : Go-Lang | Define user variable for non-ssh use] ******
ok: [localhost]
Tuesday 21 December 2021  20:48:47 +0000 (0:00:00.050)       0:04:47.634 ****** 

TASK [fubarhouse.golang : Go-Lang | Set $HOME] *********************************
ok: [localhost]
Tuesday 21 December 2021  20:48:47 +0000 (0:00:00.048)       0:04:47.683 ****** 
Tuesday 21 December 2021  20:48:47 +0000 (0:00:00.038)       0:04:47.722 ****** 
Tuesday 21 December 2021  20:48:47 +0000 (0:00:00.035)       0:04:47.757 ****** 

TASK [fubarhouse.golang : Go-Lang | Include OS-Specific tasks (Debian)] ********
included: /home/azureuser/.ansible/roles/fubarhouse.golang/tasks/tasks-Debian.yml for localhost
Tuesday 21 December 2021  20:48:47 +0000 (0:00:00.061)       0:04:47.819 ****** 

TASK [fubarhouse.golang : Go-Lang | Install dependencies] **********************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.890)       0:04:48.710 ****** 

TASK [fubarhouse.golang : Go-Lang | Define GOARCH] *****************************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.047)       0:04:48.758 ****** 

TASK [fubarhouse.golang : Go-Lang | Define GOOS] *******************************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.045)       0:04:48.803 ****** 
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.040)       0:04:48.844 ****** 
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.041)       0:04:48.885 ****** 

TASK [fubarhouse.golang : Go-Lang | Define GO111MODULE] ************************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.053)       0:04:48.939 ****** 

TASK [fubarhouse.golang : Go-Lang | Define GOROOT] *****************************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.050)       0:04:48.989 ****** 

TASK [fubarhouse.golang : Go-Lang | Define GOPATH] *****************************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.046)       0:04:49.036 ****** 

TASK [fubarhouse.golang : Go-Lang | Define GOPROXY] ****************************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.046)       0:04:49.082 ****** 
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.039)       0:04:49.121 ****** 
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.045)       0:04:49.167 ****** 

TASK [fubarhouse.golang : Go-Lang | Define version comparrison string] *********
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.038)       0:04:49.205 ****** 
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.040)       0:04:49.246 ****** 

TASK [fubarhouse.golang : Go-Lang | Define URL for distribution] ***************
ok: [localhost]
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.040)       0:04:49.287 ****** 
Tuesday 21 December 2021  20:48:48 +0000 (0:00:00.036)       0:04:49.323 ****** 

TASK [fubarhouse.golang : Go-Lang | Looking for existing installation] *********
ok: [localhost]
Tuesday 21 December 2021  20:48:49 +0000 (0:00:00.652)       0:04:49.976 ****** 

TASK [fubarhouse.golang : Go-Lang | Getting version information] ***************
ok: [localhost]
Tuesday 21 December 2021  20:48:49 +0000 (0:00:00.383)       0:04:50.360 ****** 

TASK [fubarhouse.golang : Go-Lang | Define expected version output] ************
ok: [localhost]
Tuesday 21 December 2021  20:48:49 +0000 (0:00:00.045)       0:04:50.406 ****** 

TASK [fubarhouse.golang : Include tasks to clean installation] *****************
included: /home/azureuser/.ansible/roles/fubarhouse.golang/tasks/cleanup.yml for localhost
Tuesday 21 December 2021  20:48:49 +0000 (0:00:00.055)       0:04:50.462 ****** 

TASK [fubarhouse.golang : Go-Lang | Removing GOROOT] ***************************
changed: [localhost]
Tuesday 21 December 2021  20:48:51 +0000 (0:00:01.650)       0:04:52.112 ****** 

TASK [fubarhouse.golang : Go-Lang | Removing GOPATH] ***************************
ok: [localhost]
Tuesday 21 December 2021  20:48:51 +0000 (0:00:00.239)       0:04:52.352 ****** 
Tuesday 21 December 2021  20:48:51 +0000 (0:00:00.044)       0:04:52.396 ****** 
Tuesday 21 December 2021  20:48:51 +0000 (0:00:00.040)       0:04:52.436 ****** 

TASK [fubarhouse.golang : Include tasks for installation] **********************
included: /home/azureuser/.ansible/roles/fubarhouse.golang/tasks/install.yml for localhost
Tuesday 21 December 2021  20:48:51 +0000 (0:00:00.093)       0:04:52.529 ****** 
Tuesday 21 December 2021  20:48:52 +0000 (0:00:00.064)       0:04:52.594 ****** 
Tuesday 21 December 2021  20:48:52 +0000 (0:00:00.061)       0:04:52.655 ****** 

TASK [fubarhouse.golang : Go-Lang | Include distro install tasks] **************
included: /home/azureuser/.ansible/roles/fubarhouse.golang/tasks/install-distro.yml for localhost
Tuesday 21 December 2021  20:48:52 +0000 (0:00:00.085)       0:04:52.740 ****** 

TASK [fubarhouse.golang : Go-Lang | Download distribution] *********************
changed: [localhost]
Tuesday 21 December 2021  20:48:54 +0000 (0:00:01.997)       0:04:54.738 ****** 

TASK [fubarhouse.golang : Go-Lang | Empty destination directory] ***************
ok: [localhost]
Tuesday 21 December 2021  20:48:54 +0000 (0:00:00.238)       0:04:54.977 ****** 

TASK [fubarhouse.golang : Go-Lang | Ensure directory is writable] **************
changed: [localhost]
Tuesday 21 December 2021  20:48:54 +0000 (0:00:00.237)       0:04:55.215 ****** 

TASK [fubarhouse.golang : Go-Lang | Unpack distribution] ***********************
changed: [localhost]
Tuesday 21 December 2021  20:49:02 +0000 (0:00:08.262)       0:05:03.477 ****** 

TASK [fubarhouse.golang : Go-Lang | Removing existing installation] ************
changed: [localhost]
Tuesday 21 December 2021  20:49:03 +0000 (0:00:00.251)       0:05:03.728 ****** 

TASK [fubarhouse.golang : Go-Lang | Moving to installation directory] **********
changed: [localhost]
Tuesday 21 December 2021  20:49:03 +0000 (0:00:00.242)       0:05:03.970 ****** 

TASK [fubarhouse.golang : Go-Lang | Remove temporary data] *********************
ok: [localhost]
Tuesday 21 December 2021  20:49:03 +0000 (0:00:00.230)       0:05:04.201 ****** 

TASK [fubarhouse.golang : Go-Lang | Verify version] ****************************
ok: [localhost]
Tuesday 21 December 2021  20:49:03 +0000 (0:00:00.236)       0:05:04.437 ****** 
Tuesday 21 December 2021  20:49:03 +0000 (0:00:00.065)       0:05:04.503 ****** 
Tuesday 21 December 2021  20:49:03 +0000 (0:00:00.042)       0:05:04.546 ****** 

TASK [fubarhouse.golang : Include tasks for setting Go permissions] ************
included: /home/azureuser/.ansible/roles/fubarhouse.golang/tasks/perm.yml for localhost
Tuesday 21 December 2021  20:49:04 +0000 (0:00:00.058)       0:05:04.604 ****** 

TASK [fubarhouse.golang : Go-Lang | Set codebase permissions] ******************
ok: [localhost]
Tuesday 21 December 2021  20:49:04 +0000 (0:00:00.243)       0:05:04.847 ****** 

TASK [fubarhouse.golang : Go-Lang | Set workspace permissions] *****************
ok: [localhost] => (item=src)
ok: [localhost] => (item=pkg)
ok: [localhost] => (item=bin)
Tuesday 21 December 2021  20:49:04 +0000 (0:00:00.671)       0:05:05.519 ****** 

PLAY RECAP *********************************************************************
localhost                  : ok=50   changed=18   unreachable=0    failed=0    skipped=23   rescued=0    ignored=0   

Tuesday 21 December 2021  20:49:04 +0000 (0:00:00.057)       0:05:05.576 ****** 
=============================================================================== 
packages_installation : Update all packages to their latest version --- 110.22s
packages_installation : Install common packages using standard package manager for Ubuntu -- 95.75s
packages_installation : Update all packages to their latest version ---- 20.50s
+ source lib/network.sh
++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ export POD_CIDR=192.168.0.0/18
++ POD_CIDR=192.168.0.0/18
++ PROVISIONING_IPV6=false
++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
++ [[ false == \t\r\u\e ]]
++ export BOOT_MODE=legacy
++ BOOT_MODE=legacy
++ export PROVISIONING_NETWORK=172.22.0.0/24
++ PROVISIONING_NETWORK=172.22.0.0/24
++ [[ legacy == \l\e\g\a\c\y ]]
++ export LIBVIRT_FIRMWARE=bios
++ LIBVIRT_FIRMWARE=bios
++ export LIBVIRT_SECURE_BOOT=false
++ LIBVIRT_SECURE_BOOT=false
++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
++ resultvar=PROVISIONING_CIDR
++ network=172.22.0.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
++ result=24
++ eval PROVISIONING_CIDR=24
+++ PROVISIONING_CIDR=24
++ export PROVISIONING_CIDR
++ export PROVISIONING_CIDR
++ export PROVISIONING_NETMASK=255.255.255.0
++ PROVISIONING_NETMASK=255.255.255.0
++ network_address PROVISIONING_IP 172.22.0.0/24 1
++ resultvar=PROVISIONING_IP
++ network=172.22.0.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
++ result=172.22.0.1
++ eval PROVISIONING_IP=172.22.0.1
+++ PROVISIONING_IP=172.22.0.1
++ export PROVISIONING_IP
++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
++ resultvar=CLUSTER_PROVISIONING_IP
++ network=172.22.0.0/24
++ record=2
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
++ result=172.22.0.2
++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
+++ CLUSTER_PROVISIONING_IP=172.22.0.2
++ export CLUSTER_PROVISIONING_IP
++ export PROVISIONING_IP
++ export CLUSTER_PROVISIONING_IP
++ [[ 172.22.0.1 == *\:* ]]
++ export PROVISIONING_URL_HOST=172.22.0.1
++ PROVISIONING_URL_HOST=172.22.0.1
++ export CLUSTER_URL_HOST=172.22.0.2
++ CLUSTER_URL_HOST=172.22.0.2
++ [[ 192.168.111.249 == *\:* ]]
++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ export CLUSTER_APIENDPOINT_PORT=6443
++ CLUSTER_APIENDPOINT_PORT=6443
++ network_address dhcp_range_start 172.22.0.0/24 10
++ resultvar=dhcp_range_start
++ network=172.22.0.0/24
++ record=10
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
++ result=172.22.0.10
++ eval dhcp_range_start=172.22.0.10
+++ dhcp_range_start=172.22.0.10
++ export dhcp_range_start
++ network_address dhcp_range_end 172.22.0.0/24 100
++ resultvar=dhcp_range_end
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval dhcp_range_end=172.22.0.100
+++ dhcp_range_end=172.22.0.100
++ export dhcp_range_end
++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
++ resultvar=PROVISIONING_POOL_RANGE_START
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ PROVISIONING_POOL_RANGE_START=172.22.0.100
++ export PROVISIONING_POOL_RANGE_START
++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
++ resultvar=PROVISIONING_POOL_RANGE_END
++ network=172.22.0.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
++ result=172.22.0.200
++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ PROVISIONING_POOL_RANGE_END=172.22.0.200
++ export PROVISIONING_POOL_RANGE_END
++ export PROVISIONING_POOL_RANGE_START
++ export PROVISIONING_POOL_RANGE_END
++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ EXTERNAL_SUBNET=
++ [[ -n '' ]]
++ export IP_STACK=v4
++ IP_STACK=v4
++ [[ v4 == \v\4 ]]
++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
++ EXTERNAL_SUBNET_V4=192.168.111.0/24
++ export EXTERNAL_SUBNET_V6=
++ EXTERNAL_SUBNET_V6=
++ [[ kind == \m\i\n\i\k\u\b\e ]]
++ [[ -n 192.168.111.0/24 ]]
++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
++ network=192.168.111.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
++ result=24
++ eval EXTERNAL_SUBNET_V4_PREFIX=24
+++ EXTERNAL_SUBNET_V4_PREFIX=24
++ export EXTERNAL_SUBNET_V4_PREFIX
++ export EXTERNAL_SUBNET_V4_PREFIX
++ [[ -z 192.168.111.1 ]]
++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
++ resultvar=VIRSH_DHCP_V4_START
++ network=192.168.111.0/24
++ record=20
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
++ result=192.168.111.20
++ eval VIRSH_DHCP_V4_START=192.168.111.20
+++ VIRSH_DHCP_V4_START=192.168.111.20
++ export VIRSH_DHCP_V4_START
++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
++ resultvar=VIRSH_DHCP_V4_END
++ network=192.168.111.0/24
++ record=60
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
++ result=192.168.111.60
++ eval VIRSH_DHCP_V4_END=192.168.111.60
+++ VIRSH_DHCP_V4_END=192.168.111.60
++ export VIRSH_DHCP_V4_END
++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
++ resultvar=BAREMETALV4_POOL_RANGE_START
++ network=192.168.111.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
++ result=192.168.111.100
++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
++ export BAREMETALV4_POOL_RANGE_START
++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
++ resultvar=BAREMETALV4_POOL_RANGE_END
++ network=192.168.111.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
++ result=192.168.111.200
++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
++ export BAREMETALV4_POOL_RANGE_END
++ export VIRSH_DHCP_V4_START
++ export VIRSH_DHCP_V4_END
++ export BAREMETALV4_POOL_RANGE_START
++ export BAREMETALV4_POOL_RANGE_END
++ [[ -n '' ]]
++ export EXTERNAL_SUBNET_V6_HOST=
++ EXTERNAL_SUBNET_V6_HOST=
++ export EXTERNAL_SUBNET_V6_PREFIX=
++ EXTERNAL_SUBNET_V6_PREFIX=
++ export BAREMETALV6_POOL_RANGE_START=
++ BAREMETALV6_POOL_RANGE_START=
++ export BAREMETALV6_POOL_RANGE_END=
++ BAREMETALV6_POOL_RANGE_END=
++ export REGISTRY_PORT=5000
++ REGISTRY_PORT=5000
++ export HTTP_PORT=6180
++ HTTP_PORT=6180
++ export IRONIC_INSPECTOR_PORT=5050
++ IRONIC_INSPECTOR_PORT=5050
++ export IRONIC_API_PORT=6385
++ IRONIC_API_PORT=6385
++ [[ -n 192.168.111.1 ]]
++ export REGISTRY=192.168.111.1:5000
++ REGISTRY=192.168.111.1:5000
++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
++ resultvar=INITIAL_IRONICBRIDGE_IP
++ network=172.22.0.0/24
++ record=9
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
++ result=172.22.0.9
++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
++ export INITIAL_IRONICBRIDGE_IP
++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ '[' true == true ']'
++ export IRONIC_URL=https://172.22.0.2:6385/v1/
++ IRONIC_URL=https://172.22.0.2:6385/v1/
++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+ source lib/images.sh
++ IMAGE_OS=Ubuntu
++ [[ Ubuntu == \U\b\u\n\t\u ]]
++ export IMAGE_NAME=UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2
++ IMAGE_NAME=UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2
++ export IMAGE_LOCATION=https://artifactory.nordix.org/artifactory/airship/images/k8s_v1.21.1/
++ IMAGE_LOCATION=https://artifactory.nordix.org/artifactory/airship/images/k8s_v1.21.1/
++ export IMAGE_URL=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2
++ IMAGE_URL=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2
++ export IMAGE_CHECKSUM=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2.md5sum
++ IMAGE_CHECKSUM=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2.md5sum
++ export IMAGE_USERNAME=metal3
++ IMAGE_USERNAME=metal3
++ IMAGE_BASE_NAME=UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1
++ export IMAGE_RAW_NAME=UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img
++ IMAGE_RAW_NAME=UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img
++ export IMAGE_RAW_URL=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img
++ IMAGE_RAW_URL=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img
++ export IMAGE_RAW_CHECKSUM=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img.md5sum
++ IMAGE_RAW_CHECKSUM=http://172.22.0.1/images/UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img.md5sum
+ GOBINARY=/usr/local/go/bin
+ [[ :/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
+ source lib/releases.sh
++ CAPM3RELEASEPATH=https://api.github.com/repos/metal3-io/cluster-api-provider-metal3/releases
++ CAPIRELEASEPATH=https://api.github.com/repos/kubernetes-sigs/cluster-api/releases
++ '[' v1beta1 == v1alpha3 ']'
++ '[' v1beta1 == v1alpha4 ']'
+++ get_latest_release https://api.github.com/repos/kubernetes-sigs/cluster-api/releases v1.0.
+++ set +x
+++ echo v1.0.2
++ export CAPIRELEASE=v1.0.2
++ CAPIRELEASE=v1.0.2
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ export CAPM3RELEASE=v1.0.0
++ CAPM3RELEASE=v1.0.0
++ [[ v1.0.2 == '' ]]
++ [[ v1.0.0 == '' ]]
+ kubectl krew
++ mktemp -d
+ cd /tmp/tmp.QbfVaKHJHo
++ uname
++ tr '[:upper:]' '[:lower:]'
+ OS=linux
++ uname -m
++ sed -e s/x86_64/amd64/ -e 's/\(arm\)\(64\)\?.*/\1\2/' -e 's/aarch64$/arm64/'
+ ARCH=amd64
+ KREW=krew-linux_amd64
+ curl -fsSLO https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-linux_amd64.tar.gz
+ tar zxvf krew-linux_amd64.tar.gz
./LICENSE
./krew-linux_amd64
+ rm -f krew-linux_amd64.tar.gz
+ ./krew-linux_amd64 install krew
Adding "default" plugin index from https://github.com/kubernetes-sigs/krew-index.git.
Updated the local copy of plugin index.
Installing plugin: krew
Installed plugin: krew
\
 | Use this plugin:
 | 	kubectl krew
 | Documentation:
 | 	https://krew.sigs.k8s.io/
 | Caveats:
 | \
 |  | krew is now installed! To start using kubectl plugins, you need to add
 |  | krew's installation directory to your PATH:
 |  | 
 |  |   * macOS/Linux:
 |  |     - Add the following to your ~/.bashrc or ~/.zshrc:
 |  |         export PATH="${KREW_ROOT:-$HOME/.krew}/bin:$PATH"
 |  |     - Restart your shell.
 |  | 
 |  |   * Windows: Add %USERPROFILE%\.krew\bin to your PATH environment variable
 |  | 
 |  | To list krew commands and to get help, run:
 |  |   $ kubectl krew
 |  | For a full list of available plugins, run:
 |  |   $ kubectl krew search
 |  | 
 |  | You can find documentation at
 |  |   https://krew.sigs.k8s.io/docs/user-guide/quickstart/.
 | /
/
+ echo export PATH=/home/azureuser/.krew/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin
+ id azureuser
+ grep -q libvirt
+ '[' kind == minikube ']'
+ command -v kind
/usr/local/bin/kind
++ kind version -q
+ [[ v0.11.1 != \v\0\.\1\1\.\1 ]]
+ '[' kind == tilt ']'
++ curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt
+ KUBECTL_LATEST=v1.23.1
++ kubectl version --client --short
++ cut -d : -f2
++ sed 's/[[:space:]]//g'
+ KUBECTL_LOCAL=v1.21.1
++ whereis -b kubectl
++ cut -d : -f2
++ awk '{print $1}'
+ KUBECTL_PATH=/usr/local/bin/kubectl
+ '[' v1.21.1 '!=' v1.23.1 ']'
+ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.1/bin/linux/amd64/kubectl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 44.4M  100 44.4M    0     0   160M      0 --:--:-- --:--:-- --:--:--  160M
+ chmod +x kubectl
+ KUBECTL_PATH=/usr/local/bin/kubectl
+ sudo mv kubectl /usr/local/bin/kubectl
+ command -v kustomize
/usr/local/bin/kustomize
++ command -v clusterctl
+ '[' -x '' ']'
+ install_clusterctl
+ curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.0.2/clusterctl-linux-amd64 -o clusterctl
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   661  100   661    0     0   7184      0 --:--:-- --:--:-- --:--:--  7263
100 51.5M  100 51.5M    0     0  99.2M      0 --:--:-- --:--:-- --:--:-- 99.2M
+ chmod +x ./clusterctl
+ sudo mv ./clusterctl /usr/local/bin/clusterctl
+ remove_ironic_containers
+ for name in ipa-downloader vbmc sushy-tools httpd-infra
+ sudo docker ps
+ grep -w 'ipa-downloader$'
+ true
+ sudo docker ps --all
+ grep -w 'ipa-downloader$'
+ true
+ for name in ipa-downloader vbmc sushy-tools httpd-infra
+ sudo docker ps
+ grep -w 'vbmc$'
+ true
+ sudo docker ps --all
+ grep -w 'vbmc$'
+ true
+ for name in ipa-downloader vbmc sushy-tools httpd-infra
+ sudo docker ps
+ grep -w 'sushy-tools$'
+ true
+ sudo docker ps --all
+ grep -w 'sushy-tools$'
+ true
+ for name in ipa-downloader vbmc sushy-tools httpd-infra
+ sudo docker ps
+ grep -w 'httpd-infra$'
+ true
+ sudo docker ps --all
+ grep -w 'httpd-infra$'
+ true
+ case $CONTAINER_RUNTIME in
+ mkdir -p /opt/metal3-dev-env/ironic/html/images
+ pushd /opt/metal3-dev-env/ironic/html/images
/opt/metal3-dev-env/ironic/html/images /tmp/tmp.QbfVaKHJHo
+ '[' '!' -f UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2 ']'
+ wget --no-verbose --no-check-certificate https://artifactory.nordix.org/artifactory/airship/images/k8s_v1.21.1//UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2
2021-12-21 20:50:13 URL:https://artifactory.nordix.org/artifactory/airship/images/k8s_v1.21.1//UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2 [1385037824/1385037824] -> "UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2" [1]
+ IMAGE_SUFFIX=qcow2
+ '[' qcow2 == xz ']'
+ '[' qcow2 == bz2 ']'
+ '[' qcow2 '!=' iso ']'
+ qemu-img convert -O raw UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1.qcow2 UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img
+ md5sum UBUNTU_20.04_NODE_IMAGE_K8S_v1.21.1-raw.img
+ awk '{print $1}'
+ popd
/tmp/tmp.QbfVaKHJHo
++ env
++ grep -v _LOCAL_IMAGE=
++ grep _IMAGE=
++ grep -o '^[^=]*'
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=registry:2.7.1
+ pull_container_image_if_missing registry:2.7.1
+ local IMAGE=registry:2.7.1
+ '[' docker == docker ']'
++ sudo docker image ls registry:2.7.1
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull registry:2.7.1
2.7.1: Pulling from library/registry
79e9f2f55bf5: Pulling fs layer
0d96da54f60b: Pulling fs layer
5b27040df4a2: Pulling fs layer
e2ead8259a04: Pulling fs layer
3790aef225b9: Pulling fs layer
e2ead8259a04: Waiting
3790aef225b9: Waiting
0d96da54f60b: Verifying Checksum
0d96da54f60b: Download complete
79e9f2f55bf5: Verifying Checksum
5b27040df4a2: Verifying Checksum
5b27040df4a2: Download complete
e2ead8259a04: Download complete
3790aef225b9: Download complete
79e9f2f55bf5: Pull complete
0d96da54f60b: Pull complete
5b27040df4a2: Pull complete
e2ead8259a04: Pull complete
3790aef225b9: Pull complete
Digest: sha256:169211e20e2f2d5d115674681eb79d21a217b296b43374b8e39f97fcf866b375
Status: Downloaded newer image for registry:2.7.1
docker.io/library/registry:2.7.1
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/baremetal-operator
+ pull_container_image_if_missing quay.io/metal3-io/baremetal-operator
+ local IMAGE=quay.io/metal3-io/baremetal-operator
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/baremetal-operator
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/baremetal-operator
Using default tag: latest
latest: Pulling from metal3-io/baremetal-operator
e8614d09b7be: Pulling fs layer
c6f4d1a13b69: Pulling fs layer
b9063922a5d6: Pulling fs layer
e8614d09b7be: Download complete
c6f4d1a13b69: Download complete
e8614d09b7be: Pull complete
b9063922a5d6: Download complete
c6f4d1a13b69: Pull complete
b9063922a5d6: Pull complete
Digest: sha256:b1ab481ff5db5d92e0eaafd3a46910fb96ca6f0dc47adcfa97cdab5c0bb4d0a2
Status: Downloaded newer image for quay.io/metal3-io/baremetal-operator:latest
quay.io/metal3-io/baremetal-operator:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-client
+ pull_container_image_if_missing quay.io/metal3-io/ironic-client
+ local IMAGE=quay.io/metal3-io/ironic-client
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/ironic-client
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/ironic-client
Using default tag: latest
latest: Pulling from metal3-io/ironic-client
7a0437f04f83: Pulling fs layer
ff6b4c2f0f92: Pulling fs layer
d6afcd6863af: Pulling fs layer
ff6b4c2f0f92: Download complete
7a0437f04f83: Verifying Checksum
7a0437f04f83: Download complete
d6afcd6863af: Verifying Checksum
7a0437f04f83: Pull complete
ff6b4c2f0f92: Pull complete
d6afcd6863af: Pull complete
Digest: sha256:0abe9a3de15449f9cb7c8ec3daa5dceaf124c2e1705c51ef73dfc54f92dacec4
Status: Downloaded newer image for quay.io/metal3-io/ironic-client:latest
quay.io/metal3-io/ironic-client:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic
+ pull_container_image_if_missing quay.io/metal3-io/ironic
+ local IMAGE=quay.io/metal3-io/ironic
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/ironic
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/ironic
Using default tag: latest
latest: Pulling from metal3-io/ironic
203c612978b4: Pulling fs layer
e3ff3ac72de5: Pulling fs layer
758c773fb4aa: Pulling fs layer
5186e8704530: Pulling fs layer
db8b47d17071: Pulling fs layer
224b30631488: Pulling fs layer
c990d893af5f: Pulling fs layer
ad1c3378588c: Pulling fs layer
43cac59fcfce: Pulling fs layer
0fa3e884fb4a: Pulling fs layer
471899ff9f89: Pulling fs layer
db8b47d17071: Waiting
cb590f4e9fc4: Pulling fs layer
ae569c981739: Pulling fs layer
942103bd94ac: Pulling fs layer
5186e8704530: Waiting
18d232a501dc: Pulling fs layer
2fb2319d5d97: Pulling fs layer
c35169bdc4fe: Pulling fs layer
ecda47a700db: Pulling fs layer
44fb388893d9: Pulling fs layer
224b30631488: Waiting
1cc4127e9cbf: Pulling fs layer
c990d893af5f: Waiting
0fa3e884fb4a: Waiting
2fb2319d5d97: Waiting
ae569c981739: Waiting
ad1c3378588c: Waiting
471899ff9f89: Waiting
c35169bdc4fe: Waiting
ecda47a700db: Waiting
43cac59fcfce: Waiting
942103bd94ac: Waiting
18d232a501dc: Waiting
44fb388893d9: Waiting
1cc4127e9cbf: Waiting
e3ff3ac72de5: Download complete
758c773fb4aa: Verifying Checksum
758c773fb4aa: Download complete
db8b47d17071: Download complete
224b30631488: Verifying Checksum
224b30631488: Download complete
5186e8704530: Verifying Checksum
5186e8704530: Download complete
ad1c3378588c: Verifying Checksum
ad1c3378588c: Download complete
203c612978b4: Verifying Checksum
203c612978b4: Download complete
43cac59fcfce: Download complete
0fa3e884fb4a: Verifying Checksum
0fa3e884fb4a: Download complete
471899ff9f89: Verifying Checksum
471899ff9f89: Download complete
cb590f4e9fc4: Download complete
ae569c981739: Verifying Checksum
ae569c981739: Download complete
942103bd94ac: Download complete
2fb2319d5d97: Verifying Checksum
2fb2319d5d97: Download complete
18d232a501dc: Verifying Checksum
18d232a501dc: Download complete
c35169bdc4fe: Verifying Checksum
c35169bdc4fe: Download complete
ecda47a700db: Verifying Checksum
ecda47a700db: Download complete
44fb388893d9: Verifying Checksum
44fb388893d9: Download complete
1cc4127e9cbf: Verifying Checksum
1cc4127e9cbf: Download complete
c990d893af5f: Verifying Checksum
c990d893af5f: Download complete
203c612978b4: Pull complete
e3ff3ac72de5: Pull complete
758c773fb4aa: Pull complete
5186e8704530: Pull complete
db8b47d17071: Pull complete
224b30631488: Pull complete
c990d893af5f: Pull complete
ad1c3378588c: Pull complete
43cac59fcfce: Pull complete
0fa3e884fb4a: Pull complete
471899ff9f89: Pull complete
cb590f4e9fc4: Pull complete
ae569c981739: Pull complete
942103bd94ac: Pull complete
18d232a501dc: Pull complete
2fb2319d5d97: Pull complete
c35169bdc4fe: Pull complete
ecda47a700db: Pull complete
44fb388893d9: Pull complete
1cc4127e9cbf: Pull complete
Digest: sha256:444518fec926d2b9bc00b1d7e77f687177025e3e8cc789cc7e0e318cc152e96f
Status: Downloaded newer image for quay.io/metal3-io/ironic:latest
quay.io/metal3-io/ironic:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ pull_container_image_if_missing quay.io/metal3-io/ironic-ipa-downloader
+ local IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/ironic-ipa-downloader
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/ironic-ipa-downloader
Using default tag: latest
latest: Pulling from metal3-io/ironic-ipa-downloader
7a0437f04f83: Already exists
45be930747df: Pulling fs layer
f6aae69b6737: Pulling fs layer
f6aae69b6737: Verifying Checksum
f6aae69b6737: Download complete
45be930747df: Verifying Checksum
45be930747df: Download complete
45be930747df: Pull complete
f6aae69b6737: Pull complete
Digest: sha256:d2d871675b629bf66514ccda2e2616c50670f7fff9d95b983a216f3a7fdaa1aa
Status: Downloaded newer image for quay.io/metal3-io/ironic-ipa-downloader:latest
quay.io/metal3-io/ironic-ipa-downloader:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/sushy-tools
+ pull_container_image_if_missing quay.io/metal3-io/sushy-tools
+ local IMAGE=quay.io/metal3-io/sushy-tools
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/sushy-tools
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/sushy-tools
Using default tag: latest
latest: Pulling from metal3-io/sushy-tools
5e0b432e8ba9: Pulling fs layer
a84cfd68b5ce: Pulling fs layer
e8b8f2315954: Pulling fs layer
0598fa43a7e7: Pulling fs layer
83098237b6d3: Pulling fs layer
b92c73d4de9a: Pulling fs layer
d8175d86ad38: Pulling fs layer
a2c9bf0d5eac: Pulling fs layer
b254bd014afa: Pulling fs layer
b0cce9b27455: Pulling fs layer
a2c9bf0d5eac: Waiting
0598fa43a7e7: Waiting
83098237b6d3: Waiting
b0cce9b27455: Waiting
b254bd014afa: Waiting
d8175d86ad38: Waiting
b92c73d4de9a: Waiting
a84cfd68b5ce: Verifying Checksum
a84cfd68b5ce: Download complete
e8b8f2315954: Verifying Checksum
e8b8f2315954: Download complete
5e0b432e8ba9: Verifying Checksum
5e0b432e8ba9: Download complete
b92c73d4de9a: Verifying Checksum
b92c73d4de9a: Download complete
d8175d86ad38: Verifying Checksum
d8175d86ad38: Download complete
0598fa43a7e7: Verifying Checksum
0598fa43a7e7: Download complete
a2c9bf0d5eac: Verifying Checksum
a2c9bf0d5eac: Download complete
b254bd014afa: Verifying Checksum
b254bd014afa: Download complete
5e0b432e8ba9: Pull complete
b0cce9b27455: Verifying Checksum
b0cce9b27455: Download complete
83098237b6d3: Download complete
a84cfd68b5ce: Pull complete
e8b8f2315954: Pull complete
0598fa43a7e7: Pull complete
83098237b6d3: Pull complete
b92c73d4de9a: Pull complete
d8175d86ad38: Pull complete
a2c9bf0d5eac: Pull complete
b254bd014afa: Pull complete
b0cce9b27455: Pull complete
Digest: sha256:f90916d628816257566b92a5e90058fb32781b0a95c95c96dd6d7920423bc5ba
Status: Downloaded newer image for quay.io/metal3-io/sushy-tools:latest
quay.io/metal3-io/sushy-tools:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/keepalived
+ pull_container_image_if_missing quay.io/metal3-io/keepalived
+ local IMAGE=quay.io/metal3-io/keepalived
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/keepalived
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/keepalived
Using default tag: latest
latest: Pulling from metal3-io/keepalived
7b1a6ab2e44d: Pulling fs layer
9ccf59d4284f: Pulling fs layer
c244c1e5b29c: Pulling fs layer
5b5a887705fb: Pulling fs layer
5b5a887705fb: Waiting
c244c1e5b29c: Verifying Checksum
c244c1e5b29c: Download complete
9ccf59d4284f: Verifying Checksum
9ccf59d4284f: Download complete
7b1a6ab2e44d: Verifying Checksum
7b1a6ab2e44d: Download complete
7b1a6ab2e44d: Pull complete
5b5a887705fb: Download complete
9ccf59d4284f: Pull complete
c244c1e5b29c: Pull complete
5b5a887705fb: Pull complete
Digest: sha256:4d2d44db445e898a08b072a29af18c325f92a06508b720ea9c95ecddc09c942c
Status: Downloaded newer image for quay.io/metal3-io/keepalived:latest
quay.io/metal3-io/keepalived:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/vbmc
+ pull_container_image_if_missing quay.io/metal3-io/vbmc
+ local IMAGE=quay.io/metal3-io/vbmc
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/vbmc
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/vbmc
Using default tag: latest
latest: Pulling from metal3-io/vbmc
203c612978b4: Already exists
e3ff3ac72de5: Already exists
758c773fb4aa: Already exists
5186e8704530: Already exists
cdff9266628c: Pulling fs layer
cdff9266628c: Verifying Checksum
cdff9266628c: Download complete
cdff9266628c: Pull complete
Digest: sha256:2f1538cf476c5a7971bf7554b98f8641bb2de9fcc26bddb0eedd6ac9c509fdab
Status: Downloaded newer image for quay.io/metal3-io/vbmc:latest
quay.io/metal3-io/vbmc:latest
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ip-address-manager:main
+ pull_container_image_if_missing quay.io/metal3-io/ip-address-manager:main
+ local IMAGE=quay.io/metal3-io/ip-address-manager:main
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/ip-address-manager:main
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/ip-address-manager:main
main: Pulling from metal3-io/ip-address-manager
e8614d09b7be: Already exists
3f5c4e713840: Pulling fs layer
3f5c4e713840: Download complete
3f5c4e713840: Pull complete
Digest: sha256:dc8880f770bbf91fdaf8379be5ecc94ccf9ab3ab0b8d281a26bc522cda40590f
Status: Downloaded newer image for quay.io/metal3-io/ip-address-manager:main
quay.io/metal3-io/ip-address-manager:main
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ pull_container_image_if_missing quay.io/metal3-io/cluster-api-provider-metal3:main
+ local IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ '[' docker == docker ']'
++ sudo docker image ls quay.io/metal3-io/cluster-api-provider-metal3:main
++ tail -n +2
+ [[ -z '' ]]
+ sudo docker pull quay.io/metal3-io/cluster-api-provider-metal3:main
main: Pulling from metal3-io/cluster-api-provider-metal3
e8614d09b7be: Already exists
1bf5c472865e: Pulling fs layer
1bf5c472865e: Verifying Checksum
1bf5c472865e: Download complete
1bf5c472865e: Pull complete
Digest: sha256:ac11ca138767da6c9f9d453eed351a2f092435a9fb214511eb081247c361d000
Status: Downloaded newer image for quay.io/metal3-io/cluster-api-provider-metal3:main
quay.io/metal3-io/cluster-api-provider-metal3:main
+ true
+ sudo docker run -d --net host --name ipa-downloader -e IPA_BASEURI= -v /opt/metal3-dev-env/ironic:/shared quay.io/metal3-io/ironic-ipa-downloader /usr/local/bin/get-resource.sh
11680e8a610584fafccedd4afee3a48273f3b0f81b9e4e1e445d14035bdc5ec9
+ sudo docker wait ipa-downloader
0
+ '[' kind == minikube ']'
./02_configure_host.sh
+ source lib/logging.sh
+++ dirname ./02_configure_host.sh
++ LOGDIR=./logs
++ '[' '!' -d ./logs ']'
+++ basename ./02_configure_host.sh .sh
+++ date +%F-%H%M%S
++ LOGFILE=./logs/02_configure_host-2021-12-21-205254.log
++ echo 'Logging to ./logs/02_configure_host-2021-12-21-205254.log'
Logging to ./logs/02_configure_host-2021-12-21-205254.log
++ exec
+++ tee ./logs/02_configure_host-2021-12-21-205254.log
+ source lib/common.sh
++ [[ :/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin
+++ go env
++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/azureuser/.cache/go-build"
GOENV="/home/azureuser/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/azureuser/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/azureuser/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build455203989=/tmp/go-build -gno-record-gcc-switches"'
+++ GO111MODULE=
+++ GOARCH=amd64
+++ GOBIN=
+++ GOCACHE=/home/azureuser/.cache/go-build
+++ GOENV=/home/azureuser/.config/go/env
+++ GOEXE=
+++ GOFLAGS=
+++ GOHOSTARCH=amd64
+++ GOHOSTOS=linux
+++ GOINSECURE=
+++ GOMODCACHE=/home/azureuser/go/pkg/mod
+++ GONOPROXY=
+++ GONOSUMDB=
+++ GOOS=linux
+++ GOPATH=/home/azureuser/go
+++ GOPRIVATE=
+++ GOPROXY=https://proxy.golang.org,direct
+++ GOROOT=/usr/local/go
+++ GOSUMDB=sum.golang.org
+++ GOTMPDIR=
+++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
+++ GOVCS=
+++ GOVERSION=go1.16.7
+++ GCCGO=gccgo
+++ AR=ar
+++ CC=gcc
+++ CXX=g++
+++ CGO_ENABLED=1
+++ GOMOD=/dev/null
+++ CGO_CFLAGS='-g -O2'
+++ CGO_CPPFLAGS=
+++ CGO_CXXFLAGS='-g -O2'
+++ CGO_FFLAGS='-g -O2'
+++ CGO_LDFLAGS='-g -O2'
+++ PKG_CONFIG=pkg-config
+++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build455203989=/tmp/go-build -gno-record-gcc-switches'
++ export GOPATH
++++ dirname lib/common.sh
+++ cd lib/..
+++ pwd
++ SCRIPTDIR=/home/azureuser/projects/metal3-dev-env
+++ whoami
++ USER=azureuser
++ export USER=azureuser
++ USER=azureuser
++ '[' -z '' ']'
++ '[' '!' -f /home/azureuser/projects/metal3-dev-env/config_azureuser.sh ']'
++ CONFIG=/home/azureuser/projects/metal3-dev-env/config_azureuser.sh
++ source /home/azureuser/projects/metal3-dev-env/config_azureuser.sh
+++ export KUBECONFIG=/home/azureuser/.kube/config
+++ KUBECONFIG=/home/azureuser/.kube/config
+++ export K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
+++ K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
+++ export IMAGE_OS=Ubuntu
+++ IMAGE_OS=Ubuntu
+++ export EPHEMERAL_CLUSTER=kind
+++ EPHEMERAL_CLUSTER=kind
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ export CAPM3_VERSION=v1beta1
+++ CAPM3_VERSION=v1beta1
+++ export CAPI_VERSION=v1beta1
+++ CAPI_VERSION=v1beta1
+++ export NUM_NODES=5
+++ NUM_NODES=5
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=2
+++ NUM_OF_WORKER_REPLICAS=2
+++ export KUBERNETES_VERSION=v1.21.1
+++ KUBERNETES_VERSION=v1.21.1
+++ export UPGRADED_K8S_VERSION=v1.22.2
+++ UPGRADED_K8S_VERSION=v1.22.2
++ export MARIADB_HOST=mariaDB
++ MARIADB_HOST=mariaDB
++ export MARIADB_HOST_IP=127.0.0.1
++ MARIADB_HOST_IP=127.0.0.1
++ ADDN_DNS=
++ EXT_IF=
++ PRO_IF=
++ MANAGE_BR_BRIDGE=y
++ MANAGE_PRO_BRIDGE=y
++ MANAGE_INT_BRIDGE=y
++ INT_IF=
++ ROOT_DISK_NAME=/dev/sda
++ NODE_HOSTNAME_FORMAT=node-%d
++ source /etc/os-release
+++ NAME=Ubuntu
+++ VERSION='20.04.3 LTS (Focal Fossa)'
+++ ID=ubuntu
+++ ID_LIKE=debian
+++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
+++ VERSION_ID=20.04
+++ HOME_URL=https://www.ubuntu.com/
+++ SUPPORT_URL=https://help.ubuntu.com/
+++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
+++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
+++ VERSION_CODENAME=focal
+++ UBUNTU_CODENAME=focal
++ export DISTRO=ubuntu20
++ DISTRO=ubuntu20
++ export OS=ubuntu
++ OS=ubuntu
++ export OS_VERSION_ID=20.04
++ OS_VERSION_ID=20.04
++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
++ export SUPPORTED_DISTROS
++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
++ [[ ubuntu == ubuntu ]]
++ export CONTAINER_RUNTIME=docker
++ CONTAINER_RUNTIME=docker
++ [[ docker == \p\o\d\m\a\n ]]
++ export POD_NAME=
++ POD_NAME=
++ export POD_NAME_INFRA=
++ POD_NAME_INFRA=
++ export SSH_KEY=/home/azureuser/.ssh/id_rsa
++ SSH_KEY=/home/azureuser/.ssh/id_rsa
++ export SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
++ SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
++ '[' '!' -f /home/azureuser/.ssh/id_rsa ']'
++ FILESYSTEM=/
++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
++ export CAPM3_VERSION=v1beta1
++ CAPM3_VERSION=v1beta1
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ '[' v1beta1 == v1beta1 ']'
++ export CAPI_VERSION=v1beta1
++ CAPI_VERSION=v1beta1
++ export M3PATH=/home/azureuser/go/src/github.com/metal3-io
++ M3PATH=/home/azureuser/go/src/github.com/metal3-io
++ export BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
++ BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
++ export RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ export CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ export IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
++ IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
++ export IPAM_BASE_URL=metal3-io/ip-address-manager
++ IPAM_BASE_URL=metal3-io/ip-address-manager
++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ '[' v1beta1 == v1alpha3 ']'
++ '[' v1beta1 == v1alpha4 ']'
++ IPAMBRANCH=main
++ IPA_DOWNLOAD_ENABLED=true
++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ CAPM3BRANCH=main
++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
++ BMOBRANCH=master
++ FORCE_REPO_UPDATE=true
++ BMOCOMMIT=HEAD
++ BMO_RUN_LOCAL=false
++ CAPM3_RUN_LOCAL=false
++ WORKING_DIR=/opt/metal3-dev-env
++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
++ NODES_PLATFORM=libvirt
++ export NAMESPACE=metal3
++ NAMESPACE=metal3
++ export NUM_NODES=5
++ NUM_NODES=5
++ export NUM_OF_MASTER_REPLICAS=3
++ NUM_OF_MASTER_REPLICAS=3
++ export NUM_OF_WORKER_REPLICAS=2
++ NUM_OF_WORKER_REPLICAS=2
++ export VM_EXTRADISKS=false
++ VM_EXTRADISKS=false
++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
++ VM_EXTRADISKS_FILE_SYSTEM=ext4
++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ export NODE_DRAIN_TIMEOUT=0s
++ NODE_DRAIN_TIMEOUT=0s
++ export MAX_SURGE_VALUE=1
++ MAX_SURGE_VALUE=1
++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ export CONTAINER_REGISTRY=quay.io
++ CONTAINER_REGISTRY=quay.io
++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
++ VBMC_IMAGE=quay.io/metal3-io/vbmc
++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ export IRONIC_TLS_SETUP=true
++ IRONIC_TLS_SETUP=true
++ export IRONIC_BASIC_AUTH=true
++ IRONIC_BASIC_AUTH=true
++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
++ IRONIC_IMAGE=quay.io/metal3-io/ironic
++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ '[' v1beta1 == v1alpha4 ']'
++ export IRONIC_NAMESPACE=baremetal-operator-system
++ IRONIC_NAMESPACE=baremetal-operator-system
++ export NAMEPREFIX=baremetal-operator
++ NAMEPREFIX=baremetal-operator
++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ export OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
++ OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ export DEFAULT_HOSTS_MEMORY=4096
++ DEFAULT_HOSTS_MEMORY=4096
++ export CLUSTER_NAME=test1
++ CLUSTER_NAME=test1
++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
++ CLUSTER_APIENDPOINT_IP=192.168.111.249
++ export KUBERNETES_VERSION=v1.21.1
++ KUBERNETES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_VERSION=v1.21.1
++ KUBERNETES_BINARIES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ '[' docker == docker ']'
++ export EPHEMERAL_CLUSTER=kind
++ EPHEMERAL_CLUSTER=kind
++ export KUSTOMIZE_VERSION=v4.1.3
++ KUSTOMIZE_VERSION=v4.1.3
++ export KIND_VERSION=v0.11.1
++ KIND_VERSION=v0.11.1
++ '[' v1.21.1 == v1.21.2 ']'
++ export KIND_NODE_IMAGE_VERSION=v1.22.2
++ KIND_NODE_IMAGE_VERSION=v1.22.2
++ export MINIKUBE_VERSION=v1.23.2
++ MINIKUBE_VERSION=v1.23.2
++ export ANSIBLE_VERSION=4.8.0
++ ANSIBLE_VERSION=4.8.0
++ SKIP_RETRIES=false
++ TEST_TIME_INTERVAL=10
++ TEST_MAX_TIME=240
++ FAILS=0
++ RESULT_STR=
++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ '[' 5 -lt 5 ']'
++ export LIBVIRT_DEFAULT_URI=qemu:///system
++ LIBVIRT_DEFAULT_URI=qemu:///system
++ '[' azureuser '!=' root ']'
++ '[' /run/user/1000 == /run/user/0 ']'
++ sudo -n uptime
++ export USE_FIREWALLD=False
++ USE_FIREWALLD=False
++ [[ ubuntu20 == \r\h\e\l\8 ]]
++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
+++ df / --output=fstype
+++ tail -n 1
++ FSTYPE=ext4
++ case ${FSTYPE} in
++ '[' '!' -d /opt/metal3-dev-env ']'
+ source lib/network.sh
++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ export POD_CIDR=192.168.0.0/18
++ POD_CIDR=192.168.0.0/18
++ PROVISIONING_IPV6=false
++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
++ [[ false == \t\r\u\e ]]
++ export BOOT_MODE=legacy
++ BOOT_MODE=legacy
++ export PROVISIONING_NETWORK=172.22.0.0/24
++ PROVISIONING_NETWORK=172.22.0.0/24
++ [[ legacy == \l\e\g\a\c\y ]]
++ export LIBVIRT_FIRMWARE=bios
++ LIBVIRT_FIRMWARE=bios
++ export LIBVIRT_SECURE_BOOT=false
++ LIBVIRT_SECURE_BOOT=false
++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
++ resultvar=PROVISIONING_CIDR
++ network=172.22.0.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
++ result=24
++ eval PROVISIONING_CIDR=24
+++ PROVISIONING_CIDR=24
++ export PROVISIONING_CIDR
++ export PROVISIONING_CIDR
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").netmask)'
++ export PROVISIONING_NETMASK=255.255.255.0
++ PROVISIONING_NETMASK=255.255.255.0
++ network_address PROVISIONING_IP 172.22.0.0/24 1
++ resultvar=PROVISIONING_IP
++ network=172.22.0.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
++ result=172.22.0.1
++ eval PROVISIONING_IP=172.22.0.1
+++ PROVISIONING_IP=172.22.0.1
++ export PROVISIONING_IP
++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
++ resultvar=CLUSTER_PROVISIONING_IP
++ network=172.22.0.0/24
++ record=2
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
++ result=172.22.0.2
++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
+++ CLUSTER_PROVISIONING_IP=172.22.0.2
++ export CLUSTER_PROVISIONING_IP
++ export PROVISIONING_IP
++ export CLUSTER_PROVISIONING_IP
++ [[ 172.22.0.1 == *\:* ]]
++ export PROVISIONING_URL_HOST=172.22.0.1
++ PROVISIONING_URL_HOST=172.22.0.1
++ export CLUSTER_URL_HOST=172.22.0.2
++ CLUSTER_URL_HOST=172.22.0.2
++ [[ 192.168.111.249 == *\:* ]]
++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ export CLUSTER_APIENDPOINT_PORT=6443
++ CLUSTER_APIENDPOINT_PORT=6443
++ network_address dhcp_range_start 172.22.0.0/24 10
++ resultvar=dhcp_range_start
++ network=172.22.0.0/24
++ record=10
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
++ result=172.22.0.10
++ eval dhcp_range_start=172.22.0.10
+++ dhcp_range_start=172.22.0.10
++ export dhcp_range_start
++ network_address dhcp_range_end 172.22.0.0/24 100
++ resultvar=dhcp_range_end
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval dhcp_range_end=172.22.0.100
+++ dhcp_range_end=172.22.0.100
++ export dhcp_range_end
++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
++ resultvar=PROVISIONING_POOL_RANGE_START
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ PROVISIONING_POOL_RANGE_START=172.22.0.100
++ export PROVISIONING_POOL_RANGE_START
++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
++ resultvar=PROVISIONING_POOL_RANGE_END
++ network=172.22.0.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
++ result=172.22.0.200
++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ PROVISIONING_POOL_RANGE_END=172.22.0.200
++ export PROVISIONING_POOL_RANGE_END
++ export PROVISIONING_POOL_RANGE_START
++ export PROVISIONING_POOL_RANGE_END
++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ EXTERNAL_SUBNET=
++ [[ -n '' ]]
++ export IP_STACK=v4
++ IP_STACK=v4
++ [[ v4 == \v\4 ]]
++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
++ EXTERNAL_SUBNET_V4=192.168.111.0/24
++ export EXTERNAL_SUBNET_V6=
++ EXTERNAL_SUBNET_V6=
++ [[ kind == \m\i\n\i\k\u\b\e ]]
++ [[ -n 192.168.111.0/24 ]]
++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
++ network=192.168.111.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
++ result=24
++ eval EXTERNAL_SUBNET_V4_PREFIX=24
+++ EXTERNAL_SUBNET_V4_PREFIX=24
++ export EXTERNAL_SUBNET_V4_PREFIX
++ export EXTERNAL_SUBNET_V4_PREFIX
++ [[ -z '' ]]
++ network_address EXTERNAL_SUBNET_V4_HOST 192.168.111.0/24 1
++ resultvar=EXTERNAL_SUBNET_V4_HOST
++ network=192.168.111.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 1 - 1, None)))'
++ result=192.168.111.1
++ eval EXTERNAL_SUBNET_V4_HOST=192.168.111.1
+++ EXTERNAL_SUBNET_V4_HOST=192.168.111.1
++ export EXTERNAL_SUBNET_V4_HOST
++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
++ resultvar=VIRSH_DHCP_V4_START
++ network=192.168.111.0/24
++ record=20
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
++ result=192.168.111.20
++ eval VIRSH_DHCP_V4_START=192.168.111.20
+++ VIRSH_DHCP_V4_START=192.168.111.20
++ export VIRSH_DHCP_V4_START
++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
++ resultvar=VIRSH_DHCP_V4_END
++ network=192.168.111.0/24
++ record=60
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
++ result=192.168.111.60
++ eval VIRSH_DHCP_V4_END=192.168.111.60
+++ VIRSH_DHCP_V4_END=192.168.111.60
++ export VIRSH_DHCP_V4_END
++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
++ resultvar=BAREMETALV4_POOL_RANGE_START
++ network=192.168.111.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
++ result=192.168.111.100
++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
++ export BAREMETALV4_POOL_RANGE_START
++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
++ resultvar=BAREMETALV4_POOL_RANGE_END
++ network=192.168.111.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
++ result=192.168.111.200
++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
++ export BAREMETALV4_POOL_RANGE_END
++ export VIRSH_DHCP_V4_START
++ export VIRSH_DHCP_V4_END
++ export BAREMETALV4_POOL_RANGE_START
++ export BAREMETALV4_POOL_RANGE_END
++ [[ -n '' ]]
++ export EXTERNAL_SUBNET_V6_HOST=
++ EXTERNAL_SUBNET_V6_HOST=
++ export EXTERNAL_SUBNET_V6_PREFIX=
++ EXTERNAL_SUBNET_V6_PREFIX=
++ export BAREMETALV6_POOL_RANGE_START=
++ BAREMETALV6_POOL_RANGE_START=
++ export BAREMETALV6_POOL_RANGE_END=
++ BAREMETALV6_POOL_RANGE_END=
++ export REGISTRY_PORT=5000
++ REGISTRY_PORT=5000
++ export HTTP_PORT=6180
++ HTTP_PORT=6180
++ export IRONIC_INSPECTOR_PORT=5050
++ IRONIC_INSPECTOR_PORT=5050
++ export IRONIC_API_PORT=6385
++ IRONIC_API_PORT=6385
++ [[ -n 192.168.111.1 ]]
++ export REGISTRY=192.168.111.1:5000
++ REGISTRY=192.168.111.1:5000
++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
++ resultvar=INITIAL_IRONICBRIDGE_IP
++ network=172.22.0.0/24
++ record=9
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
++ result=172.22.0.9
++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
++ export INITIAL_IRONICBRIDGE_IP
++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ '[' true == true ']'
++ export IRONIC_URL=https://172.22.0.2:6385/v1/
++ IRONIC_URL=https://172.22.0.2:6385/v1/
++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+ sudo '[' '!' -f /root/.ssh/id_rsa_virt_power ']'
+ sudo ssh-keygen -f /root/.ssh/id_rsa_virt_power -P ''
Generating public/private rsa key pair.
Your identification has been saved in /root/.ssh/id_rsa_virt_power
Your public key has been saved in /root/.ssh/id_rsa_virt_power.pub
The key fingerprint is:
SHA256:7CrhDPPWJpobhgpra+OfqW28QDR6m+Tx8/HniTZOvJM root@sidneyshiba-capm3-vm
The key's randomart image is:
+---[RSA 3072]----+
|                 |
|                 |
| o               |
|o .    .         |
|..+     S        |
|.=o=.  o         |
|o.B*oo. +.       |
|o*o=Xoo=Eo..     |
|*+XX.+oo==o      |
+----[SHA256]-----+
+ sudo cat /root/.ssh/id_rsa_virt_power.pub
+ sudo tee -a /root/.ssh/authorized_keys
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCoXPh9A7OGi4WeR8vFLvlXwJLjoIAbtWP2AjRsn2QwLn20sIrtMeAzWJjmnwLG86HKR7pfROa4Fe6A5ZbptigfhEP617iZ7lS7JVdQH8hU0z7aV4hP2SCt/d5eEcLjwz9cCzeVw4J1lZ6cjSTAYiv6psQdaEou7O1woqDIitDHfUSsLeSa2oU6iow0ji4PV4JOtcudJnAN1LZAxuc/gKE0phCEpUWVQQmHHWv5sibik9eNl5kMVuRUhAL266DxSVc1s+vLMVdzoiIUdyaQj/NQQ75oHSaAJWveQaBrKl46BdEP0zOxnDIrNUJmHPzLCMLyzcLRi4krltAyiDTxs/f7zxXtU27AloPmedOIddJw2nYfa9a0hE8A0xkG6Vn9uBUT/NxC8OdQ1TbiFPYYZ/X5flKwaxBbvR65w/+lfNSDNWJqBozuKqXDAeANXKGWgho7wqRLjh7AdGoLqFeIUBfDwglFdJUTOSyJV5iE+47XWaUyvEDRBxjPD9NFQoQr+Qs= root@sidneyshiba-capm3-vm
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e working_dir=/opt/metal3-dev-env -e num_nodes=5 -e extradisks=false -e virthost=sidneyshiba-capm3-vm -e platform=libvirt -e libvirt_firmware=bios -e libvirt_secure_boot=false -e default_memory=4096 -e manage_baremetal=y -e provisioning_url_host=172.22.0.1 -e nodes_file=/opt/metal3-dev-env/ironic_nodes.json -e node_hostname_format=node-%d -i vm-setup/inventory.ini -b vm-setup/setup-playbook.yml
[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names 
to new standard, use callbacks_enabled instead. This feature will be removed 
from ansible-core in version 2.15. Deprecation warnings can be disabled by 
setting deprecation_warnings=False in ansible.cfg.

PLAY [Setup dummy baremetal VMs] ***********************************************
Tuesday 21 December 2021  20:52:55 +0000 (0:00:00.017)       0:00:00.017 ****** 

TASK [Gathering Facts] *********************************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:56 +0000 (0:00:01.316)       0:00:01.333 ****** 

TASK [common : set_fact] *******************************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:56 +0000 (0:00:00.065)       0:00:01.399 ****** 

TASK [common : Set an empty default for vm_nodes if not already defined] *******
ok: [localhost]
Tuesday 21 December 2021  20:52:56 +0000 (0:00:00.050)       0:00:01.449 ****** 

TASK [common : Populate vm_nodes if not already defined] ***********************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/common/tasks/vm_nodes_tasks.yml for localhost => (item={'key': 'node', 'value': {'memory': '4096', 'disk': '50', 'vcpu': '2', 'extradisks': False}})
Tuesday 21 December 2021  20:52:56 +0000 (0:00:00.098)       0:00:01.548 ****** 

TASK [common : set_fact] *******************************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:56 +0000 (0:00:00.064)       0:00:01.613 ****** 

TASK [common : set_fact] *******************************************************
ok: [localhost] => (item=0)
ok: [localhost] => (item=1)
ok: [localhost] => (item=2)
ok: [localhost] => (item=3)
ok: [localhost] => (item=4)
Tuesday 21 December 2021  20:52:56 +0000 (0:00:00.122)       0:00:01.735 ****** 

TASK [common : set_fact] *******************************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.071)       0:00:01.807 ****** 

TASK [common : debug] **********************************************************
ok: [localhost] => {
    "vm_nodes": [
        {
            "flavor": "node",
            "name": "node_0",
            "virtualbmc_port": 6230
        },
        {
            "flavor": "node",
            "name": "node_1",
            "virtualbmc_port": 6231
        },
        {
            "flavor": "node",
            "name": "node_2",
            "virtualbmc_port": 6232
        },
        {
            "flavor": "node",
            "name": "node_3",
            "virtualbmc_port": 6233
        },
        {
            "flavor": "node",
            "name": "node_4",
            "virtualbmc_port": 6234
        }
    ]
}
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.080)       0:00:01.888 ****** 

TASK [common : set_fact] *******************************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.079)       0:00:01.967 ****** 
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.042)       0:00:02.009 ****** 
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.084)       0:00:02.093 ****** 
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.047)       0:00:02.141 ****** 

TASK [libvirt : include_tasks] *************************************************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/install_setup_tasks.yml for localhost
Tuesday 21 December 2021  20:52:57 +0000 (0:00:00.053)       0:00:02.194 ****** 

TASK [libvirt : Start libvirtd] ************************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:58 +0000 (0:00:00.716)       0:00:02.911 ****** 

TASK [libvirt : include_tasks] *************************************************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/network_setup_tasks.yml for localhost
Tuesday 21 December 2021  20:52:58 +0000 (0:00:00.079)       0:00:02.990 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.
Tuesday 21 December 2021  20:52:58 +0000 (0:00:00.133)       0:00:03.123 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.
Tuesday 21 December 2021  20:52:58 +0000 (0:00:00.200)       0:00:03.324 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : get a list of MACs to use] *************************************
ok: [localhost]
Tuesday 21 December 2021  20:52:59 +0000 (0:00:00.470)       0:00:03.794 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : Create libvirt networks] ***************************************
changed: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})
changed: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})
Tuesday 21 December 2021  20:52:59 +0000 (0:00:00.933)       0:00:04.728 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : Start libvirt networks] ****************************************
changed: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})
changed: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})
Tuesday 21 December 2021  20:53:00 +0000 (0:00:00.773)       0:00:05.502 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : Mark  libvirt networks as autostarted] *************************
changed: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})
changed: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})
Tuesday 21 December 2021  20:53:01 +0000 (0:00:00.658)       0:00:06.160 ****** 
Tuesday 21 December 2021  20:53:01 +0000 (0:00:00.046)       0:00:06.207 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.
Tuesday 21 December 2021  20:53:01 +0000 (0:00:00.154)       0:00:06.361 ****** 
Tuesday 21 December 2021  20:53:01 +0000 (0:00:00.111)       0:00:06.472 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.
Tuesday 21 December 2021  20:53:01 +0000 (0:00:00.183)       0:00:06.656 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.
Tuesday 21 December 2021  20:53:02 +0000 (0:00:00.186)       0:00:06.842 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.
Tuesday 21 December 2021  20:53:02 +0000 (0:00:00.166)       0:00:07.009 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : Whitelist bridges for unprivileged access on Ubuntu or Fedora] ***
changed: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})
changed: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})
Tuesday 21 December 2021  20:53:02 +0000 (0:00:00.714)       0:00:07.724 ****** 

TASK [libvirt : Ensure remote working dir exists] ******************************
ok: [localhost]
Tuesday 21 December 2021  20:53:03 +0000 (0:00:00.402)       0:00:08.127 ****** 

TASK [libvirt : include_tasks] *************************************************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/vm_setup_tasks.yml for localhost
Tuesday 21 December 2021  20:53:03 +0000 (0:00:00.084)       0:00:08.211 ****** 

TASK [libvirt : ensure libvirt volume path exists] *****************************
changed: [localhost]
Tuesday 21 December 2021  20:53:03 +0000 (0:00:00.253)       0:00:08.465 ****** 

TASK [libvirt : Check volume pool] *********************************************
task path: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/vm_setup_tasks.yml:12
fatal: [localhost]: FAILED! => {"changed": false, "cmd": ["virsh", "pool-uuid", "oooq_pool"], "delta": "0:00:00.024820", "end": "2021-12-21 20:53:04.087778", "msg": "non-zero return code", "rc": 1, "start": "2021-12-21 20:53:04.062958", "stderr": "error: failed to get pool 'oooq_pool'\nerror: Storage pool not found: no storage pool with matching name 'oooq_pool'", "stderr_lines": ["error: failed to get pool 'oooq_pool'", "error: Storage pool not found: no storage pool with matching name 'oooq_pool'"], "stdout": "", "stdout_lines": []}
...ignoring
Tuesday 21 December 2021  20:53:04 +0000 (0:00:00.433)       0:00:08.898 ****** 

TASK [libvirt : create the volume pool xml file] *******************************
changed: [localhost]
Tuesday 21 December 2021  20:53:04 +0000 (0:00:00.674)       0:00:09.573 ****** 

TASK [libvirt : Define volume pool] ********************************************
changed: [localhost]
Tuesday 21 December 2021  20:53:05 +0000 (0:00:00.307)       0:00:09.880 ****** 

TASK [libvirt : Start volume pool] *********************************************
changed: [localhost]
Tuesday 21 December 2021  20:53:05 +0000 (0:00:00.467)       0:00:10.348 ****** 

TASK [libvirt : ensure tripleo-quickstart volume pool is defined] **************
changed: [localhost]
Tuesday 21 December 2021  20:53:05 +0000 (0:00:00.299)       0:00:10.648 ****** 

TASK [libvirt : Mark volume pool for autostart] ********************************
changed: [localhost]
Tuesday 21 December 2021  20:53:06 +0000 (0:00:00.336)       0:00:10.984 ****** 

TASK [libvirt : Check if vm volumes exist] *************************************
failed: [localhost] (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_0.qcow2"], "delta": "0:00:00.025175", "end": "2021-12-21 20:53:06.495868", "item": {"flavor": "node", "name": "node_0", "virtualbmc_port": 6230}, "msg": "non-zero return code", "rc": 1, "start": "2021-12-21 20:53:06.470693", "stderr": "error: failed to get vol 'node_0.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_0.qcow2'", "stderr_lines": ["error: failed to get vol 'node_0.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_0.qcow2'"], "stdout": "", "stdout_lines": []}
failed: [localhost] (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_1.qcow2"], "delta": "0:00:00.025992", "end": "2021-12-21 20:53:06.740905", "item": {"flavor": "node", "name": "node_1", "virtualbmc_port": 6231}, "msg": "non-zero return code", "rc": 1, "start": "2021-12-21 20:53:06.714913", "stderr": "error: failed to get vol 'node_1.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_1.qcow2'", "stderr_lines": ["error: failed to get vol 'node_1.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_1.qcow2'"], "stdout": "", "stdout_lines": []}
failed: [localhost] (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_2.qcow2"], "delta": "0:00:00.023364", "end": "2021-12-21 20:53:06.963431", "item": {"flavor": "node", "name": "node_2", "virtualbmc_port": 6232}, "msg": "non-zero return code", "rc": 1, "start": "2021-12-21 20:53:06.940067", "stderr": "error: failed to get vol 'node_2.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_2.qcow2'", "stderr_lines": ["error: failed to get vol 'node_2.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_2.qcow2'"], "stdout": "", "stdout_lines": []}
failed: [localhost] (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_3.qcow2"], "delta": "0:00:00.028146", "end": "2021-12-21 20:53:07.259208", "item": {"flavor": "node", "name": "node_3", "virtualbmc_port": 6233}, "msg": "non-zero return code", "rc": 1, "start": "2021-12-21 20:53:07.231062", "stderr": "error: failed to get vol 'node_3.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_3.qcow2'", "stderr_lines": ["error: failed to get vol 'node_3.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_3.qcow2'"], "stdout": "", "stdout_lines": []}
failed: [localhost] (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_4.qcow2"], "delta": "0:00:00.027143", "end": "2021-12-21 20:53:07.492476", "item": {"flavor": "node", "name": "node_4", "virtualbmc_port": 6234}, "msg": "non-zero return code", "rc": 1, "start": "2021-12-21 20:53:07.465333", "stderr": "error: failed to get vol 'node_4.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_4.qcow2'", "stderr_lines": ["error: failed to get vol 'node_4.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_4.qcow2'"], "stdout": "", "stdout_lines": []}
...ignoring
Tuesday 21 December 2021  20:53:07 +0000 (0:00:01.338)       0:00:12.323 ****** 

TASK [libvirt : Create vm vm storage] ******************************************
changed: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_0.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_0.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_0.qcow2'], 'start': '2021-12-21 20:53:06.470693', 'end': '2021-12-21 20:53:06.495868', 'delta': '0:00:00.025175', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_0.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_0.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_0.qcow2'"], 'item': {'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}, 'ansible_loop_var': 'item'})
changed: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_1.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_1.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_1.qcow2'], 'start': '2021-12-21 20:53:06.714913', 'end': '2021-12-21 20:53:06.740905', 'delta': '0:00:00.025992', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_1.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_1.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_1.qcow2'"], 'item': {'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}, 'ansible_loop_var': 'item'})
changed: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_2.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_2.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_2.qcow2'], 'start': '2021-12-21 20:53:06.940067', 'end': '2021-12-21 20:53:06.963431', 'delta': '0:00:00.023364', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_2.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_2.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_2.qcow2'"], 'item': {'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}, 'ansible_loop_var': 'item'})
changed: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_3.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_3.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_3.qcow2'], 'start': '2021-12-21 20:53:07.231062', 'end': '2021-12-21 20:53:07.259208', 'delta': '0:00:00.028146', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_3.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_3.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_3.qcow2'"], 'item': {'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}, 'ansible_loop_var': 'item'})
changed: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_4.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_4.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_4.qcow2'], 'start': '2021-12-21 20:53:07.465333', 'end': '2021-12-21 20:53:07.492476', 'delta': '0:00:00.027143', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_4.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_4.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_4.qcow2'"], 'item': {'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234}, 'ansible_loop_var': 'item'})
Tuesday 21 December 2021  20:53:09 +0000 (0:00:01.474)       0:00:13.798 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : Define vm vms] *************************************************
changed: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})
changed: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})
changed: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})
changed: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})
changed: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})
[WARNING]: 'xml' is given - ignoring 'name'
Tuesday 21 December 2021  20:53:11 +0000 (0:00:02.359)       0:00:16.157 ****** 
Tuesday 21 December 2021  20:53:11 +0000 (0:00:00.173)       0:00:16.330 ****** 
Tuesday 21 December 2021  20:53:11 +0000 (0:00:00.153)       0:00:16.484 ****** 
Tuesday 21 December 2021  20:53:11 +0000 (0:00:00.185)       0:00:16.669 ****** 

TASK [libvirt : Get vm uuid] ***************************************************
changed: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})
changed: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})
changed: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})
changed: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})
changed: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})
Tuesday 21 December 2021  20:53:13 +0000 (0:00:01.174)       0:00:17.844 ****** 

TASK [libvirt : set_fact] ******************************************************
ok: [localhost] => (item={'changed': True, 'stdout': '3f3ba50f-7bf5-4a88-a710-3471c81c3918', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_0'], 'start': '2021-12-21 20:53:12.127726', 'end': '2021-12-21 20:53:12.148988', 'delta': '0:00:00.021262', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_0"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['3f3ba50f-7bf5-4a88-a710-3471c81c3918'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}, 'ansible_loop_var': 'item'})
ok: [localhost] => (item={'changed': True, 'stdout': '28db03d3-e978-4cf9-833d-3aad09af1181', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_1'], 'start': '2021-12-21 20:53:12.347489', 'end': '2021-12-21 20:53:12.368212', 'delta': '0:00:00.020723', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_1"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['28db03d3-e978-4cf9-833d-3aad09af1181'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}, 'ansible_loop_var': 'item'})
ok: [localhost] => (item={'changed': True, 'stdout': '630d70ec-c3f7-4535-9772-1e609c2185cf', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_2'], 'start': '2021-12-21 20:53:12.564623', 'end': '2021-12-21 20:53:12.587910', 'delta': '0:00:00.023287', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_2"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['630d70ec-c3f7-4535-9772-1e609c2185cf'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}, 'ansible_loop_var': 'item'})
ok: [localhost] => (item={'changed': True, 'stdout': '0ca8a968-0ff4-4ee2-891a-35ce1bb6080d', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_3'], 'start': '2021-12-21 20:53:12.780283', 'end': '2021-12-21 20:53:12.801871', 'delta': '0:00:00.021588', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_3"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['0ca8a968-0ff4-4ee2-891a-35ce1bb6080d'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}, 'ansible_loop_var': 'item'})
ok: [localhost] => (item={'changed': True, 'stdout': '4268b271-db25-4ff0-b5f5-73ed5b7b948b', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_4'], 'start': '2021-12-21 20:53:12.998759', 'end': '2021-12-21 20:53:13.021452', 'delta': '0:00:00.022693', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_4"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['4268b271-db25-4ff0-b5f5-73ed5b7b948b'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234}, 'ansible_loop_var': 'item'})
Tuesday 21 December 2021  20:53:13 +0000 (0:00:00.152)       0:00:17.996 ****** 

TASK [libvirt : set_fact BMC Driver] *******************************************
ok: [localhost]
Tuesday 21 December 2021  20:53:13 +0000 (0:00:00.083)       0:00:18.079 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [libvirt : Write ironic node json files] **********************************
changed: [localhost]
Tuesday 21 December 2021  20:53:13 +0000 (0:00:00.517)       0:00:18.597 ****** 
Tuesday 21 December 2021  20:53:13 +0000 (0:00:00.042)       0:00:18.640 ****** 
Tuesday 21 December 2021  20:53:13 +0000 (0:00:00.046)       0:00:18.687 ****** 

TASK [virtbmc : include_tasks] *************************************************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/virtbmc/tasks/setup_tasks.yml for localhost
Tuesday 21 December 2021  20:53:13 +0000 (0:00:00.085)       0:00:18.772 ****** 

TASK [virtbmc : Create VirtualBMC directories] *********************************
changed: [localhost] => (item=/opt/metal3-dev-env/virtualbmc)
changed: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc)
changed: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc/conf)
changed: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc/log)
changed: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/sushy-tools)
Tuesday 21 December 2021  20:53:15 +0000 (0:00:01.072)       0:00:19.844 ****** 

TASK [virtbmc : Create VirtualBMC configuration file] **************************
changed: [localhost]
Tuesday 21 December 2021  20:53:15 +0000 (0:00:00.448)       0:00:20.293 ****** 

TASK [virtbmc : get virthost non_root_user userid] *****************************
changed: [localhost]
Tuesday 21 December 2021  20:53:15 +0000 (0:00:00.294)       0:00:20.587 ****** 

TASK [virtbmc : set fact on non_root_user_uid] *********************************
ok: [localhost]
Tuesday 21 December 2021  20:53:15 +0000 (0:00:00.073)       0:00:20.661 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [virtbmc : set vbmc address (v4) if there is a (nat) network defined with an address] ***
ok: [localhost]
Tuesday 21 December 2021  20:53:16 +0000 (0:00:00.177)       0:00:20.839 ****** 
[WARNING]: The value '' is not a valid IP address or network, passing this
value to ipaddr filter might result in breaking change in future.

TASK [virtbmc : set vbmc address (v6) if there is a (nat) network defined with an address] ***
ok: [localhost]
Tuesday 21 December 2021  20:53:16 +0000 (0:00:00.140)       0:00:20.979 ****** 

TASK [virtbmc : set vbmc address from IPv4 networks if possible, otherwise IPv6] ***
ok: [localhost]
Tuesday 21 December 2021  20:53:16 +0000 (0:00:00.092)       0:00:21.071 ****** 

TASK [virtbmc : set qemu uri for qemu:///system usage] *************************
ok: [localhost]
Tuesday 21 December 2021  20:53:16 +0000 (0:00:00.098)       0:00:21.169 ****** 
Tuesday 21 December 2021  20:53:16 +0000 (0:00:00.071)       0:00:21.240 ****** 

TASK [virtbmc : Create VirtualBMC directories] *********************************
changed: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})
changed: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})
changed: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})
changed: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})
changed: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})
Tuesday 21 December 2021  20:53:17 +0000 (0:00:01.088)       0:00:22.329 ****** 

TASK [virtbmc : Create the Virtual BMCs] ***************************************
changed: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})
changed: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})
changed: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})
changed: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})
changed: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})
Tuesday 21 December 2021  20:53:19 +0000 (0:00:02.002)       0:00:24.332 ****** 

TASK [virtbmc : Create a password file for Redfish Virtual BMCs] ***************
changed: [localhost]
Tuesday 21 December 2021  20:53:20 +0000 (0:00:00.681)       0:00:25.014 ****** 

TASK [virtbmc : Create the Redfish Virtual BMCs] *******************************
changed: [localhost]
Tuesday 21 December 2021  20:53:20 +0000 (0:00:00.422)       0:00:25.436 ****** 

PLAY RECAP *********************************************************************
localhost                  : ok=46   changed=22   unreachable=0    failed=0    skipped=18   rescued=0    ignored=2   

Tuesday 21 December 2021  20:53:20 +0000 (0:00:00.088)       0:00:25.525 ****** 
=============================================================================== 
libvirt : Define vm vms ------------------------------------------------- 2.36s
virtbmc : Create the Virtual BMCs --------------------------------------- 2.00s
libvirt : Create vm vm storage ------------------------------------------ 1.47s
+ sudo virsh pool-uuid default
+ sudo virsh pool-define /dev/stdin
Pool default defined from /dev/stdin

+ sudo virsh pool-start default
Pool default started

+ sudo virsh pool-autostart default
Pool default marked as autostarted

+ [[ ubuntu == ubuntu ]]
+ source ubuntu_bridge_network_configuration.sh
++ set -xe
++ source lib/logging.sh
++++ dirname ./02_configure_host.sh
+++ LOGDIR=./logs
+++ '[' '!' -d ./logs ']'
++++ basename ./02_configure_host.sh .sh
++++ date +%F-%H%M%S
+++ LOGFILE=./logs/02_configure_host-2021-12-21-205320.log
+++ echo 'Logging to ./logs/02_configure_host-2021-12-21-205320.log'
Logging to ./logs/02_configure_host-2021-12-21-205320.log
+++ exec
++++ tee ./logs/02_configure_host-2021-12-21-205320.log
++ source lib/common.sh
+++ [[ :/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
++++ go env
+++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/azureuser/.cache/go-build"
GOENV="/home/azureuser/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/azureuser/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/azureuser/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build3681847654=/tmp/go-build -gno-record-gcc-switches"'
++++ GO111MODULE=
++++ GOARCH=amd64
++++ GOBIN=
++++ GOCACHE=/home/azureuser/.cache/go-build
++++ GOENV=/home/azureuser/.config/go/env
++++ GOEXE=
++++ GOFLAGS=
++++ GOHOSTARCH=amd64
++++ GOHOSTOS=linux
++++ GOINSECURE=
++++ GOMODCACHE=/home/azureuser/go/pkg/mod
++++ GONOPROXY=
++++ GONOSUMDB=
++++ GOOS=linux
++++ GOPATH=/home/azureuser/go
++++ GOPRIVATE=
++++ GOPROXY=https://proxy.golang.org,direct
++++ GOROOT=/usr/local/go
++++ GOSUMDB=sum.golang.org
++++ GOTMPDIR=
++++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
++++ GOVCS=
++++ GOVERSION=go1.16.7
++++ GCCGO=gccgo
++++ AR=ar
++++ CC=gcc
++++ CXX=g++
++++ CGO_ENABLED=1
++++ GOMOD=/dev/null
++++ CGO_CFLAGS='-g -O2'
++++ CGO_CPPFLAGS=
++++ CGO_CXXFLAGS='-g -O2'
++++ CGO_FFLAGS='-g -O2'
++++ CGO_LDFLAGS='-g -O2'
++++ PKG_CONFIG=pkg-config
++++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build3681847654=/tmp/go-build -gno-record-gcc-switches'
+++ export GOPATH
+++++ dirname lib/common.sh
++++ cd lib/..
++++ pwd
+++ SCRIPTDIR=/home/azureuser/projects/metal3-dev-env
++++ whoami
+++ USER=azureuser
+++ export USER=azureuser
+++ USER=azureuser
+++ '[' -z /home/azureuser/projects/metal3-dev-env/config_azureuser.sh ']'
+++ source /home/azureuser/projects/metal3-dev-env/config_azureuser.sh
++++ export KUBECONFIG=/home/azureuser/.kube/config
++++ KUBECONFIG=/home/azureuser/.kube/config
++++ export K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
++++ K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
++++ export IMAGE_OS=Ubuntu
++++ IMAGE_OS=Ubuntu
++++ export EPHEMERAL_CLUSTER=kind
++++ EPHEMERAL_CLUSTER=kind
++++ export CONTAINER_RUNTIME=docker
++++ CONTAINER_RUNTIME=docker
++++ export CAPM3_VERSION=v1beta1
++++ CAPM3_VERSION=v1beta1
++++ export CAPI_VERSION=v1beta1
++++ CAPI_VERSION=v1beta1
++++ export NUM_NODES=5
++++ NUM_NODES=5
++++ export NUM_OF_MASTER_REPLICAS=3
++++ NUM_OF_MASTER_REPLICAS=3
++++ export NUM_OF_WORKER_REPLICAS=2
++++ NUM_OF_WORKER_REPLICAS=2
++++ export KUBERNETES_VERSION=v1.21.1
++++ KUBERNETES_VERSION=v1.21.1
++++ export UPGRADED_K8S_VERSION=v1.22.2
++++ UPGRADED_K8S_VERSION=v1.22.2
+++ export MARIADB_HOST=mariaDB
+++ MARIADB_HOST=mariaDB
+++ export MARIADB_HOST_IP=127.0.0.1
+++ MARIADB_HOST_IP=127.0.0.1
+++ ADDN_DNS=
+++ EXT_IF=
+++ PRO_IF=
+++ MANAGE_BR_BRIDGE=y
+++ MANAGE_PRO_BRIDGE=y
+++ MANAGE_INT_BRIDGE=y
+++ INT_IF=
+++ ROOT_DISK_NAME=/dev/sda
+++ NODE_HOSTNAME_FORMAT=node-%d
+++ source /etc/os-release
++++ NAME=Ubuntu
++++ VERSION='20.04.3 LTS (Focal Fossa)'
++++ ID=ubuntu
++++ ID_LIKE=debian
++++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
++++ VERSION_ID=20.04
++++ HOME_URL=https://www.ubuntu.com/
++++ SUPPORT_URL=https://help.ubuntu.com/
++++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
++++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
++++ VERSION_CODENAME=focal
++++ UBUNTU_CODENAME=focal
+++ export DISTRO=ubuntu20
+++ DISTRO=ubuntu20
+++ export OS=ubuntu
+++ OS=ubuntu
+++ export OS_VERSION_ID=20.04
+++ OS_VERSION_ID=20.04
+++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
+++ export SUPPORTED_DISTROS
+++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
+++ [[ ubuntu == ubuntu ]]
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ [[ docker == \p\o\d\m\a\n ]]
+++ export POD_NAME=
+++ POD_NAME=
+++ export POD_NAME_INFRA=
+++ POD_NAME_INFRA=
+++ export SSH_KEY=/home/azureuser/.ssh/id_rsa
+++ SSH_KEY=/home/azureuser/.ssh/id_rsa
+++ export SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
+++ SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
+++ '[' '!' -f /home/azureuser/.ssh/id_rsa ']'
+++ FILESYSTEM=/
+++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
+++ export CAPM3_VERSION=v1beta1
+++ CAPM3_VERSION=v1beta1
+++ '[' v1beta1 == v1alpha4 ']'
+++ '[' v1beta1 == v1alpha5 ']'
+++ '[' v1beta1 == v1beta1 ']'
+++ export CAPI_VERSION=v1beta1
+++ CAPI_VERSION=v1beta1
+++ export M3PATH=/home/azureuser/go/src/github.com/metal3-io
+++ M3PATH=/home/azureuser/go/src/github.com/metal3-io
+++ export BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
+++ BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
+++ export RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ export CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
+++ CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
+++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
+++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
+++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
+++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
+++ export IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
+++ IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
+++ export IPAM_BASE_URL=metal3-io/ip-address-manager
+++ IPAM_BASE_URL=metal3-io/ip-address-manager
+++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
+++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
+++ '[' v1beta1 == v1alpha3 ']'
+++ '[' v1beta1 == v1alpha4 ']'
+++ IPAMBRANCH=main
+++ IPA_DOWNLOAD_ENABLED=true
+++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
+++ '[' v1beta1 == v1alpha4 ']'
+++ '[' v1beta1 == v1alpha5 ']'
+++ CAPM3BRANCH=main
+++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
+++ BMOBRANCH=master
+++ FORCE_REPO_UPDATE=true
+++ BMOCOMMIT=HEAD
+++ BMO_RUN_LOCAL=false
+++ CAPM3_RUN_LOCAL=false
+++ WORKING_DIR=/opt/metal3-dev-env
+++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
+++ NODES_PLATFORM=libvirt
+++ export NAMESPACE=metal3
+++ NAMESPACE=metal3
+++ export NUM_NODES=5
+++ NUM_NODES=5
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=2
+++ NUM_OF_WORKER_REPLICAS=2
+++ export VM_EXTRADISKS=false
+++ VM_EXTRADISKS=false
+++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
+++ VM_EXTRADISKS_FILE_SYSTEM=ext4
+++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
+++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
+++ export NODE_DRAIN_TIMEOUT=0s
+++ NODE_DRAIN_TIMEOUT=0s
+++ export MAX_SURGE_VALUE=1
+++ MAX_SURGE_VALUE=1
+++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
+++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
+++ export CONTAINER_REGISTRY=quay.io
+++ CONTAINER_REGISTRY=quay.io
+++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
+++ VBMC_IMAGE=quay.io/metal3-io/vbmc
+++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+++ export IRONIC_TLS_SETUP=true
+++ IRONIC_TLS_SETUP=true
+++ export IRONIC_BASIC_AUTH=true
+++ IRONIC_BASIC_AUTH=true
+++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
+++ IRONIC_IMAGE=quay.io/metal3-io/ironic
+++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
+++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
+++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
+++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
+++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
+++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
+++ '[' v1beta1 == v1alpha4 ']'
+++ export IRONIC_NAMESPACE=baremetal-operator-system
+++ IRONIC_NAMESPACE=baremetal-operator-system
+++ export NAMEPREFIX=baremetal-operator
+++ NAMEPREFIX=baremetal-operator
+++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
+++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
+++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
+++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
+++ export OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
+++ OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
+++ '[' v1beta1 == v1alpha4 ']'
+++ '[' v1beta1 == v1alpha5 ']'
+++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
+++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
+++ export DEFAULT_HOSTS_MEMORY=4096
+++ DEFAULT_HOSTS_MEMORY=4096
+++ export CLUSTER_NAME=test1
+++ CLUSTER_NAME=test1
+++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
+++ CLUSTER_APIENDPOINT_IP=192.168.111.249
+++ export KUBERNETES_VERSION=v1.21.1
+++ KUBERNETES_VERSION=v1.21.1
+++ export KUBERNETES_BINARIES_VERSION=v1.21.1
+++ KUBERNETES_BINARIES_VERSION=v1.21.1
+++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
+++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
+++ '[' docker == docker ']'
+++ export EPHEMERAL_CLUSTER=kind
+++ EPHEMERAL_CLUSTER=kind
+++ export KUSTOMIZE_VERSION=v4.1.3
+++ KUSTOMIZE_VERSION=v4.1.3
+++ export KIND_VERSION=v0.11.1
+++ KIND_VERSION=v0.11.1
+++ '[' v1.21.1 == v1.21.2 ']'
+++ export KIND_NODE_IMAGE_VERSION=v1.22.2
+++ KIND_NODE_IMAGE_VERSION=v1.22.2
+++ export MINIKUBE_VERSION=v1.23.2
+++ MINIKUBE_VERSION=v1.23.2
+++ export ANSIBLE_VERSION=4.8.0
+++ ANSIBLE_VERSION=4.8.0
+++ SKIP_RETRIES=false
+++ TEST_TIME_INTERVAL=10
+++ TEST_MAX_TIME=240
+++ FAILS=0
+++ RESULT_STR=
+++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
+++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
+++ '[' 5 -lt 5 ']'
+++ export LIBVIRT_DEFAULT_URI=qemu:///system
+++ LIBVIRT_DEFAULT_URI=qemu:///system
+++ '[' azureuser '!=' root ']'
+++ '[' /run/user/1000 == /run/user/0 ']'
+++ sudo -n uptime
+++ export USE_FIREWALLD=False
+++ USE_FIREWALLD=False
+++ [[ ubuntu20 == \r\h\e\l\8 ]]
+++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
++++ df / --output=fstype
++++ tail -n 1
+++ FSTYPE=ext4
+++ case ${FSTYPE} in
+++ '[' '!' -d /opt/metal3-dev-env ']'
++ source lib/network.sh
+++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
+++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
+++ export POD_CIDR=192.168.0.0/18
+++ POD_CIDR=192.168.0.0/18
+++ PROVISIONING_IPV6=false
+++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
+++ [[ false == \t\r\u\e ]]
+++ export BOOT_MODE=legacy
+++ BOOT_MODE=legacy
+++ export PROVISIONING_NETWORK=172.22.0.0/24
+++ PROVISIONING_NETWORK=172.22.0.0/24
+++ [[ legacy == \l\e\g\a\c\y ]]
+++ export LIBVIRT_FIRMWARE=bios
+++ LIBVIRT_FIRMWARE=bios
+++ export LIBVIRT_SECURE_BOOT=false
+++ LIBVIRT_SECURE_BOOT=false
+++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
+++ resultvar=PROVISIONING_CIDR
+++ network=172.22.0.0/24
++++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
+++ result=24
+++ eval PROVISIONING_CIDR=24
++++ PROVISIONING_CIDR=24
+++ export PROVISIONING_CIDR
+++ export PROVISIONING_CIDR
+++ export PROVISIONING_NETMASK=255.255.255.0
+++ PROVISIONING_NETMASK=255.255.255.0
+++ network_address PROVISIONING_IP 172.22.0.0/24 1
+++ resultvar=PROVISIONING_IP
+++ network=172.22.0.0/24
+++ record=1
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
+++ result=172.22.0.1
+++ eval PROVISIONING_IP=172.22.0.1
++++ PROVISIONING_IP=172.22.0.1
+++ export PROVISIONING_IP
+++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
+++ resultvar=CLUSTER_PROVISIONING_IP
+++ network=172.22.0.0/24
+++ record=2
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
+++ result=172.22.0.2
+++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
++++ CLUSTER_PROVISIONING_IP=172.22.0.2
+++ export CLUSTER_PROVISIONING_IP
+++ export PROVISIONING_IP
+++ export CLUSTER_PROVISIONING_IP
+++ [[ 172.22.0.1 == *\:* ]]
+++ export PROVISIONING_URL_HOST=172.22.0.1
+++ PROVISIONING_URL_HOST=172.22.0.1
+++ export CLUSTER_URL_HOST=172.22.0.2
+++ CLUSTER_URL_HOST=172.22.0.2
+++ [[ 192.168.111.249 == *\:* ]]
+++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
+++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
+++ export CLUSTER_APIENDPOINT_PORT=6443
+++ CLUSTER_APIENDPOINT_PORT=6443
+++ network_address dhcp_range_start 172.22.0.0/24 10
+++ resultvar=dhcp_range_start
+++ network=172.22.0.0/24
+++ record=10
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
+++ result=172.22.0.10
+++ eval dhcp_range_start=172.22.0.10
++++ dhcp_range_start=172.22.0.10
+++ export dhcp_range_start
+++ network_address dhcp_range_end 172.22.0.0/24 100
+++ resultvar=dhcp_range_end
+++ network=172.22.0.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
+++ result=172.22.0.100
+++ eval dhcp_range_end=172.22.0.100
++++ dhcp_range_end=172.22.0.100
+++ export dhcp_range_end
+++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
+++ resultvar=PROVISIONING_POOL_RANGE_START
+++ network=172.22.0.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
+++ result=172.22.0.100
+++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
++++ PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ export PROVISIONING_POOL_RANGE_START
+++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
+++ resultvar=PROVISIONING_POOL_RANGE_END
+++ network=172.22.0.0/24
+++ record=200
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
+++ result=172.22.0.200
+++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
++++ PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ export PROVISIONING_POOL_RANGE_END
+++ export PROVISIONING_POOL_RANGE_START
+++ export PROVISIONING_POOL_RANGE_END
+++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+++ EXTERNAL_SUBNET=
+++ [[ -n '' ]]
+++ export IP_STACK=v4
+++ IP_STACK=v4
+++ [[ v4 == \v\4 ]]
+++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
+++ EXTERNAL_SUBNET_V4=192.168.111.0/24
+++ export EXTERNAL_SUBNET_V6=
+++ EXTERNAL_SUBNET_V6=
+++ [[ kind == \m\i\n\i\k\u\b\e ]]
+++ [[ -n 192.168.111.0/24 ]]
+++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
+++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
+++ network=192.168.111.0/24
++++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
+++ result=24
+++ eval EXTERNAL_SUBNET_V4_PREFIX=24
++++ EXTERNAL_SUBNET_V4_PREFIX=24
+++ export EXTERNAL_SUBNET_V4_PREFIX
+++ export EXTERNAL_SUBNET_V4_PREFIX
+++ [[ -z 192.168.111.1 ]]
+++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
+++ resultvar=VIRSH_DHCP_V4_START
+++ network=192.168.111.0/24
+++ record=20
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
+++ result=192.168.111.20
+++ eval VIRSH_DHCP_V4_START=192.168.111.20
++++ VIRSH_DHCP_V4_START=192.168.111.20
+++ export VIRSH_DHCP_V4_START
+++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
+++ resultvar=VIRSH_DHCP_V4_END
+++ network=192.168.111.0/24
+++ record=60
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
+++ result=192.168.111.60
+++ eval VIRSH_DHCP_V4_END=192.168.111.60
++++ VIRSH_DHCP_V4_END=192.168.111.60
+++ export VIRSH_DHCP_V4_END
+++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
+++ resultvar=BAREMETALV4_POOL_RANGE_START
+++ network=192.168.111.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
+++ result=192.168.111.100
+++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
++++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ export BAREMETALV4_POOL_RANGE_START
+++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
+++ resultvar=BAREMETALV4_POOL_RANGE_END
+++ network=192.168.111.0/24
+++ record=200
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
+++ result=192.168.111.200
+++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
++++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ export BAREMETALV4_POOL_RANGE_END
+++ export VIRSH_DHCP_V4_START
+++ export VIRSH_DHCP_V4_END
+++ export BAREMETALV4_POOL_RANGE_START
+++ export BAREMETALV4_POOL_RANGE_END
+++ [[ -n '' ]]
+++ export EXTERNAL_SUBNET_V6_HOST=
+++ EXTERNAL_SUBNET_V6_HOST=
+++ export EXTERNAL_SUBNET_V6_PREFIX=
+++ EXTERNAL_SUBNET_V6_PREFIX=
+++ export BAREMETALV6_POOL_RANGE_START=
+++ BAREMETALV6_POOL_RANGE_START=
+++ export BAREMETALV6_POOL_RANGE_END=
+++ BAREMETALV6_POOL_RANGE_END=
+++ export REGISTRY_PORT=5000
+++ REGISTRY_PORT=5000
+++ export HTTP_PORT=6180
+++ HTTP_PORT=6180
+++ export IRONIC_INSPECTOR_PORT=5050
+++ IRONIC_INSPECTOR_PORT=5050
+++ export IRONIC_API_PORT=6385
+++ IRONIC_API_PORT=6385
+++ [[ -n 192.168.111.1 ]]
+++ export REGISTRY=192.168.111.1:5000
+++ REGISTRY=192.168.111.1:5000
+++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
+++ resultvar=INITIAL_IRONICBRIDGE_IP
+++ network=172.22.0.0/24
+++ record=9
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
+++ result=172.22.0.9
+++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
++++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ export INITIAL_IRONICBRIDGE_IP
+++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+++ '[' true == true ']'
+++ export IRONIC_URL=https://172.22.0.2:6385/v1/
+++ IRONIC_URL=https://172.22.0.2:6385/v1/
+++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ '[' y == y ']'
++ sudo ip link add ironicendpoint type veth peer name ironic-peer
++ sudo brctl addbr provisioning
++ sudo ip link set provisioning up
++ [[ false == \t\r\u\e ]]
++ sudo ip addr add dev ironicendpoint 172.22.0.1/24
++ sudo brctl addif provisioning ironic-peer
++ sudo ip link set ironicendpoint up
++ sudo ip link set ironic-peer up
++ '[' '' ']'
++ '[' y == y ']'
+++ ip a show baremetal
++ [[ -n 6: baremetal: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:f9:67:fd brd ff:ff:ff:ff:ff:ff
    inet 192.168.111.1/24 brd 192.168.111.255 scope global baremetal
       valid_lft forever preferred_lft forever ]]
++ '[' '' ']'
++ '[' y == y ']'
++ sudo virsh net-destroy baremetal
Network baremetal destroyed

++ sudo virsh net-start baremetal
Network baremetal started

++ '[' '' ']'
+ source disable_apparmor_driver_libvirtd.sh
++ selinux='#security_driver = "selinux"'
++ apparmor='security_driver = "apparmor"'
++ none='security_driver = "none"'
++ sudo sed -i 's/#security_driver = "selinux"/security_driver = "none"/g' /etc/libvirt/qemu.conf
++ sudo sed -i 's/security_driver = "apparmor"/security_driver = "none"/g' /etc/libvirt/qemu.conf
++ sudo systemctl restart libvirtd
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e '{use_firewalld: False}' -e 'external_subnet_v4: 192.168.111.0/24' -i vm-setup/inventory.ini -b vm-setup/firewall.yml
[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names 
to new standard, use callbacks_enabled instead. This feature will be removed 
from ansible-core in version 2.15. Deprecation warnings can be disabled by 
setting deprecation_warnings=False in ansible.cfg.

PLAY [Setup dummy baremetal VMs] ***********************************************
Tuesday 21 December 2021  20:53:22 +0000 (0:00:00.048)       0:00:00.048 ****** 

TASK [Gathering Facts] *********************************************************
ok: [localhost]
Tuesday 21 December 2021  20:53:23 +0000 (0:00:01.333)       0:00:01.381 ****** 
Tuesday 21 December 2021  20:53:23 +0000 (0:00:00.037)       0:00:01.419 ****** 

TASK [firewall : iptables] *****************************************************
included: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/firewall/tasks/iptables.yaml for localhost
Tuesday 21 December 2021  20:53:24 +0000 (0:00:00.051)       0:00:01.470 ****** 

TASK [firewall : iptables: Firewalld service stopped] **************************
task path: /home/azureuser/projects/metal3-dev-env/vm-setup/roles/firewall/tasks/iptables.yaml:1
fatal: [localhost]: FAILED! => {"changed": false, "msg": "Could not find the requested service firewalld: host"}
...ignoring
Tuesday 21 December 2021  20:53:24 +0000 (0:00:00.638)       0:00:02.109 ****** 

TASK [firewall : iptables: VBMC Ports] *****************************************
changed: [localhost]
Tuesday 21 December 2021  20:53:25 +0000 (0:00:00.393)       0:00:02.503 ****** 

TASK [firewall : iptables: sushy Port] *****************************************
changed: [localhost]
Tuesday 21 December 2021  20:53:25 +0000 (0:00:00.282)       0:00:02.785 ****** 

TASK [firewall : iptables: Established and related] ****************************
changed: [localhost]
Tuesday 21 December 2021  20:53:25 +0000 (0:00:00.270)       0:00:03.055 ****** 

TASK [firewall : iptables: Ironic Ports] ***************************************
changed: [localhost] => (item=6180)
changed: [localhost] => (item=5050)
changed: [localhost] => (item=6385)
changed: [localhost] => (item=9999)
changed: [localhost] => (item=80)
Tuesday 21 December 2021  20:53:26 +0000 (0:00:01.192)       0:00:04.248 ****** 

TASK [firewall : iptables: Provisioning host ports] ****************************
changed: [localhost] => (item=80)
changed: [localhost] => (item=5000)
changed: [localhost] => (item=53)
Tuesday 21 December 2021  20:53:27 +0000 (0:00:00.742)       0:00:04.990 ****** 

TASK [firewall : iptables: PXE Ports] ******************************************
changed: [localhost] => (item=5353)
changed: [localhost] => (item=67)
changed: [localhost] => (item=68)
changed: [localhost] => (item=546)
changed: [localhost] => (item=547)
changed: [localhost] => (item=69)
Tuesday 21 December 2021  20:53:28 +0000 (0:00:01.458)       0:00:06.449 ****** 

TASK [firewall : iptables: Ironic Endpoint Keepalived] *************************
changed: [localhost] => (item=112)
changed: [localhost] => (item=icmp)
Tuesday 21 December 2021  20:53:29 +0000 (0:00:00.602)       0:00:07.051 ****** 

TASK [firewall : iptables: Allow access to baremetal network from kind] ********
changed: [localhost]

PLAY RECAP *********************************************************************
localhost                  : ok=11   changed=8    unreachable=0    failed=0    skipped=1    rescued=0    ignored=1   

Tuesday 21 December 2021  20:53:29 +0000 (0:00:00.326)       0:00:07.378 ****** 
=============================================================================== 
firewall : iptables: PXE Ports ------------------------------------------ 1.46s
Gathering Facts --------------------------------------------------------- 1.33s
firewall : iptables: Ironic Ports --------------------------------------- 1.19s
+ '[' False == True ']'
+ '[' '' ']'
++ sudo docker inspect registry --format '{{.State.Status}}'
Error: No such object: registry
++ echo error
+ reg_state='
error'
+ [[ 
error == \e\x\i\t\e\d ]]
+ [[ 
error != \r\u\n\n\i\n\g ]]
+ sudo docker rm registry -f
Error: No such container: registry
+ sudo docker run -d -p 5000:5000 --name registry registry:2.7.1
7808d3f9364f948c6996304be04f26c9fa1f129e4514120891cca91c55dc6e55
+ sleep 5
++ env
++ grep -v _LOCAL_IMAGE=
++ grep _IMAGE=
++ grep -o '^[^=]*'
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=registry:2.7.1
+ IMAGE_NAME=registry:2.7.1
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
+ sudo docker tag registry:2.7.1 192.168.111.1:5000/localimages/registry:2.7.1
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/registry:2.7.1
The push refers to repository [192.168.111.1:5000/localimages/registry]
aeccf26589a7: Preparing
f640be0d5aad: Preparing
aa4330046b37: Preparing
ad10b481abe7: Preparing
69715584ec78: Preparing
aeccf26589a7: Pushed
f640be0d5aad: Pushed
ad10b481abe7: Pushed
69715584ec78: Pushed
aa4330046b37: Pushed
2.7.1: digest: sha256:36cb5b157911061fb610d8884dc09e0b0300a767a350563cbfd88b4b85324ce4 size: 1363
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/baremetal-operator
+ IMAGE_NAME=baremetal-operator
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
+ sudo docker tag quay.io/metal3-io/baremetal-operator 192.168.111.1:5000/localimages/baremetal-operator
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/baremetal-operator
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/baremetal-operator]
21bf11a603aa: Preparing
0b3d0512394d: Preparing
6d75f23be3dd: Preparing
6d75f23be3dd: Pushed
0b3d0512394d: Pushed
21bf11a603aa: Pushed
latest: digest: sha256:b1ab481ff5db5d92e0eaafd3a46910fb96ca6f0dc47adcfa97cdab5c0bb4d0a2 size: 950
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-client
+ IMAGE_NAME=ironic-client
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-client
+ sudo docker tag quay.io/metal3-io/ironic-client 192.168.111.1:5000/localimages/ironic-client
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic-client
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic-client]
297a1a71e453: Preparing
1591f6d87eef: Preparing
2653d992f4ef: Preparing
1591f6d87eef: Pushed
297a1a71e453: Pushed
2653d992f4ef: Pushed
latest: digest: sha256:0abe9a3de15449f9cb7c8ec3daa5dceaf124c2e1705c51ef73dfc54f92dacec4 size: 948
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic
+ IMAGE_NAME=ironic
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic
+ sudo docker tag quay.io/metal3-io/ironic 192.168.111.1:5000/localimages/ironic
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic]
2887ce1a05ad: Preparing
3f5658f85430: Preparing
73b945a47080: Preparing
3f7e15bb91a1: Preparing
20adf6e320d7: Preparing
c33007d20ef1: Preparing
0732b02685ef: Preparing
06759e89fc02: Preparing
5d28ad2d9d5c: Preparing
523826c0e215: Preparing
f6a7b8b5934b: Preparing
f1a4c011277e: Preparing
8514271a8c2c: Preparing
a9a75d53f23a: Preparing
5f83bad8acd2: Preparing
aaf1c228b4da: Preparing
95c51ae9ba96: Preparing
0732b02685ef: Waiting
e7a4bda8f16d: Preparing
06759e89fc02: Waiting
525ed45dbdb1: Preparing
5bc03dec6239: Preparing
5d28ad2d9d5c: Waiting
523826c0e215: Waiting
95c51ae9ba96: Waiting
e7a4bda8f16d: Waiting
c33007d20ef1: Waiting
aaf1c228b4da: Waiting
525ed45dbdb1: Waiting
5bc03dec6239: Waiting
f1a4c011277e: Waiting
f6a7b8b5934b: Waiting
8514271a8c2c: Waiting
a9a75d53f23a: Waiting
5f83bad8acd2: Waiting
20adf6e320d7: Pushed
3f5658f85430: Pushed
3f7e15bb91a1: Pushed
2887ce1a05ad: Pushed
c33007d20ef1: Pushed
0732b02685ef: Pushed
5d28ad2d9d5c: Pushed
06759e89fc02: Pushed
523826c0e215: Pushed
8514271a8c2c: Pushed
f1a4c011277e: Pushed
5f83bad8acd2: Pushed
f6a7b8b5934b: Pushed
aaf1c228b4da: Pushed
525ed45dbdb1: Pushed
73b945a47080: Pushed
e7a4bda8f16d: Pushed
95c51ae9ba96: Pushed
5bc03dec6239: Pushed
a9a75d53f23a: Pushed
latest: digest: sha256:8bd2bcebdee01784b5b1318a074c5fe5750760411ad5ee3a19e47dca1685fc35 size: 4496
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ IMAGE_NAME=ironic-ipa-downloader
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ sudo docker tag quay.io/metal3-io/ironic-ipa-downloader 192.168.111.1:5000/localimages/ironic-ipa-downloader
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic-ipa-downloader
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic-ipa-downloader]
ec89db9873bd: Preparing
17be93712683: Preparing
2653d992f4ef: Preparing
2653d992f4ef: Mounted from localimages/ironic-client
ec89db9873bd: Pushed
17be93712683: Pushed
latest: digest: sha256:d2d871675b629bf66514ccda2e2616c50670f7fff9d95b983a216f3a7fdaa1aa size: 948
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/sushy-tools
+ IMAGE_NAME=sushy-tools
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/sushy-tools
+ sudo docker tag quay.io/metal3-io/sushy-tools 192.168.111.1:5000/localimages/sushy-tools
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/sushy-tools
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/sushy-tools]
238f2ee898ef: Preparing
580285e1db2a: Preparing
4a727df1bd7d: Preparing
b9800a668b57: Preparing
3ff9c5f71066: Preparing
d4151e7436b1: Preparing
3b441d7cb46b: Preparing
d3710de04cb3: Preparing
91f7336bbfff: Preparing
e2e8c39e0f77: Preparing
d3710de04cb3: Waiting
91f7336bbfff: Waiting
e2e8c39e0f77: Waiting
d4151e7436b1: Waiting
3b441d7cb46b: Waiting
4a727df1bd7d: Pushed
580285e1db2a: Pushed
3ff9c5f71066: Pushed
d3710de04cb3: Pushed
b9800a668b57: Pushed
91f7336bbfff: Pushed
238f2ee898ef: Pushed
3b441d7cb46b: Pushed
e2e8c39e0f77: Pushed
d4151e7436b1: Pushed
latest: digest: sha256:f90916d628816257566b92a5e90058fb32781b0a95c95c96dd6d7920423bc5ba size: 2430
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/keepalived
+ IMAGE_NAME=keepalived
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/keepalived
+ sudo docker tag quay.io/metal3-io/keepalived 192.168.111.1:5000/localimages/keepalived
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/keepalived
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/keepalived]
366f541fef14: Preparing
c94b2040d772: Preparing
309a780d3dd8: Preparing
9f54eef41275: Preparing
309a780d3dd8: Pushed
c94b2040d772: Pushed
9f54eef41275: Pushed
366f541fef14: Pushed
latest: digest: sha256:4d2d44db445e898a08b072a29af18c325f92a06508b720ea9c95ecddc09c942c size: 1155
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/vbmc
+ IMAGE_NAME=vbmc
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/vbmc
+ sudo docker tag quay.io/metal3-io/vbmc 192.168.111.1:5000/localimages/vbmc
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/vbmc
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/vbmc]
091a214fa651: Preparing
95c51ae9ba96: Preparing
e7a4bda8f16d: Preparing
525ed45dbdb1: Preparing
5bc03dec6239: Preparing
e7a4bda8f16d: Mounted from localimages/ironic
95c51ae9ba96: Mounted from localimages/ironic
5bc03dec6239: Mounted from localimages/ironic
525ed45dbdb1: Mounted from localimages/ironic
091a214fa651: Pushed
latest: digest: sha256:859a7038a299476b7f0c402869b34c1be58262c0bfbfdb6dda6c062c0af63b36 size: 1373
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ip-address-manager:main
+ IMAGE_NAME=ip-address-manager:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:main
+ sudo docker tag quay.io/metal3-io/ip-address-manager:main 192.168.111.1:5000/localimages/ip-address-manager:main
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ip-address-manager:main
The push refers to repository [192.168.111.1:5000/localimages/ip-address-manager]
13e6a06d9485: Preparing
6d75f23be3dd: Preparing
6d75f23be3dd: Mounted from localimages/baremetal-operator
13e6a06d9485: Pushed
main: digest: sha256:dc8880f770bbf91fdaf8379be5ecc94ccf9ab3ab0b8d281a26bc522cda40590f size: 739
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ IMAGE_NAME=cluster-api-provider-metal3:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ sudo docker tag quay.io/metal3-io/cluster-api-provider-metal3:main 192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
The push refers to repository [192.168.111.1:5000/localimages/cluster-api-provider-metal3]
519c67b16402: Preparing
6d75f23be3dd: Preparing
6d75f23be3dd: Mounted from localimages/ip-address-manager
519c67b16402: Pushed
main: digest: sha256:ac11ca138767da6c9f9d453eed351a2f092435a9fb214511eb081247c361d000 size: 739
++ env
++ grep _LOCAL_IMAGE=
++ grep -o '^[^=]*'
+ IRONIC_IMAGE=quay.io/metal3-io/ironic
+ VBMC_IMAGE=quay.io/metal3-io/vbmc
+ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+ [[ ubuntu == ubuntu ]]
+ sudo docker run -d --net host --privileged --name httpd-infra -v /opt/metal3-dev-env/ironic:/shared --entrypoint /bin/runhttpd --env PROVISIONING_INTERFACE=ironicendpoint quay.io/metal3-io/ironic
1f2631fec05687cf749bf8d78a7e47d6e3a7cdcc6b8a75fb7ee7a8ca386fa9f8
+ sudo docker run -d --net host --name vbmc -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/.vbmc -v /root/.ssh:/root/ssh quay.io/metal3-io/vbmc
19ff805b2a3c53a6b8b34011b0d7043f25c67f3363dd1f88758f0483ceabcd00
+ sudo docker run -d --net host --name sushy-tools -v /opt/metal3-dev-env/virtualbmc/sushy-tools:/root/sushy -v /root/.ssh:/root/ssh quay.io/metal3-io/sushy-tools
4d2e45475a76d8f2b7c303f72da2222f1a6a94d51188812858cabbc366b365be
+ OPENSTACKCLIENT_PATH=/usr/local/bin/openstack
+ command -v openstack
+ grep -v /usr/local/bin/openstack
+ sudo ln -sf /home/azureuser/projects/metal3-dev-env/openstackclient.sh /usr/local/bin/openstack
++ dirname /usr/local/bin/openstack
+ sudo ln -sf /home/azureuser/projects/metal3-dev-env/openstackclient.sh /usr/local/bin/baremetal
+ VBMC_PATH=/usr/local/bin/vbmc
+ command -v vbmc
+ grep -v /usr/local/bin/vbmc
+ sudo ln -sf /home/azureuser/projects/metal3-dev-env/vbmc.sh /usr/local/bin/vbmc
./03_launch_mgmt_cluster.sh
+ source lib/logging.sh
+++ dirname ./03_launch_mgmt_cluster.sh
++ LOGDIR=./logs
++ '[' '!' -d ./logs ']'
+++ basename ./03_launch_mgmt_cluster.sh .sh
+++ date +%F-%H%M%S
++ LOGFILE=./logs/03_launch_mgmt_cluster-2021-12-21-205526.log
++ echo 'Logging to ./logs/03_launch_mgmt_cluster-2021-12-21-205526.log'
Logging to ./logs/03_launch_mgmt_cluster-2021-12-21-205526.log
++ exec
+++ tee ./logs/03_launch_mgmt_cluster-2021-12-21-205526.log
+ source lib/common.sh
++ [[ :/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
++ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin
+++ go env
++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/azureuser/.cache/go-build"
GOENV="/home/azureuser/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/azureuser/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/azureuser/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build2513133715=/tmp/go-build -gno-record-gcc-switches"'
+++ GO111MODULE=
+++ GOARCH=amd64
+++ GOBIN=
+++ GOCACHE=/home/azureuser/.cache/go-build
+++ GOENV=/home/azureuser/.config/go/env
+++ GOEXE=
+++ GOFLAGS=
+++ GOHOSTARCH=amd64
+++ GOHOSTOS=linux
+++ GOINSECURE=
+++ GOMODCACHE=/home/azureuser/go/pkg/mod
+++ GONOPROXY=
+++ GONOSUMDB=
+++ GOOS=linux
+++ GOPATH=/home/azureuser/go
+++ GOPRIVATE=
+++ GOPROXY=https://proxy.golang.org,direct
+++ GOROOT=/usr/local/go
+++ GOSUMDB=sum.golang.org
+++ GOTMPDIR=
+++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
+++ GOVCS=
+++ GOVERSION=go1.16.7
+++ GCCGO=gccgo
+++ AR=ar
+++ CC=gcc
+++ CXX=g++
+++ CGO_ENABLED=1
+++ GOMOD=/dev/null
+++ CGO_CFLAGS='-g -O2'
+++ CGO_CPPFLAGS=
+++ CGO_CXXFLAGS='-g -O2'
+++ CGO_FFLAGS='-g -O2'
+++ CGO_LDFLAGS='-g -O2'
+++ PKG_CONFIG=pkg-config
+++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build2513133715=/tmp/go-build -gno-record-gcc-switches'
++ export GOPATH
++++ dirname lib/common.sh
+++ cd lib/..
+++ pwd
++ SCRIPTDIR=/home/azureuser/projects/metal3-dev-env
+++ whoami
++ USER=azureuser
++ export USER=azureuser
++ USER=azureuser
++ '[' -z '' ']'
++ '[' '!' -f /home/azureuser/projects/metal3-dev-env/config_azureuser.sh ']'
++ CONFIG=/home/azureuser/projects/metal3-dev-env/config_azureuser.sh
++ source /home/azureuser/projects/metal3-dev-env/config_azureuser.sh
+++ export KUBECONFIG=/home/azureuser/.kube/config
+++ KUBECONFIG=/home/azureuser/.kube/config
+++ export K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
+++ K8S_AUTH_KUBECONFIG=/home/azureuser/.kube/config
+++ export IMAGE_OS=Ubuntu
+++ IMAGE_OS=Ubuntu
+++ export EPHEMERAL_CLUSTER=kind
+++ EPHEMERAL_CLUSTER=kind
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ export CAPM3_VERSION=v1beta1
+++ CAPM3_VERSION=v1beta1
+++ export CAPI_VERSION=v1beta1
+++ CAPI_VERSION=v1beta1
+++ export NUM_NODES=5
+++ NUM_NODES=5
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=2
+++ NUM_OF_WORKER_REPLICAS=2
+++ export KUBERNETES_VERSION=v1.21.1
+++ KUBERNETES_VERSION=v1.21.1
+++ export UPGRADED_K8S_VERSION=v1.22.2
+++ UPGRADED_K8S_VERSION=v1.22.2
++ export MARIADB_HOST=mariaDB
++ MARIADB_HOST=mariaDB
++ export MARIADB_HOST_IP=127.0.0.1
++ MARIADB_HOST_IP=127.0.0.1
++ ADDN_DNS=
++ EXT_IF=
++ PRO_IF=
++ MANAGE_BR_BRIDGE=y
++ MANAGE_PRO_BRIDGE=y
++ MANAGE_INT_BRIDGE=y
++ INT_IF=
++ ROOT_DISK_NAME=/dev/sda
++ NODE_HOSTNAME_FORMAT=node-%d
++ source /etc/os-release
+++ NAME=Ubuntu
+++ VERSION='20.04.3 LTS (Focal Fossa)'
+++ ID=ubuntu
+++ ID_LIKE=debian
+++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
+++ VERSION_ID=20.04
+++ HOME_URL=https://www.ubuntu.com/
+++ SUPPORT_URL=https://help.ubuntu.com/
+++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
+++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
+++ VERSION_CODENAME=focal
+++ UBUNTU_CODENAME=focal
++ export DISTRO=ubuntu20
++ DISTRO=ubuntu20
++ export OS=ubuntu
++ OS=ubuntu
++ export OS_VERSION_ID=20.04
++ OS_VERSION_ID=20.04
++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
++ export SUPPORTED_DISTROS
++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
++ [[ ubuntu == ubuntu ]]
++ export CONTAINER_RUNTIME=docker
++ CONTAINER_RUNTIME=docker
++ [[ docker == \p\o\d\m\a\n ]]
++ export POD_NAME=
++ POD_NAME=
++ export POD_NAME_INFRA=
++ POD_NAME_INFRA=
++ export SSH_KEY=/home/azureuser/.ssh/id_rsa
++ SSH_KEY=/home/azureuser/.ssh/id_rsa
++ export SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
++ SSH_PUB_KEY=/home/azureuser/.ssh/id_rsa.pub
++ '[' '!' -f /home/azureuser/.ssh/id_rsa ']'
++ FILESYSTEM=/
++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
++ export CAPM3_VERSION=v1beta1
++ CAPM3_VERSION=v1beta1
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ '[' v1beta1 == v1beta1 ']'
++ export CAPI_VERSION=v1beta1
++ CAPI_VERSION=v1beta1
++ export M3PATH=/home/azureuser/go/src/github.com/metal3-io
++ M3PATH=/home/azureuser/go/src/github.com/metal3-io
++ export BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
++ BMOPATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
++ export RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ RUN_LOCAL_IRONIC_SCRIPT=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ export CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ export IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
++ IPAMPATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
++ export IPAM_BASE_URL=metal3-io/ip-address-manager
++ IPAM_BASE_URL=metal3-io/ip-address-manager
++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ '[' v1beta1 == v1alpha3 ']'
++ '[' v1beta1 == v1alpha4 ']'
++ IPAMBRANCH=main
++ IPA_DOWNLOAD_ENABLED=true
++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ CAPM3BRANCH=main
++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
++ BMOBRANCH=master
++ FORCE_REPO_UPDATE=true
++ BMOCOMMIT=HEAD
++ BMO_RUN_LOCAL=false
++ CAPM3_RUN_LOCAL=false
++ WORKING_DIR=/opt/metal3-dev-env
++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
++ NODES_PLATFORM=libvirt
++ export NAMESPACE=metal3
++ NAMESPACE=metal3
++ export NUM_NODES=5
++ NUM_NODES=5
++ export NUM_OF_MASTER_REPLICAS=3
++ NUM_OF_MASTER_REPLICAS=3
++ export NUM_OF_WORKER_REPLICAS=2
++ NUM_OF_WORKER_REPLICAS=2
++ export VM_EXTRADISKS=false
++ VM_EXTRADISKS=false
++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
++ VM_EXTRADISKS_FILE_SYSTEM=ext4
++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ export NODE_DRAIN_TIMEOUT=0s
++ NODE_DRAIN_TIMEOUT=0s
++ export MAX_SURGE_VALUE=1
++ MAX_SURGE_VALUE=1
++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ export CONTAINER_REGISTRY=quay.io
++ CONTAINER_REGISTRY=quay.io
++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
++ VBMC_IMAGE=quay.io/metal3-io/vbmc
++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ export IRONIC_TLS_SETUP=true
++ IRONIC_TLS_SETUP=true
++ export IRONIC_BASIC_AUTH=true
++ IRONIC_BASIC_AUTH=true
++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
++ IRONIC_IMAGE=quay.io/metal3-io/ironic
++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ '[' v1beta1 == v1alpha4 ']'
++ export IRONIC_NAMESPACE=baremetal-operator-system
++ IRONIC_NAMESPACE=baremetal-operator-system
++ export NAMEPREFIX=baremetal-operator
++ NAMEPREFIX=baremetal-operator
++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ export OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
++ OPENSTACK_CONFIG=/home/azureuser/.config/openstack/clouds.yaml
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ export DEFAULT_HOSTS_MEMORY=4096
++ DEFAULT_HOSTS_MEMORY=4096
++ export CLUSTER_NAME=test1
++ CLUSTER_NAME=test1
++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
++ CLUSTER_APIENDPOINT_IP=192.168.111.249
++ export KUBERNETES_VERSION=v1.21.1
++ KUBERNETES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_VERSION=v1.21.1
++ KUBERNETES_BINARIES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ '[' docker == docker ']'
++ export EPHEMERAL_CLUSTER=kind
++ EPHEMERAL_CLUSTER=kind
++ export KUSTOMIZE_VERSION=v4.1.3
++ KUSTOMIZE_VERSION=v4.1.3
++ export KIND_VERSION=v0.11.1
++ KIND_VERSION=v0.11.1
++ '[' v1.21.1 == v1.21.2 ']'
++ export KIND_NODE_IMAGE_VERSION=v1.22.2
++ KIND_NODE_IMAGE_VERSION=v1.22.2
++ export MINIKUBE_VERSION=v1.23.2
++ MINIKUBE_VERSION=v1.23.2
++ export ANSIBLE_VERSION=4.8.0
++ ANSIBLE_VERSION=4.8.0
++ SKIP_RETRIES=false
++ TEST_TIME_INTERVAL=10
++ TEST_MAX_TIME=240
++ FAILS=0
++ RESULT_STR=
++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ '[' 5 -lt 5 ']'
++ export LIBVIRT_DEFAULT_URI=qemu:///system
++ LIBVIRT_DEFAULT_URI=qemu:///system
++ '[' azureuser '!=' root ']'
++ '[' /run/user/1000 == /run/user/0 ']'
++ sudo -n uptime
++ export USE_FIREWALLD=False
++ USE_FIREWALLD=False
++ [[ ubuntu20 == \r\h\e\l\8 ]]
++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
+++ df / --output=fstype
+++ tail -n 1
++ FSTYPE=ext4
++ case ${FSTYPE} in
++ '[' '!' -d /opt/metal3-dev-env ']'
+ source lib/releases.sh
++ CAPM3RELEASEPATH=https://api.github.com/repos/metal3-io/cluster-api-provider-metal3/releases
++ CAPIRELEASEPATH=https://api.github.com/repos/kubernetes-sigs/cluster-api/releases
++ '[' v1beta1 == v1alpha3 ']'
++ '[' v1beta1 == v1alpha4 ']'
+++ get_latest_release https://api.github.com/repos/kubernetes-sigs/cluster-api/releases v1.0.
+++ set +x
+++ echo v1.0.2
++ export CAPIRELEASE=v1.0.2
++ CAPIRELEASE=v1.0.2
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ export CAPM3RELEASE=v1.0.0
++ CAPM3RELEASE=v1.0.0
++ [[ v1.0.2 == '' ]]
++ [[ v1.0.0 == '' ]]
+ source lib/network.sh
++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ export POD_CIDR=192.168.0.0/18
++ POD_CIDR=192.168.0.0/18
++ PROVISIONING_IPV6=false
++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
++ [[ false == \t\r\u\e ]]
++ export BOOT_MODE=legacy
++ BOOT_MODE=legacy
++ export PROVISIONING_NETWORK=172.22.0.0/24
++ PROVISIONING_NETWORK=172.22.0.0/24
++ [[ legacy == \l\e\g\a\c\y ]]
++ export LIBVIRT_FIRMWARE=bios
++ LIBVIRT_FIRMWARE=bios
++ export LIBVIRT_SECURE_BOOT=false
++ LIBVIRT_SECURE_BOOT=false
++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
++ resultvar=PROVISIONING_CIDR
++ network=172.22.0.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
++ result=24
++ eval PROVISIONING_CIDR=24
+++ PROVISIONING_CIDR=24
++ export PROVISIONING_CIDR
++ export PROVISIONING_CIDR
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").netmask)'
++ export PROVISIONING_NETMASK=255.255.255.0
++ PROVISIONING_NETMASK=255.255.255.0
++ network_address PROVISIONING_IP 172.22.0.0/24 1
++ resultvar=PROVISIONING_IP
++ network=172.22.0.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
++ result=172.22.0.1
++ eval PROVISIONING_IP=172.22.0.1
+++ PROVISIONING_IP=172.22.0.1
++ export PROVISIONING_IP
++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
++ resultvar=CLUSTER_PROVISIONING_IP
++ network=172.22.0.0/24
++ record=2
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
++ result=172.22.0.2
++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
+++ CLUSTER_PROVISIONING_IP=172.22.0.2
++ export CLUSTER_PROVISIONING_IP
++ export PROVISIONING_IP
++ export CLUSTER_PROVISIONING_IP
++ [[ 172.22.0.1 == *\:* ]]
++ export PROVISIONING_URL_HOST=172.22.0.1
++ PROVISIONING_URL_HOST=172.22.0.1
++ export CLUSTER_URL_HOST=172.22.0.2
++ CLUSTER_URL_HOST=172.22.0.2
++ [[ 192.168.111.249 == *\:* ]]
++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ export CLUSTER_APIENDPOINT_PORT=6443
++ CLUSTER_APIENDPOINT_PORT=6443
++ network_address dhcp_range_start 172.22.0.0/24 10
++ resultvar=dhcp_range_start
++ network=172.22.0.0/24
++ record=10
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
++ result=172.22.0.10
++ eval dhcp_range_start=172.22.0.10
+++ dhcp_range_start=172.22.0.10
++ export dhcp_range_start
++ network_address dhcp_range_end 172.22.0.0/24 100
++ resultvar=dhcp_range_end
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval dhcp_range_end=172.22.0.100
+++ dhcp_range_end=172.22.0.100
++ export dhcp_range_end
++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
++ resultvar=PROVISIONING_POOL_RANGE_START
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ PROVISIONING_POOL_RANGE_START=172.22.0.100
++ export PROVISIONING_POOL_RANGE_START
++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
++ resultvar=PROVISIONING_POOL_RANGE_END
++ network=172.22.0.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
++ result=172.22.0.200
++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ PROVISIONING_POOL_RANGE_END=172.22.0.200
++ export PROVISIONING_POOL_RANGE_END
++ export PROVISIONING_POOL_RANGE_START
++ export PROVISIONING_POOL_RANGE_END
++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ EXTERNAL_SUBNET=
++ [[ -n '' ]]
++ export IP_STACK=v4
++ IP_STACK=v4
++ [[ v4 == \v\4 ]]
++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
++ EXTERNAL_SUBNET_V4=192.168.111.0/24
++ export EXTERNAL_SUBNET_V6=
++ EXTERNAL_SUBNET_V6=
++ [[ kind == \m\i\n\i\k\u\b\e ]]
++ [[ -n 192.168.111.0/24 ]]
++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
++ network=192.168.111.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
++ result=24
++ eval EXTERNAL_SUBNET_V4_PREFIX=24
+++ EXTERNAL_SUBNET_V4_PREFIX=24
++ export EXTERNAL_SUBNET_V4_PREFIX
++ export EXTERNAL_SUBNET_V4_PREFIX
++ [[ -z '' ]]
++ network_address EXTERNAL_SUBNET_V4_HOST 192.168.111.0/24 1
++ resultvar=EXTERNAL_SUBNET_V4_HOST
++ network=192.168.111.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 1 - 1, None)))'
++ result=192.168.111.1
++ eval EXTERNAL_SUBNET_V4_HOST=192.168.111.1
+++ EXTERNAL_SUBNET_V4_HOST=192.168.111.1
++ export EXTERNAL_SUBNET_V4_HOST
++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
++ resultvar=VIRSH_DHCP_V4_START
++ network=192.168.111.0/24
++ record=20
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
++ result=192.168.111.20
++ eval VIRSH_DHCP_V4_START=192.168.111.20
+++ VIRSH_DHCP_V4_START=192.168.111.20
++ export VIRSH_DHCP_V4_START
++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
++ resultvar=VIRSH_DHCP_V4_END
++ network=192.168.111.0/24
++ record=60
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
++ result=192.168.111.60
++ eval VIRSH_DHCP_V4_END=192.168.111.60
+++ VIRSH_DHCP_V4_END=192.168.111.60
++ export VIRSH_DHCP_V4_END
++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
++ resultvar=BAREMETALV4_POOL_RANGE_START
++ network=192.168.111.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
++ result=192.168.111.100
++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
++ export BAREMETALV4_POOL_RANGE_START
++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
++ resultvar=BAREMETALV4_POOL_RANGE_END
++ network=192.168.111.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
++ result=192.168.111.200
++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
++ export BAREMETALV4_POOL_RANGE_END
++ export VIRSH_DHCP_V4_START
++ export VIRSH_DHCP_V4_END
++ export BAREMETALV4_POOL_RANGE_START
++ export BAREMETALV4_POOL_RANGE_END
++ [[ -n '' ]]
++ export EXTERNAL_SUBNET_V6_HOST=
++ EXTERNAL_SUBNET_V6_HOST=
++ export EXTERNAL_SUBNET_V6_PREFIX=
++ EXTERNAL_SUBNET_V6_PREFIX=
++ export BAREMETALV6_POOL_RANGE_START=
++ BAREMETALV6_POOL_RANGE_START=
++ export BAREMETALV6_POOL_RANGE_END=
++ BAREMETALV6_POOL_RANGE_END=
++ export REGISTRY_PORT=5000
++ REGISTRY_PORT=5000
++ export HTTP_PORT=6180
++ HTTP_PORT=6180
++ export IRONIC_INSPECTOR_PORT=5050
++ IRONIC_INSPECTOR_PORT=5050
++ export IRONIC_API_PORT=6385
++ IRONIC_API_PORT=6385
++ [[ -n 192.168.111.1 ]]
++ export REGISTRY=192.168.111.1:5000
++ REGISTRY=192.168.111.1:5000
++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
++ resultvar=INITIAL_IRONICBRIDGE_IP
++ network=172.22.0.0/24
++ record=9
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
++ result=172.22.0.9
++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
++ export INITIAL_IRONICBRIDGE_IP
++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ '[' true == true ']'
++ export IRONIC_URL=https://172.22.0.2:6385/v1/
++ IRONIC_URL=https://172.22.0.2:6385/v1/
++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+ export IRONIC_HOST=172.22.0.2
+ IRONIC_HOST=172.22.0.2
+ export IRONIC_HOST_IP=172.22.0.2
+ IRONIC_HOST_IP=172.22.0.2
+ sudo mkdir -p /opt/metal3-dev-env/ironic
+ sudo chown -R azureuser:azureuser /opt/metal3-dev-env/ironic
+ source lib/ironic_tls_setup.sh
++ '[' true == true ']'
++ pushd /opt/metal3-dev-env
/opt/metal3-dev-env ~/projects/metal3-dev-env
++ mkdir -p /opt/metal3-dev-env/certs
++ pushd /opt/metal3-dev-env/certs
/opt/metal3-dev-env/certs /opt/metal3-dev-env ~/projects/metal3-dev-env
++ export IRONIC_BASE_URL=https://172.22.0.2
++ IRONIC_BASE_URL=https://172.22.0.2
++ export IRONIC_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
++ IRONIC_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
++ export IRONIC_CAKEY_FILE=/opt/metal3-dev-env/certs/ironic-ca.key
++ IRONIC_CAKEY_FILE=/opt/metal3-dev-env/certs/ironic-ca.key
++ export IRONIC_CERT_FILE=/opt/metal3-dev-env/certs/ironic.crt
++ IRONIC_CERT_FILE=/opt/metal3-dev-env/certs/ironic.crt
++ export IRONIC_KEY_FILE=/opt/metal3-dev-env/certs/ironic.key
++ IRONIC_KEY_FILE=/opt/metal3-dev-env/certs/ironic.key
++ export IRONIC_INSPECTOR_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
++ IRONIC_INSPECTOR_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
++ export IRONIC_INSPECTOR_CAKEY_FILE=/opt/metal3-dev-env/certs/ironic-ca.key
++ IRONIC_INSPECTOR_CAKEY_FILE=/opt/metal3-dev-env/certs/ironic-ca.key
++ export IRONIC_INSPECTOR_CERT_FILE=/opt/metal3-dev-env/certs/ironic-inspector.crt
++ IRONIC_INSPECTOR_CERT_FILE=/opt/metal3-dev-env/certs/ironic-inspector.crt
++ export IRONIC_INSPECTOR_KEY_FILE=/opt/metal3-dev-env/certs/ironic-inspector.key
++ IRONIC_INSPECTOR_KEY_FILE=/opt/metal3-dev-env/certs/ironic-inspector.key
++ export MARIADB_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
++ MARIADB_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
++ export MARIADB_CAKEY_FILE=/opt/metal3-dev-env/certs/ironic-ca.key
++ MARIADB_CAKEY_FILE=/opt/metal3-dev-env/certs/ironic-ca.key
++ export MARIADB_CERT_FILE=/opt/metal3-dev-env/certs/mariadb.crt
++ MARIADB_CERT_FILE=/opt/metal3-dev-env/certs/mariadb.crt
++ export MARIADB_KEY_FILE=/opt/metal3-dev-env/certs/mariadb.key
++ MARIADB_KEY_FILE=/opt/metal3-dev-env/certs/mariadb.key
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-ca.key ']'
++ openssl genrsa -out /opt/metal3-dev-env/certs/ironic-ca.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
...+++++
..+++++
e is 65537 (0x010001)
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-ca.key ']'
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-ca.key ']'
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-ca.pem ']'
++ openssl req -x509 -new -nodes -key /opt/metal3-dev-env/certs/ironic-ca.key -sha256 -days 1825 -out /opt/metal3-dev-env/certs/ironic-ca.pem -subj '/CN=ironic CA/'
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-ca.pem ']'
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-ca.pem ']'
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic.key ']'
++ openssl genrsa -out /opt/metal3-dev-env/certs/ironic.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
.............+++++
...........................................................................+++++
e is 65537 (0x010001)
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-inspector.key ']'
++ openssl genrsa -out /opt/metal3-dev-env/certs/ironic-inspector.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
....................................................+++++
..............................................................................................................+++++
e is 65537 (0x010001)
++ '[' '!' -f /opt/metal3-dev-env/certs/mariadb.key ']'
++ openssl genrsa -out /opt/metal3-dev-env/certs/mariadb.key 2048
Generating RSA private key, 2048 bit long modulus (2 primes)
............................................................................................................................................................................+++++
.......................................................................................................................................+++++
e is 65537 (0x010001)
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic.crt ']'
++ openssl req -new -key /opt/metal3-dev-env/certs/ironic.key -out /tmp/ironic.csr -subj /CN=172.22.0.2/
++ openssl x509 -req -in /tmp/ironic.csr -CA /opt/metal3-dev-env/certs/ironic-ca.pem -CAkey /opt/metal3-dev-env/certs/ironic-ca.key -CAcreateserial -out /opt/metal3-dev-env/certs/ironic.crt -days 825 -sha256 -extfile /dev/fd/63
+++ printf subjectAltName=IP:%s 172.22.0.2
Signature ok
subject=CN = 172.22.0.2
Getting CA Private Key
++ '[' '!' -f /opt/metal3-dev-env/certs/ironic-inspector.crt ']'
++ openssl req -new -key /opt/metal3-dev-env/certs/ironic-inspector.key -out /tmp/ironic.csr -subj /CN=172.22.0.2/
++ openssl x509 -req -in /tmp/ironic.csr -CA /opt/metal3-dev-env/certs/ironic-ca.pem -CAkey /opt/metal3-dev-env/certs/ironic-ca.key -CAcreateserial -out /opt/metal3-dev-env/certs/ironic-inspector.crt -days 825 -sha256 -extfile /dev/fd/62
+++ printf subjectAltName=IP:%s 172.22.0.2
Signature ok
subject=CN = 172.22.0.2
Getting CA Private Key
++ '[' '!' -f /opt/metal3-dev-env/certs/mariadb.crt ']'
++ openssl req -new -key /opt/metal3-dev-env/certs/mariadb.key -out /tmp/mariadb.csr -subj /CN=mariaDB/
++ openssl x509 -req -in /tmp/mariadb.csr -CA /opt/metal3-dev-env/certs/ironic-ca.pem -CAkey /opt/metal3-dev-env/certs/ironic-ca.key -CAcreateserial -out /opt/metal3-dev-env/certs/mariadb.crt -days 825 -sha256 -extfile /dev/fd/61
+++ printf subjectAltName=IP:%s 127.0.0.1
Signature ok
subject=CN = mariaDB
Getting CA Private Key
++ '[' /opt/metal3-dev-env/certs/ironic-ca.pem == /opt/metal3-dev-env/certs/ironic-ca.pem ']'
+++ base64 -w 0
++ IRONIC_CA_CERT_B64=LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURDVENDQWZHZ0F3SUJBZ0lVTy9kOS9vMENZWU1wL0xMRnJvbVNRME1wZXM4d0RRWUpLb1pJaHZjTkFRRUwKQlFBd0ZERVNNQkFHQTFVRUF3d0phWEp2Ym1saklFTkJNQjRYRFRJeE1USXlNVEl3TlRVeU4xb1hEVEkyTVRJeQpNREl3TlRVeU4xb3dGREVTTUJBR0ExVUVBd3dKYVhKdmJtbGpJRU5CTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGCkFBT0NBUThBTUlJQkNnS0NBUUVBbmIrTlV2T0t6YTdjbnFRc0s1cFM1T3NlamFIc1hDckFIVnFFaHJNc21ySFYKSW1NMTVuRkRRNjhaWmVHNGJ5aEsvVDAwTmMySGFoQllrNDl4Z3RUdERxeGE3Tll6Y0VIL3V6OHZxK2grU3dvVwp4cGIyTCtSYVpMdmdBLzlQVk1qcWlhMXVoSWtSYXVYNFVTakdHd0ozUEl3UHdhNjhnZ014VzNvQnpxTGU5SXlsCkkvdDBSUFhudXozUG8xdUl2Wjh2NER2N1ZGTmUzMGtlYVh3YUt5azRGTFluTzRJelVmekFVeEVzNmd3OFl3M0EKUy9OaXY1M09oRVFlSW5sUTU5YWxYTHhrc1Urdjg0WGZvWmNhVzkxMjRPUHBlNGg3aS9ueWVSNmwzeVNKRUo4OQpDTTRMR2RoSVcvbFFPMW9zSmVUQTdFM3kxVUdhZXVoQVd3RUVrOG9tMFFJREFRQUJvMU13VVRBZEJnTlZIUTRFCkZnUVVSZGs1TlRCOFZSL3F1N3VCUFlKQnE0TW1FbEl3SHdZRFZSMGpCQmd3Rm9BVVJkazVOVEI4VlIvcXU3dUIKUFlKQnE0TW1FbEl3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBRHpqZgpsVlN6dnJTdXRqVFVnQmxwSVlqVSttR1RBeEJJaXV4ams3ZjFLSHR1RjFFZE9JQnRUOWFwR1NockVqUXBKTGZpCjJiV09paGNJTERZZDB4UmNrcGo5YS9GYklXZjlNcnBjdklRYzZwUk0xaXMrclZ2UHlIaXFRWUxhV2ZDKzdQVE4KV1FiM1pJVzA3dzdiTmlISjc0dS9wdmRzMkpxNlVjdFN2b1FhSTcxWUZwaE1NcFNkQ0JBbzIwQ1JiOG1FaEV0Lwo5VnJjRU1WZlZHMjV5cmgvSDdJYVUvRTdqNnI3aGsxaUJVNms3WCtFRnBBN0dML1hsU2U1Zmg0N2xxWWh4TTh2Cm1hNGQxT1JKTDRaRnBCclNXU3JYSGgxRnZXTWV4SzNwRUVObU1rcHROa29TVzJVWXF4L1V4MEtXT3JUY2lWclQKRlplczUrSlBPVW5ySERIVlN3PT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
++ export IRONIC_CA_CERT_B64
++ popd
/opt/metal3-dev-env ~/projects/metal3-dev-env
++ popd
~/projects/metal3-dev-env
++ unset IRONIC_NO_CA_CERT
+ source lib/ironic_basic_auth.sh
++ '[' true == true ']'
++ IRONIC_AUTH_DIR=/opt/metal3-dev-env/ironic/auth/
++ mkdir -p /opt/metal3-dev-env/ironic/auth/
++ '[' -z '' ']'
++ '[' '!' -f /opt/metal3-dev-env/ironic/auth/ironic-username ']'
+++ tr -dc a-zA-Z0-9
+++ fold -w 12
+++ head -n 1
++ IRONIC_USERNAME=tDqch2vdB5bP
++ echo tDqch2vdB5bP
++ '[' -z '' ']'
++ '[' '!' -f /opt/metal3-dev-env/ironic/auth/ironic-password ']'
+++ tr -dc a-zA-Z0-9
+++ fold -w 12
+++ head -n 1
++ IRONIC_PASSWORD=xTubq1zZimea
++ echo xTubq1zZimea
++ IRONIC_INSPECTOR_USERNAME=tDqch2vdB5bP
++ IRONIC_INSPECTOR_PASSWORD=xTubq1zZimea
++ export IRONIC_USERNAME
++ export IRONIC_PASSWORD
++ export IRONIC_INSPECTOR_USERNAME
++ export IRONIC_INSPECTOR_PASSWORD
++ unset IRONIC_NO_BASIC_AUTH
++ unset IRONIC_INSPECTOR_NO_BASIC_AUTH
+ clone_repos
+ mkdir -p /home/azureuser/go/src/github.com/metal3-io
+ clone_repo https://github.com/metal3-io/baremetal-operator.git master /home/azureuser/go/src/github.com/metal3-io/baremetal-operator HEAD
+ local REPO_URL=https://github.com/metal3-io/baremetal-operator.git
+ local REPO_BRANCH=master
+ local REPO_PATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
+ local REPO_COMMIT=HEAD
+ [[ -d /home/azureuser/go/src/github.com/metal3-io/baremetal-operator ]]
+ '[' '!' -d /home/azureuser/go/src/github.com/metal3-io/baremetal-operator ']'
+ pushd /home/azureuser/go/src/github.com/metal3-io
~/go/src/github.com/metal3-io ~/projects/metal3-dev-env
+ git clone https://github.com/metal3-io/baremetal-operator.git /home/azureuser/go/src/github.com/metal3-io/baremetal-operator
Cloning into '/home/azureuser/go/src/github.com/metal3-io/baremetal-operator'...
+ popd
~/projects/metal3-dev-env
+ pushd /home/azureuser/go/src/github.com/metal3-io/baremetal-operator
~/go/src/github.com/metal3-io/baremetal-operator ~/projects/metal3-dev-env
+ git checkout master
Switched to a new branch 'master'
Branch 'master' set up to track remote branch 'master' from 'origin'.
+ git checkout HEAD
Your branch is up to date with 'origin/master'.
+ git pull -r
Already up to date.
Current branch master is up to date.
+ popd
~/projects/metal3-dev-env
+ clone_repo https://github.com/metal3-io/cluster-api-provider-metal3 main /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
+ local REPO_URL=https://github.com/metal3-io/cluster-api-provider-metal3
+ local REPO_BRANCH=main
+ local REPO_PATH=/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
+ local REPO_COMMIT=HEAD
+ [[ -d /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3 ]]
+ '[' '!' -d /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3 ']'
+ pushd /home/azureuser/go/src/github.com/metal3-io
~/go/src/github.com/metal3-io ~/projects/metal3-dev-env
+ git clone https://github.com/metal3-io/cluster-api-provider-metal3 /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
Cloning into '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'...
+ popd
~/projects/metal3-dev-env
+ pushd /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
~/go/src/github.com/metal3-io/cluster-api-provider-metal3 ~/projects/metal3-dev-env
+ git checkout main
Already on 'main'
Your branch is up to date with 'origin/main'.
+ git checkout HEAD
Your branch is up to date with 'origin/main'.
+ git pull -r
Already up to date.
Current branch main is up to date.
+ popd
~/projects/metal3-dev-env
+ clone_repo https://github.com/metal3-io/ip-address-manager main /home/azureuser/go/src/github.com/metal3-io/ip-address-manager
+ local REPO_URL=https://github.com/metal3-io/ip-address-manager
+ local REPO_BRANCH=main
+ local REPO_PATH=/home/azureuser/go/src/github.com/metal3-io/ip-address-manager
+ local REPO_COMMIT=HEAD
+ [[ -d /home/azureuser/go/src/github.com/metal3-io/ip-address-manager ]]
+ '[' '!' -d /home/azureuser/go/src/github.com/metal3-io/ip-address-manager ']'
+ pushd /home/azureuser/go/src/github.com/metal3-io
~/go/src/github.com/metal3-io ~/projects/metal3-dev-env
+ git clone https://github.com/metal3-io/ip-address-manager /home/azureuser/go/src/github.com/metal3-io/ip-address-manager
Cloning into '/home/azureuser/go/src/github.com/metal3-io/ip-address-manager'...
+ popd
~/projects/metal3-dev-env
+ pushd /home/azureuser/go/src/github.com/metal3-io/ip-address-manager
~/go/src/github.com/metal3-io/ip-address-manager ~/projects/metal3-dev-env
+ git checkout main
Already on 'main'
Your branch is up to date with 'origin/main'.
+ git checkout HEAD
Your branch is up to date with 'origin/main'.
+ git pull -r
Already up to date.
Current branch main is up to date.
+ popd
~/projects/metal3-dev-env
+ /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/remove_local_ironic.sh
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic$'
+ sudo docker ps --all
+ grep -w 'ironic$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-api$'
+ sudo docker ps --all
+ grep -w 'ironic-api$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-conductor$'
+ sudo docker ps --all
+ grep -w 'ironic-conductor$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-inspector$'
+ sudo docker ps --all
+ grep -w 'ironic-inspector$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'dnsmasq$'
+ sudo docker ps --all
+ grep -w 'dnsmasq$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'httpd$'
+ sudo docker ps --all
+ grep -w 'httpd$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'mariadb$'
+ sudo docker ps --all
+ grep -w 'mariadb$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ipa-downloader$'
+ sudo docker ps --all
+ grep -w 'ipa-downloader$'
11680e8a6105   quay.io/metal3-io/ironic-ipa-downloader   "/usr/local/bin/get-â€¦"   2 minutes ago    Exited (0) 2 minutes ago                                               ipa-downloader
+ sudo docker rm ipa-downloader -f
ipa-downloader
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-endpoint-keepalived$'
+ sudo docker ps --all
+ grep -w 'ironic-endpoint-keepalived$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-log-watch$'
+ sudo docker ps --all
+ grep -w 'ironic-log-watch$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'httpd-reverse-proxy$'
+ sudo docker ps --all
+ grep -w 'httpd-reverse-proxy$'
+ set +xe
+ create_clouds_yaml
+ mkdir -p /home/azureuser/projects/metal3-dev-env/_clouds_yaml
+ '[' true == true ']'
+ cp /opt/metal3-dev-env/certs/ironic-ca.pem /home/azureuser/projects/metal3-dev-env/_clouds_yaml/ironic-ca.crt
+ render_j2_config /home/azureuser/projects/metal3-dev-env/clouds.yaml.j2
+ python3 -c 'import os; import sys; import jinja2; sys.stdout.write(jinja2.Template(sys.stdin.read()).render(env=os.environ))'
+ '[' kind '!=' tilt ']'
+ start_management_cluster
+ '[' kind == kind ']'
+ launch_kind
+ cat
+ sudo su -l -c 'kind create cluster --name kind --image=kindest/node:v1.22.2 --config=- ' azureuser
Creating cluster "kind" ...
 â€¢ Ensuring node image (kindest/node:v1.22.2) ðŸ–¼  ...
 âœ“ Ensuring node image (kindest/node:v1.22.2) ðŸ–¼
 â€¢ Preparing nodes ðŸ“¦   ...
 âœ“ Preparing nodes ðŸ“¦ 
 â€¢ Writing configuration ðŸ“œ  ...
 âœ“ Writing configuration ðŸ“œ
 â€¢ Starting control-plane ðŸ•¹ï¸  ...
 âœ“ Starting control-plane ðŸ•¹ï¸
 â€¢ Installing CNI ðŸ”Œ  ...
 âœ“ Installing CNI ðŸ”Œ
 â€¢ Installing StorageClass ðŸ’¾  ...
 âœ“ Installing StorageClass ðŸ’¾
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Thanks for using kind! ðŸ˜Š
+ kubectl create namespace metal3
namespace/metal3 created
+ '[' kind '!=' tilt ']'
+ patch_clusterctl
+ pushd /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
~/go/src/github.com/metal3-io/cluster-api-provider-metal3 ~/projects/metal3-dev-env
+ mkdir -p /home/azureuser/.cluster-api
+ touch /home/azureuser/.cluster-api/clusterctl.yaml
+ '[' -n '' ']'
+ update_component_image CAPM3 quay.io/metal3-io/cluster-api-provider-metal3:main
+ IMPORT=CAPM3
+ ORIG_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ TMP_IMAGE=cluster-api-provider-metal3:main
+ TMP_IMAGE_NAME=cluster-api-provider-metal3
+ TMP_IMAGE_TAG=main
+ '[' cluster-api-provider-metal3 == main ']'
+ '[' CAPM3 == BMO ']'
+ '[' CAPM3 == CAPM3 ']'
+ export MANIFEST_IMG=192.168.111.1:5000/localimages/cluster-api-provider-metal3
+ MANIFEST_IMG=192.168.111.1:5000/localimages/cluster-api-provider-metal3
+ export MANIFEST_TAG=main
+ MANIFEST_TAG=main
+ make set-manifest-image
make[1]: Entering directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
Updating kustomize image patch file for manager resource
sed -i'' -e 's@image: .*@image: '"192.168.111.1:5000/localimages/cluster-api-provider-metal3:main"'@' ./config/default/capm3/manager_image_patch.yaml
make[1]: Leaving directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
+ '[' v1beta1 == v1alpha4 ']'
+ '[' -n '' ']'
+ update_component_image IPAM quay.io/metal3-io/ip-address-manager:main
+ IMPORT=IPAM
+ ORIG_IMAGE=quay.io/metal3-io/ip-address-manager:main
+ TMP_IMAGE=ip-address-manager:main
+ TMP_IMAGE_NAME=ip-address-manager
+ TMP_IMAGE_TAG=main
+ '[' ip-address-manager == main ']'
+ '[' IPAM == BMO ']'
+ '[' IPAM == CAPM3 ']'
+ '[' IPAM == IPAM ']'
+ export MANIFEST_IMG_IPAM=192.168.111.1:5000/localimages/ip-address-manager
+ MANIFEST_IMG_IPAM=192.168.111.1:5000/localimages/ip-address-manager
+ export MANIFEST_TAG_IPAM=main
+ MANIFEST_TAG_IPAM=main
+ make set-manifest-image-ipam
make[1]: Entering directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
Updating kustomize image patch file for IPAM controller
sed -i'' -e 's@image: .*@image: '"192.168.111.1:5000/localimages/ip-address-manager:main"'@' ./config/ipam/image_patch.yaml
make[1]: Leaving directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
+ update_capm3_imports
+ pushd /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
~/go/src/github.com/metal3-io/cluster-api-provider-metal3 ~/go/src/github.com/metal3-io/cluster-api-provider-metal3 ~/projects/metal3-dev-env
+ '[' true == false ']'
+ '[' v1beta1 == v1alpha4 ']'
+ cp config/ipam/kustomization.yaml config/ipam/kustomization.yaml.orig
+ make hack/tools/bin/kustomize
make[1]: Entering directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
cd hack/tools; ./install_kustomize.sh
Verifying kustomize version
kustomize not found, installing
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   674  100   674    0     0   8753      0 --:--:-- --:--:-- --:--:--  8868
100 5094k  100 5094k    0     0  21.5M      0 --:--:-- --:--:-- --:--:-- 84.3M
kustomize
make[1]: Leaving directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
+ '[' v1beta1 == v1alpha4 ']'
+ ./hack/tools/bin/kustomize build /home/azureuser/go/src/github.com/metal3-io/ip-address-manager/config/default
+ sed -i -e 's#https://github.com/metal3-io/ip-address-manager/releases/download/v.*/ipam-components.yaml#metal3-ipam-components.yaml#' config/ipam/kustomization.yaml
+ popd
~/go/src/github.com/metal3-io/cluster-api-provider-metal3 ~/projects/metal3-dev-env
+ make release-manifests
make[1]: Entering directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
cd hack/tools; ./install_kustomize.sh
Verifying kustomize version
mkdir -p out/
hack/tools/bin/kustomize build config/default > out/infrastructure-components.yaml
cp metadata.yaml out/metadata.yaml
cp examples/clusterctl-templates/clusterctl-cluster.yaml out/cluster-template.yaml
cp examples/clusterctl-templates/example_variables.rc out/example_variables.rc
make[1]: Leaving directory '/home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3'
+ '[' v1beta1 == v1alpha4 ']'
+ mv config/ipam/kustomization.yaml.orig config/ipam/kustomization.yaml
+ rm config/ipam/metal3-ipam-components.yaml
+ rm -rf /home/azureuser/.cluster-api/overrides/infrastructure-metal3/v1.0.0
+ mkdir -p /home/azureuser/.cluster-api/overrides/infrastructure-metal3/v1.0.0
+ cp out/cluster-template.yaml out/infrastructure-components.yaml out/metadata.yaml /home/azureuser/.cluster-api/overrides/infrastructure-metal3/v1.0.0
+ popd
~/projects/metal3-dev-env
+ launch_cluster_api_provider_metal3
+ pushd /home/azureuser/go/src/github.com/metal3-io/cluster-api-provider-metal3
~/go/src/github.com/metal3-io/cluster-api-provider-metal3 ~/projects/metal3-dev-env
+ clusterctl init --core cluster-api:v1.0.2 --bootstrap kubeadm:v1.0.2 --control-plane kubeadm:v1.0.2 --infrastructure=metal3:v1.0.0 -v5
Using configuration File="/home/azureuser/.cluster-api/clusterctl.yaml"
Installing the clusterctl inventory CRD
Creating CustomResourceDefinition="providers.clusterctl.cluster.x-k8s.io"
Fetching providers
Fetching File="core-components.yaml" Provider="cluster-api" Type="CoreProvider" Version="v1.0.2"
Fetching File="bootstrap-components.yaml" Provider="kubeadm" Type="BootstrapProvider" Version="v1.0.2"
Fetching File="control-plane-components.yaml" Provider="kubeadm" Type="ControlPlaneProvider" Version="v1.0.2"
Using Override="infrastructure-components.yaml" Provider="infrastructure-metal3" Version="v1.0.0"
Fetching File="metadata.yaml" Provider="cluster-api" Type="CoreProvider" Version="v1.0.2"
Fetching File="metadata.yaml" Provider="kubeadm" Type="BootstrapProvider" Version="v1.0.2"
Fetching File="metadata.yaml" Provider="kubeadm" Type="ControlPlaneProvider" Version="v1.0.2"
Using Override="metadata.yaml" Provider="infrastructure-metal3" Version="v1.0.0"
Creating Namespace="cert-manager-test"
Installing cert-manager Version="v1.5.3"
Fetching File="cert-manager.yaml" Provider="cert-manager" Type="" Version="v1.5.3"
Creating Namespace="cert-manager"
Creating CustomResourceDefinition="certificaterequests.cert-manager.io"
Creating CustomResourceDefinition="certificates.cert-manager.io"
Creating CustomResourceDefinition="challenges.acme.cert-manager.io"
Creating CustomResourceDefinition="clusterissuers.cert-manager.io"
Creating CustomResourceDefinition="issuers.cert-manager.io"
Creating CustomResourceDefinition="orders.acme.cert-manager.io"
Creating ServiceAccount="cert-manager-cainjector" Namespace="cert-manager"
Creating ServiceAccount="cert-manager" Namespace="cert-manager"
Creating ServiceAccount="cert-manager-webhook" Namespace="cert-manager"
Creating ClusterRole="cert-manager-cainjector"
Creating ClusterRole="cert-manager-controller-issuers"
Creating ClusterRole="cert-manager-controller-clusterissuers"
Creating ClusterRole="cert-manager-controller-certificates"
Creating ClusterRole="cert-manager-controller-orders"
Creating ClusterRole="cert-manager-controller-challenges"
Creating ClusterRole="cert-manager-controller-ingress-shim"
Creating ClusterRole="cert-manager-view"
Creating ClusterRole="cert-manager-edit"
Creating ClusterRole="cert-manager-controller-approve:cert-manager-io"
Creating ClusterRole="cert-manager-controller-certificatesigningrequests"
Creating ClusterRole="cert-manager-webhook:subjectaccessreviews"
Creating ClusterRoleBinding="cert-manager-cainjector"
Creating ClusterRoleBinding="cert-manager-controller-issuers"
Creating ClusterRoleBinding="cert-manager-controller-clusterissuers"
Creating ClusterRoleBinding="cert-manager-controller-certificates"
Creating ClusterRoleBinding="cert-manager-controller-orders"
Creating ClusterRoleBinding="cert-manager-controller-challenges"
Creating ClusterRoleBinding="cert-manager-controller-ingress-shim"
Creating ClusterRoleBinding="cert-manager-controller-approve:cert-manager-io"
Creating ClusterRoleBinding="cert-manager-controller-certificatesigningrequests"
Creating ClusterRoleBinding="cert-manager-webhook:subjectaccessreviews"
Creating Role="cert-manager-cainjector:leaderelection" Namespace="kube-system"
Creating Role="cert-manager:leaderelection" Namespace="kube-system"
Creating Role="cert-manager-webhook:dynamic-serving" Namespace="cert-manager"
Creating RoleBinding="cert-manager-cainjector:leaderelection" Namespace="kube-system"
Creating RoleBinding="cert-manager:leaderelection" Namespace="kube-system"
Creating RoleBinding="cert-manager-webhook:dynamic-serving" Namespace="cert-manager"
Creating Service="cert-manager" Namespace="cert-manager"
Creating Service="cert-manager-webhook" Namespace="cert-manager"
Creating Deployment="cert-manager-cainjector" Namespace="cert-manager"
Creating Deployment="cert-manager" Namespace="cert-manager"
Creating Deployment="cert-manager-webhook" Namespace="cert-manager"
Creating MutatingWebhookConfiguration="cert-manager-webhook"
Creating ValidatingWebhookConfiguration="cert-manager-webhook"
Waiting for cert-manager to be available...
Updating Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Issuer="test-selfsigned" Namespace="cert-manager-test"
Creating Certificate="selfsigned-cert" Namespace="cert-manager-test"
Deleting Namespace="cert-manager-test"
Deleting Issuer="test-selfsigned" Namespace="cert-manager-test"
Deleting Certificate="selfsigned-cert" Namespace="cert-manager-test"
Installing Provider="cluster-api" Version="v1.0.2" TargetNamespace="capi-system"
Creating objects Provider="cluster-api" Version="v1.0.2" TargetNamespace="capi-system"
Creating Namespace="capi-system"
Creating CustomResourceDefinition="clusterclasses.cluster.x-k8s.io"
Creating CustomResourceDefinition="clusterresourcesetbindings.addons.cluster.x-k8s.io"
Creating CustomResourceDefinition="clusterresourcesets.addons.cluster.x-k8s.io"
Creating CustomResourceDefinition="clusters.cluster.x-k8s.io"
Creating CustomResourceDefinition="machinedeployments.cluster.x-k8s.io"
Creating CustomResourceDefinition="machinehealthchecks.cluster.x-k8s.io"
Creating CustomResourceDefinition="machinepools.cluster.x-k8s.io"
Creating CustomResourceDefinition="machines.cluster.x-k8s.io"
Creating CustomResourceDefinition="machinesets.cluster.x-k8s.io"
Creating ServiceAccount="capi-manager" Namespace="capi-system"
Creating Role="capi-leader-election-role" Namespace="capi-system"
Creating ClusterRole="capi-system-capi-aggregated-manager-role"
Creating ClusterRole="capi-system-capi-manager-role"
Creating RoleBinding="capi-leader-election-rolebinding" Namespace="capi-system"
Creating ClusterRoleBinding="capi-system-capi-manager-rolebinding"
Creating Service="capi-webhook-service" Namespace="capi-system"
Creating Deployment="capi-controller-manager" Namespace="capi-system"
Creating Certificate="capi-serving-cert" Namespace="capi-system"
Creating Issuer="capi-selfsigned-issuer" Namespace="capi-system"
Creating MutatingWebhookConfiguration="capi-mutating-webhook-configuration"
Creating ValidatingWebhookConfiguration="capi-validating-webhook-configuration"
Creating inventory entry Provider="cluster-api" Version="v1.0.2" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v1.0.2" TargetNamespace="capi-kubeadm-bootstrap-system"
Creating objects Provider="bootstrap-kubeadm" Version="v1.0.2" TargetNamespace="capi-kubeadm-bootstrap-system"
Creating Namespace="capi-kubeadm-bootstrap-system"
Creating CustomResourceDefinition="kubeadmconfigs.bootstrap.cluster.x-k8s.io"
Creating CustomResourceDefinition="kubeadmconfigtemplates.bootstrap.cluster.x-k8s.io"
Creating ServiceAccount="capi-kubeadm-bootstrap-manager" Namespace="capi-kubeadm-bootstrap-system"
Creating Role="capi-kubeadm-bootstrap-leader-election-role" Namespace="capi-kubeadm-bootstrap-system"
Creating ClusterRole="capi-kubeadm-bootstrap-system-capi-kubeadm-bootstrap-manager-role"
Creating RoleBinding="capi-kubeadm-bootstrap-leader-election-rolebinding" Namespace="capi-kubeadm-bootstrap-system"
Creating ClusterRoleBinding="capi-kubeadm-bootstrap-system-capi-kubeadm-bootstrap-manager-rolebinding"
Creating Service="capi-kubeadm-bootstrap-webhook-service" Namespace="capi-kubeadm-bootstrap-system"
Creating Deployment="capi-kubeadm-bootstrap-controller-manager" Namespace="capi-kubeadm-bootstrap-system"
Creating Certificate="capi-kubeadm-bootstrap-serving-cert" Namespace="capi-kubeadm-bootstrap-system"
Creating Issuer="capi-kubeadm-bootstrap-selfsigned-issuer" Namespace="capi-kubeadm-bootstrap-system"
Creating ValidatingWebhookConfiguration="capi-kubeadm-bootstrap-validating-webhook-configuration"
Creating inventory entry Provider="bootstrap-kubeadm" Version="v1.0.2" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v1.0.2" TargetNamespace="capi-kubeadm-control-plane-system"
Creating objects Provider="control-plane-kubeadm" Version="v1.0.2" TargetNamespace="capi-kubeadm-control-plane-system"
Creating Namespace="capi-kubeadm-control-plane-system"
Creating CustomResourceDefinition="kubeadmcontrolplanes.controlplane.cluster.x-k8s.io"
Creating CustomResourceDefinition="kubeadmcontrolplanetemplates.controlplane.cluster.x-k8s.io"
Creating ServiceAccount="capi-kubeadm-control-plane-manager" Namespace="capi-kubeadm-control-plane-system"
Creating Role="capi-kubeadm-control-plane-leader-election-role" Namespace="capi-kubeadm-control-plane-system"
Creating ClusterRole="capi-kubeadm-control-plane-system-capi-kubeadm-control-plane-aggregated-manager-role"
Creating ClusterRole="capi-kubeadm-control-plane-system-capi-kubeadm-control-plane-manager-role"
Creating RoleBinding="capi-kubeadm-control-plane-leader-election-rolebinding" Namespace="capi-kubeadm-control-plane-system"
Creating ClusterRoleBinding="capi-kubeadm-control-plane-system-capi-kubeadm-control-plane-manager-rolebinding"
Creating Service="capi-kubeadm-control-plane-webhook-service" Namespace="capi-kubeadm-control-plane-system"
Creating Deployment="capi-kubeadm-control-plane-controller-manager" Namespace="capi-kubeadm-control-plane-system"
Creating Certificate="capi-kubeadm-control-plane-serving-cert" Namespace="capi-kubeadm-control-plane-system"
Creating Issuer="capi-kubeadm-control-plane-selfsigned-issuer" Namespace="capi-kubeadm-control-plane-system"
Creating MutatingWebhookConfiguration="capi-kubeadm-control-plane-mutating-webhook-configuration"
Creating ValidatingWebhookConfiguration="capi-kubeadm-control-plane-validating-webhook-configuration"
Creating inventory entry Provider="control-plane-kubeadm" Version="v1.0.2" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-metal3" Version="v1.0.0" TargetNamespace="capm3-system"
Creating objects Provider="infrastructure-metal3" Version="v1.0.0" TargetNamespace="capm3-system"
Creating Namespace="capm3-system"
Creating CustomResourceDefinition="ipaddresses.ipam.metal3.io"
Creating CustomResourceDefinition="ipclaims.ipam.metal3.io"
Creating CustomResourceDefinition="ippools.ipam.metal3.io"
Creating CustomResourceDefinition="metal3clusters.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3dataclaims.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3datas.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3datatemplates.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3machines.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3machinetemplates.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3remediations.infrastructure.cluster.x-k8s.io"
Creating CustomResourceDefinition="metal3remediationtemplates.infrastructure.cluster.x-k8s.io"
Creating ServiceAccount="capm3-manager" Namespace="capm3-system"
Creating ServiceAccount="ipam-manager" Namespace="capm3-system"
Creating Role="capm3-leader-election-role" Namespace="capm3-system"
Creating Role="ipam-leader-election-role" Namespace="capm3-system"
Creating ClusterRole="capm3-system-capm3-manager-role"
Creating ClusterRole="capm3-system-ipam-manager-role"
Creating RoleBinding="capm3-leader-election-rolebinding" Namespace="capm3-system"
Creating RoleBinding="ipam-leader-election-rolebinding" Namespace="capm3-system"
Creating ClusterRoleBinding="capm3-system-capm3-manager-rolebinding"
Creating ClusterRoleBinding="capm3-system-ipam-manager-rolebinding"
Creating Service="capm3-webhook-service" Namespace="capm3-system"
Creating Service="ipam-webhook-service" Namespace="capm3-system"
Creating Deployment="capm3-controller-manager" Namespace="capm3-system"
Creating Deployment="ipam-controller-manager" Namespace="capm3-system"
Creating Certificate="capm3-serving-cert" Namespace="capm3-system"
Creating Certificate="ipam-serving-cert" Namespace="capm3-system"
Creating Issuer="capm3-selfsigned-issuer" Namespace="capm3-system"
Creating Issuer="ipam-selfsigned-issuer" Namespace="capm3-system"
Creating MutatingWebhookConfiguration="capm3-mutating-webhook-configuration"
Creating MutatingWebhookConfiguration="ipam-mutating-webhook-configuration"
Creating ValidatingWebhookConfiguration="capm3-validating-webhook-configuration"
Creating ValidatingWebhookConfiguration="ipam-validating-webhook-configuration"
Creating inventory entry Provider="infrastructure-metal3" Version="v1.0.0" TargetNamespace="capm3-system"

Your management cluster has been initialized successfully!

You can now create your first workload cluster by running the following:

  clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -

Using configuration File="/home/azureuser/.cluster-api/clusterctl.yaml"
+ '[' false == true ']'
+ '[' false == true ']'
+ popd
~/projects/metal3-dev-env
+ '[' kind '!=' tilt ']'
+ '[' v1beta1 '!=' v1alpha4 ']'
+ launch_baremetal_operator
+ pushd /home/azureuser/go/src/github.com/metal3-io/baremetal-operator
~/go/src/github.com/metal3-io/baremetal-operator ~/projects/metal3-dev-env
+ cp /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml.orig
+ update_kustomization_images /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ FILE_PATH=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
++ env
++ grep _LOCAL_IMAGE=
++ grep -o '^[^=]*'
++ env
++ grep -v _LOCAL_IMAGE=
++ grep _IMAGE=
++ grep -o '^[^=]*'
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=registry:2.7.1
+ IMAGE_NAME=registry:2.7.1
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
+ sed -i -E 's registry:2.7.1$ 192.168.111.1:5000/localimages/registry:2.7.1 g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/baremetal-operator
+ IMAGE_NAME=baremetal-operator
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
+ sed -i -E 's quay.io/metal3-io/baremetal-operator$ 192.168.111.1:5000/localimages/baremetal-operator g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-client
+ IMAGE_NAME=ironic-client
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-client
+ sed -i -E 's quay.io/metal3-io/ironic-client$ 192.168.111.1:5000/localimages/ironic-client g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic
+ IMAGE_NAME=ironic
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic
+ sed -i -E 's quay.io/metal3-io/ironic$ 192.168.111.1:5000/localimages/ironic g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ IMAGE_NAME=ironic-ipa-downloader
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ sed -i -E 's quay.io/metal3-io/ironic-ipa-downloader$ 192.168.111.1:5000/localimages/ironic-ipa-downloader g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/sushy-tools
+ IMAGE_NAME=sushy-tools
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/sushy-tools
+ sed -i -E 's quay.io/metal3-io/sushy-tools$ 192.168.111.1:5000/localimages/sushy-tools g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/keepalived
+ IMAGE_NAME=keepalived
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/keepalived
+ sed -i -E 's quay.io/metal3-io/keepalived$ 192.168.111.1:5000/localimages/keepalived g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/vbmc
+ IMAGE_NAME=vbmc
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/vbmc
+ sed -i -E 's quay.io/metal3-io/vbmc$ 192.168.111.1:5000/localimages/vbmc g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ip-address-manager:main
+ IMAGE_NAME=ip-address-manager:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:main
+ sed -i -E 's quay.io/metal3-io/ip-address-manager:main$ 192.168.111.1:5000/localimages/ip-address-manager:main g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ IMAGE_NAME=cluster-api-provider-metal3:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ sed -i -E 's quay.io/metal3-io/cluster-api-provider-metal3:main$ 192.168.111.1:5000/localimages/cluster-api-provider-metal3:main g' /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/manager/manager.yaml
+ cp /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/default/ironic.env /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/default/ironic.env.orig
+ cat
+ sudo tee /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/config/default/ironic.env
DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
IRONIC_ENDPOINT=https://172.22.0.2:6385/v1/
IRONIC_INSPECTOR_ENDPOINT=https://172.22.0.2:5050/v1/
+ /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/deploy.sh true false true true true
make[1]: Entering directory '/home/azureuser/go/src/github.com/metal3-io/baremetal-operator'
cd hack/tools; go build -o /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/bin/kustomize sigs.k8s.io/kustomize/kustomize/v3
go: downloading sigs.k8s.io/kustomize/kustomize/v3 v3.10.0
go: downloading github.com/spf13/cobra v1.2.1
go: downloading sigs.k8s.io/kustomize/cmd/config v0.9.1
go: downloading sigs.k8s.io/kustomize/api v0.8.0
go: downloading github.com/spf13/pflag v1.0.5
go: downloading sigs.k8s.io/kustomize/kyaml v0.10.9
go: downloading github.com/pkg/errors v0.9.1
go: downloading sigs.k8s.io/yaml v1.2.0
go: downloading gopkg.in/yaml.v2 v2.4.0
go: downloading github.com/go-errors/errors v1.0.1
go: downloading github.com/olekukonko/tablewriter v0.0.5
go: downloading github.com/go-openapi/spec v0.19.5
go: downloading gopkg.in/inf.v0 v0.9.1
go: downloading github.com/mattn/go-runewidth v0.0.9
go: downloading github.com/yujunz/go-getter v1.5.1-lite.0.20201201013212-6d9c071adddf
go: downloading github.com/google/shlex v0.0.0-20191202100458-e7afc7fbc510
go: downloading github.com/hashicorp/go-multierror v1.1.1
go: downloading github.com/evanphx/json-patch v4.9.0+incompatible
go: downloading github.com/imdario/mergo v0.3.8
go: downloading github.com/go-openapi/swag v0.19.5
go: downloading github.com/go-openapi/jsonpointer v0.19.3
go: downloading github.com/go-openapi/jsonreference v0.19.3
go: downloading github.com/hashicorp/errwrap v1.0.0
go: downloading github.com/PuerkitoBio/purell v1.1.1
go: downloading github.com/mailru/easyjson v0.7.0
go: downloading golang.org/x/text v0.3.6
go: downloading golang.org/x/net v0.0.0-20210428140749-89ef3d95e781
go: downloading github.com/PuerkitoBio/urlesc v0.0.0-20170810143723-de5bf2ad4578
go: downloading github.com/hashicorp/go-version v1.1.0
go: downloading github.com/mitchellh/go-homedir v1.1.0
go: downloading github.com/ulikunitz/xz v0.5.8
go: downloading github.com/mitchellh/go-testing-interface v1.0.0
go: downloading github.com/hashicorp/go-cleanhttp v0.5.2
go: downloading github.com/hashicorp/go-safetemp v1.0.0
go: downloading github.com/bgentry/go-netrc v0.0.0-20140422174119-9fd32a8b3d3d
go: downloading go.starlark.net v0.0.0-20200306205701-8dd3e2ee1dd5
go: downloading github.com/qri-io/starlib v0.4.2-0.20200213133954-ff2e8cd5ef8d
go: downloading github.com/go-openapi/strfmt v0.19.5
go: downloading github.com/go-openapi/validate v0.19.8
go: downloading gopkg.in/yaml.v3 v3.0.0-20210107192922-496545a6307b
go: downloading github.com/davecgh/go-spew v1.1.1
go: downloading github.com/stretchr/testify v1.7.0
go: downloading github.com/xlab/treeprint v0.0.0-20181112141820-a009c3971eca
go: downloading github.com/monochromegane/go-gitignore v0.0.0-20200626010858-205db1a8cc00
go: downloading github.com/mitchellh/mapstructure v1.4.1
go: downloading go.mongodb.org/mongo-driver v1.5.1
go: downloading github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4a
go: downloading github.com/go-openapi/errors v0.19.2
go: downloading github.com/pmezard/go-difflib v1.0.0
go: downloading github.com/go-openapi/runtime v0.19.4
go: downloading github.com/go-openapi/analysis v0.19.5
go: downloading github.com/go-openapi/loads v0.19.4
go: downloading github.com/go-stack/stack v1.8.0
make[1]: Leaving directory '/home/azureuser/go/src/github.com/metal3-io/baremetal-operator'
~/go/src/github.com/metal3-io/baremetal-operator ~/go/src/github.com/metal3-io/baremetal-operator
namespace/baremetal-operator-system created
customresourcedefinition.apiextensions.k8s.io/baremetalhosts.metal3.io created
customresourcedefinition.apiextensions.k8s.io/bmceventsubscriptions.metal3.io created
customresourcedefinition.apiextensions.k8s.io/firmwareschemas.metal3.io created
customresourcedefinition.apiextensions.k8s.io/hostfirmwaresettings.metal3.io created
customresourcedefinition.apiextensions.k8s.io/preprovisioningimages.metal3.io created
serviceaccount/baremetal-operator-controller-manager created
role.rbac.authorization.k8s.io/baremetal-operator-leader-election-role created
clusterrole.rbac.authorization.k8s.io/baremetal-operator-manager-role created
clusterrole.rbac.authorization.k8s.io/baremetal-operator-metrics-reader created
clusterrole.rbac.authorization.k8s.io/baremetal-operator-proxy-role created
rolebinding.rbac.authorization.k8s.io/baremetal-operator-leader-election-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/baremetal-operator-manager-rolebinding created
clusterrolebinding.rbac.authorization.k8s.io/baremetal-operator-proxy-rolebinding created
configmap/baremetal-operator-ironic created
configmap/baremetal-operator-manager-config created
secret/ironic-credentials-765b8t4fgh created
secret/ironic-inspector-credentials-765b8t4fgh created
service/baremetal-operator-controller-manager-metrics-service created
service/baremetal-operator-webhook-service created
deployment.apps/baremetal-operator-controller-manager created
certificate.cert-manager.io/baremetal-operator-serving-cert created
issuer.cert-manager.io/baremetal-operator-selfsigned-issuer created
validatingwebhookconfiguration.admissionregistration.k8s.io/baremetal-operator-validating-webhook-configuration created
~/go/src/github.com/metal3-io/baremetal-operator
+ '[' false == true ']'
+ popd
~/projects/metal3-dev-env
+ launch_ironic
+ pushd /home/azureuser/go/src/github.com/metal3-io/baremetal-operator
~/go/src/github.com/metal3-io/baremetal-operator ~/projects/metal3-dev-env
+ cp /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/ironic-deployment/keepalived/ironic_bmo_configmap.env /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/ironic-deployment/keepalived/ironic_bmo_configmap.env.orig
+ cat
+ sudo tee /opt/metal3-dev-env/ironic/ironic_bmo_configmap.env
HTTP_PORT=6180
PROVISIONING_IP=172.22.0.2
PROVISIONING_CIDR=24
PROVISIONING_INTERFACE=ironicendpoint
DHCP_RANGE=172.22.0.10,172.22.0.100
DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
IRONIC_ENDPOINT=https://172.22.0.2:6385/v1/
IRONIC_INSPECTOR_ENDPOINT=https://172.22.0.2:5050/v1/
CACHEURL=http://172.22.0.1/images
IRONIC_FAST_TRACK=true
RESTART_CONTAINER_CERTIFICATE_UPDATED="true"
+ '[' libvirt == libvirt ']'
+ echo IRONIC_KERNEL_PARAMS=console=ttyS0
+ sudo tee -a /opt/metal3-dev-env/ironic/ironic_bmo_configmap.env
IRONIC_KERNEL_PARAMS=console=ttyS0
+ '[' kind '!=' minikube ']'
+ update_images
++ env
++ grep _LOCAL_IMAGE=
++ grep -o '^[^=]*'
++ env
++ grep -v _LOCAL_IMAGE=
++ grep _IMAGE=
++ grep -o '^[^=]*'
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=registry:2.7.1
+ IMAGE_NAME=registry:2.7.1
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
+ eval DOCKER_REGISTRY_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
++ DOCKER_REGISTRY_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/baremetal-operator
+ IMAGE_NAME=baremetal-operator
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
+ eval BAREMETAL_OPERATOR_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
++ BAREMETAL_OPERATOR_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-client
+ IMAGE_NAME=ironic-client
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-client
+ eval IRONIC_CLIENT_IMAGE=192.168.111.1:5000/localimages/ironic-client
++ IRONIC_CLIENT_IMAGE=192.168.111.1:5000/localimages/ironic-client
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic
+ IMAGE_NAME=ironic
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic
+ eval IRONIC_IMAGE=192.168.111.1:5000/localimages/ironic
++ IRONIC_IMAGE=192.168.111.1:5000/localimages/ironic
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ IMAGE_NAME=ironic-ipa-downloader
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ eval IPA_DOWNLOADER_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
++ IPA_DOWNLOADER_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/sushy-tools
+ IMAGE_NAME=sushy-tools
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/sushy-tools
+ eval SUSHY_TOOLS_IMAGE=192.168.111.1:5000/localimages/sushy-tools
++ SUSHY_TOOLS_IMAGE=192.168.111.1:5000/localimages/sushy-tools
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/keepalived
+ IMAGE_NAME=keepalived
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/keepalived
+ eval IRONIC_KEEPALIVED_IMAGE=192.168.111.1:5000/localimages/keepalived
++ IRONIC_KEEPALIVED_IMAGE=192.168.111.1:5000/localimages/keepalived
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/vbmc
+ IMAGE_NAME=vbmc
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/vbmc
+ eval VBMC_IMAGE=192.168.111.1:5000/localimages/vbmc
++ VBMC_IMAGE=192.168.111.1:5000/localimages/vbmc
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ip-address-manager:main
+ IMAGE_NAME=ip-address-manager:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:main
+ eval IPAM_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:main
++ IPAM_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:main
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ IMAGE_NAME=cluster-api-provider-metal3:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ eval CAPM3_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
++ CAPM3_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ dirname /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ cd /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/..
++ pwd
+ SCRIPTDIR=/home/azureuser/go/src/github.com/metal3-io/baremetal-operator
+ IRONIC_IMAGE=192.168.111.1:5000/localimages/ironic
+ IRONIC_INSPECTOR_IMAGE=quay.io/metal3-io/ironic
+ IRONIC_KEEPALIVED_IMAGE=192.168.111.1:5000/localimages/keepalived
+ IPA_DOWNLOADER_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ MARIADB_IMAGE=quay.io/metal3-io/mariadb:main
+ IPA_BASEURI=
+ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+ CONTAINER_RUNTIME=docker
+ HTTP_PORT=6180
+ PROVISIONING_IP=172.22.0.1
+ CLUSTER_PROVISIONING_IP=172.22.0.2
+ PROVISIONING_INTERFACE=ironicendpoint
+ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+ IRONIC_KERNEL_PARAMS=console=ttyS0
+ IRONIC_BOOT_ISO_SOURCE=local
+ export NAMEPREFIX=baremetal-operator
+ NAMEPREFIX=baremetal-operator
+ IRONIC_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
+ IRONIC_CERT_FILE=/opt/metal3-dev-env/certs/ironic.crt
+ IRONIC_KEY_FILE=/opt/metal3-dev-env/certs/ironic.key
+ IRONIC_TLS_SETUP=true
+ IRONIC_INSPECTOR_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
+ IRONIC_INSPECTOR_CERT_FILE=/opt/metal3-dev-env/certs/ironic-inspector.crt
+ IRONIC_INSPECTOR_KEY_FILE=/opt/metal3-dev-env/certs/ironic-inspector.key
+ MARIADB_CACERT_FILE=/opt/metal3-dev-env/certs/ironic-ca.pem
+ MARIADB_CERT_FILE=/opt/metal3-dev-env/certs/mariadb.crt
+ MARIADB_KEY_FILE=/opt/metal3-dev-env/certs/mariadb.key
+ IPA_DOWNLOAD_ENABLED=true
+ USE_LOCAL_IPA=false
+ LOCAL_IPA_PATH=/tmp/dib
+ '[' -n /opt/metal3-dev-env/certs/mariadb.key ']'
+ chmod 604 /opt/metal3-dev-env/certs/mariadb.key
+ '[' -n /opt/metal3-dev-env/certs/ironic.crt ']'
+ export IRONIC_BASE_URL=https://172.22.0.2
+ IRONIC_BASE_URL=https://172.22.0.2
+ '[' -z /opt/metal3-dev-env/certs/ironic-ca.pem ']'
+ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+ DEPLOY_ISO_URL=
+ IRONIC_ENDPOINT=https://172.22.0.2:6385/v1/
+ IRONIC_INSPECTOR_ENDPOINT=https://172.22.0.2:5050/v1/
+ CACHEURL=http://172.22.0.1/images
+ IRONIC_FAST_TRACK=true
+ INSPECTOR_REVERSE_PROXY_SETUP=true
+ [[ true == *false* ]]
+ IRONIC_INSPECTOR_VLAN_INTERFACES=all
+ sudo mkdir -p /opt/metal3-dev-env/ironic
+ sudo mkdir -p /opt/metal3-dev-env/ironic/auth
+ cat
+ sudo tee /opt/metal3-dev-env/ironic/ironic-vars.env
HTTP_PORT=6180
PROVISIONING_IP=172.22.0.2
PROVISIONING_INTERFACE=ironicendpoint
DHCP_RANGE=172.22.0.10,172.22.0.100
DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
DEPLOY_ISO_URL=
IRONIC_ENDPOINT=https://172.22.0.2:6385/v1/
IRONIC_INSPECTOR_ENDPOINT=https://172.22.0.2:5050/v1/
CACHEURL=http://172.22.0.1/images
IRONIC_FAST_TRACK=true
IRONIC_KERNEL_PARAMS=console=ttyS0
IRONIC_BOOT_ISO_SOURCE=local
INSPECTOR_REVERSE_PROXY_SETUP=true
IRONIC_INSPECTOR_VLAN_INTERFACES=all
IPA_BASEURI=
+ '[' true == true ']'
+ cat
+ kubectl apply -f -
secret/ironic-cacert created
+ sudo docker pull 192.168.111.1:5000/localimages/ironic
Using default tag: latest
latest: Pulling from localimages/ironic
Digest: sha256:8bd2bcebdee01784b5b1318a074c5fe5750760411ad5ee3a19e47dca1685fc35
Status: Image is up to date for 192.168.111.1:5000/localimages/ironic:latest
192.168.111.1:5000/localimages/ironic:latest
+ sudo docker pull quay.io/metal3-io/ironic
Using default tag: latest
latest: Pulling from metal3-io/ironic
Digest: sha256:444518fec926d2b9bc00b1d7e77f687177025e3e8cc789cc7e0e318cc152e96f
Status: Image is up to date for quay.io/metal3-io/ironic:latest
quay.io/metal3-io/ironic:latest
+ sudo docker pull 192.168.111.1:5000/localimages/keepalived
Using default tag: latest
latest: Pulling from localimages/keepalived
Digest: sha256:4d2d44db445e898a08b072a29af18c325f92a06508b720ea9c95ecddc09c942c
Status: Image is up to date for 192.168.111.1:5000/localimages/keepalived:latest
192.168.111.1:5000/localimages/keepalived:latest
+ CERTS_MOUNTS=
+ '[' -n /opt/metal3-dev-env/certs/ironic-ca.pem ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt '
+ '[' -n /opt/metal3-dev-env/certs/ironic.crt ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt '
+ '[' -n /opt/metal3-dev-env/certs/ironic.key ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key '
+ '[' -n /opt/metal3-dev-env/certs/ironic-ca.pem ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt '
+ '[' -n /opt/metal3-dev-env/certs/ironic-inspector.crt ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt '
+ '[' -n /opt/metal3-dev-env/certs/ironic-inspector.key ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key '
+ '[' -n /opt/metal3-dev-env/certs/ironic-ca.pem ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt '
+ '[' -n /opt/metal3-dev-env/certs/mariadb.crt ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt  -v /opt/metal3-dev-env/certs/mariadb.crt:/certs/mariadb/tls.crt '
+ '[' -n /opt/metal3-dev-env/certs/mariadb.key ']'
+ CERTS_MOUNTS='-v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt  -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt  -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key  -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt  -v /opt/metal3-dev-env/certs/mariadb.crt:/certs/mariadb/tls.crt  -v /opt/metal3-dev-env/certs/mariadb.key:/certs/mariadb/tls.key '
+ BASIC_AUTH_MOUNTS=
+ IRONIC_HTPASSWD=
+ '[' -n tDqch2vdB5bP ']'
+ envsubst
+ envsubst
+ BASIC_AUTH_MOUNTS='-v /opt/metal3-dev-env/ironic/auth/ironic-auth-config:/auth/ironic/auth-config'
+ BASIC_AUTH_MOUNTS='-v /opt/metal3-dev-env/ironic/auth/ironic-auth-config:/auth/ironic/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-rpc-auth-config:/auth/ironic-rpc/auth-config'
++ htpasswd -n -b -B tDqch2vdB5bP xTubq1zZimea
+ IRONIC_HTPASSWD='--env HTTP_BASIC_HTPASSWD=tDqch2vdB5bP:$2y$05$ixVI5Eg8Um59VchqmMQda.Q3R7uuOgKteZmuxjU89akzzjr7bLu5.'
+ IRONIC_INSPECTOR_HTPASSWD=
+ '[' -n tDqch2vdB5bP ']'
+ envsubst
+ BASIC_AUTH_MOUNTS='-v /opt/metal3-dev-env/ironic/auth/ironic-auth-config:/auth/ironic/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-rpc-auth-config:/auth/ironic-rpc/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-inspector-auth-config:/auth/ironic-inspector/auth-config'
++ htpasswd -n -b -B tDqch2vdB5bP xTubq1zZimea
+ IRONIC_INSPECTOR_HTPASSWD='--env HTTP_BASIC_HTPASSWD=tDqch2vdB5bP:$2y$05$oNkP1mB/7QLknLKIRIOxYe5yKNFIsx1zebgkBL3F5.tM8DJkHelZK'
+ sudo mkdir -p /opt/metal3-dev-env/ironic/html/images
+ false
+ /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/tools/remove_local_ironic.sh
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic$'
+ sudo docker ps --all
+ grep -w 'ironic$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-api$'
+ sudo docker ps --all
+ grep -w 'ironic-api$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-conductor$'
+ sudo docker ps --all
+ grep -w 'ironic-conductor$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-inspector$'
+ sudo docker ps --all
+ grep -w 'ironic-inspector$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'dnsmasq$'
+ sudo docker ps --all
+ grep -w 'dnsmasq$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'httpd$'
+ sudo docker ps --all
+ grep -w 'httpd$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'mariadb$'
+ sudo docker ps --all
+ grep -w 'mariadb$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ipa-downloader$'
+ sudo docker ps --all
+ grep -w 'ipa-downloader$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-endpoint-keepalived$'
+ sudo docker ps --all
+ grep -w 'ironic-endpoint-keepalived$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'ironic-log-watch$'
+ sudo docker ps --all
+ grep -w 'ironic-log-watch$'
+ for name in ironic ironic-api ironic-conductor ironic-inspector dnsmasq httpd mariadb ipa-downloader ironic-endpoint-keepalived ironic-log-watch httpd-reverse-proxy
+ sudo docker ps
+ grep -w 'httpd-reverse-proxy$'
+ sudo docker ps --all
+ grep -w 'httpd-reverse-proxy$'
+ set +xe
++ sha256sum
+++ date
++ cut -c-20
+++ hostname
++ echo 'Tue Dec 21 20:57:40 UTC 2021
sidneyshiba-capm3-vm'
+ mariadb_password=c3808159205eb6dd573f
+ POD=
+ [[ docker == \p\o\d\m\a\n ]]
+ true
+ sudo docker run -d --net host --privileged --name ipa-downloader --env-file /opt/metal3-dev-env/ironic/ironic-vars.env -v /opt/metal3-dev-env/ironic:/shared 192.168.111.1:5000/localimages/ironic-ipa-downloader /usr/local/bin/get-resource.sh
16e78bb6df4fe50fa4b1f26e616074e785a7986be13de470f08de97e6f6c55e5
+ sudo docker wait ipa-downloader
0
+ sudo docker run -d --net host --privileged --name dnsmasq --env-file /opt/metal3-dev-env/ironic/ironic-vars.env -v /opt/metal3-dev-env/ironic:/shared --entrypoint /bin/rundnsmasq 192.168.111.1:5000/localimages/ironic
af660a2ff6f0e7369c5a91534795f03bc1a9f7d1aa86bc7448a9f87a8cfc520b
+ sudo docker run -d --net host --privileged --name mariadb -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.crt:/certs/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.key:/certs/mariadb/tls.key --env-file /opt/metal3-dev-env/ironic/ironic-vars.env -v /opt/metal3-dev-env/ironic:/shared --env MARIADB_PASSWORD=c3808159205eb6dd573f quay.io/metal3-io/mariadb:main
Unable to find image 'quay.io/metal3-io/mariadb:main' locally
main: Pulling from metal3-io/mariadb
203c612978b4: Already exists
e3ff3ac72de5: Already exists
758c773fb4aa: Already exists
5186e8704530: Already exists
05798f843f10: Pulling fs layer
bd1a3f433451: Pulling fs layer
8350a80a8b2f: Pulling fs layer
bd1a3f433451: Verifying Checksum
bd1a3f433451: Download complete
05798f843f10: Download complete
05798f843f10: Pull complete
bd1a3f433451: Pull complete
8350a80a8b2f: Verifying Checksum
8350a80a8b2f: Download complete
8350a80a8b2f: Pull complete
Digest: sha256:2ef2d0438a906ff1b86d06d5ffd289b87f1e4276c8c5b3ff4f7e486bad32325c
Status: Downloaded newer image for quay.io/metal3-io/mariadb:main
fea85fa1fb0817e54783318dbcde02deba06ba366c96e5651c6ffcf8d5060ca0
+ sudo docker run -d --net host --privileged --name ironic-api -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.crt:/certs/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.key:/certs/mariadb/tls.key -v /opt/metal3-dev-env/ironic/auth/ironic-auth-config:/auth/ironic/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-rpc-auth-config:/auth/ironic-rpc/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-inspector-auth-config:/auth/ironic-inspector/auth-config --env 'HTTP_BASIC_HTPASSWD=tDqch2vdB5bP:$2y$05$ixVI5Eg8Um59VchqmMQda.Q3R7uuOgKteZmuxjU89akzzjr7bLu5.' --env-file /opt/metal3-dev-env/ironic/ironic-vars.env --env MARIADB_PASSWORD=c3808159205eb6dd573f --entrypoint /bin/runironic-api -v /opt/metal3-dev-env/ironic:/shared 192.168.111.1:5000/localimages/ironic
04a4bb138c0420c631b2f6471922713564f46b0019ea41c1f1371920f6fa0828
+ sudo docker run -d --net host --privileged --name ironic-conductor -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.crt:/certs/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.key:/certs/mariadb/tls.key -v /opt/metal3-dev-env/ironic/auth/ironic-auth-config:/auth/ironic/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-rpc-auth-config:/auth/ironic-rpc/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-inspector-auth-config:/auth/ironic-inspector/auth-config --env 'HTTP_BASIC_HTPASSWD=tDqch2vdB5bP:$2y$05$ixVI5Eg8Um59VchqmMQda.Q3R7uuOgKteZmuxjU89akzzjr7bLu5.' --env-file /opt/metal3-dev-env/ironic/ironic-vars.env --env MARIADB_PASSWORD=c3808159205eb6dd573f --entrypoint /bin/runironic-conductor -v /opt/metal3-dev-env/ironic:/shared 192.168.111.1:5000/localimages/ironic
86a5d23998e450c8bb7c99c39e83f35722cde8ac3c09a5ca456712d5109227af
+ sudo docker run -d --net host --privileged --name ironic-endpoint-keepalived --env-file /opt/metal3-dev-env/ironic/ironic-vars.env -v /opt/metal3-dev-env/ironic:/shared 192.168.111.1:5000/localimages/keepalived
9229b2b31a685d33331df2679f49fb6081ec03dfe2e90bf6d9863776202af1da
+ sudo docker run -d --net host --privileged --name ironic-log-watch --entrypoint /bin/runlogwatch.sh -v /opt/metal3-dev-env/ironic:/shared 192.168.111.1:5000/localimages/ironic
4776bd21187675d49ded2ce3ded27e8efc179a5d282b540bf5edb039b818609e
+ sudo docker run -d --net host --privileged --name ironic-inspector -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.crt:/certs/ironic/tls.crt -v /opt/metal3-dev-env/certs/ironic.key:/certs/ironic/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.crt:/certs/ironic-inspector/tls.crt -v /opt/metal3-dev-env/certs/ironic-inspector.key:/certs/ironic-inspector/tls.key -v /opt/metal3-dev-env/certs/ironic-ca.pem:/certs/ca/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.crt:/certs/mariadb/tls.crt -v /opt/metal3-dev-env/certs/mariadb.key:/certs/mariadb/tls.key -v /opt/metal3-dev-env/ironic/auth/ironic-auth-config:/auth/ironic/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-rpc-auth-config:/auth/ironic-rpc/auth-config -v /opt/metal3-dev-env/ironic/auth/ironic-inspector-auth-config:/auth/ironic-inspector/auth-config --env 'HTTP_BASIC_HTPASSWD=tDqch2vdB5bP:$2y$05$oNkP1mB/7QLknLKIRIOxYe5yKNFIsx1zebgkBL3F5.tM8DJkHelZK' --env-file /opt/metal3-dev-env/ironic/ironic-vars.env --entrypoint /bin/runironic-inspector -v /opt/metal3-dev-env/ironic:/shared quay.io/metal3-io/ironic
c799a6e97c8eaf141bc4c8650f623c6e4d3a7edd739e058979544e4be767d9cf
+ mv /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/ironic-deployment/keepalived/ironic_bmo_configmap.env.orig /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/ironic-deployment/keepalived/ironic_bmo_configmap.env
+ popd
~/projects/metal3-dev-env
+ '[' kind '!=' tilt ']'
+ '[' v1beta1 == v1alpha4 ']'
+ BMO_NAME_PREFIX=baremetal-operator
+ [[ false != true ]]
+ kubectl rollout status deployment baremetal-operator-controller-manager -n baremetal-operator-system --timeout=5m
Waiting for deployment "baremetal-operator-controller-manager" rollout to finish: 0 of 1 updated replicas are available...
deployment "baremetal-operator-controller-manager" successfully rolled out
+ apply_bm_hosts
+ pushd /home/azureuser/go/src/github.com/metal3-io/baremetal-operator
~/go/src/github.com/metal3-io/baremetal-operator ~/projects/metal3-dev-env
+ list_nodes
+ make_bm_hosts
+ read -r name address user password mac
+ cat /opt/metal3-dev-env/ironic_nodes.json
+ jq '.nodes[] | {
           name,
           driver,
           address:.driver_info.address,
           port:.driver_info.port,
           user:.driver_info.username,
           password:.driver_info.password,
           mac: .ports[0].address
           } |
           .name + " " +
           .address + " " +
           .user + " " + .password + " " + .mac'
+ sed 's/"//g'
+ go run /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go -address ipmi://192.168.111.1:6230 -password password -user admin -boot-mac 00:a5:43:3a:00:e8 -boot-mode legacy node-0
+ read -r name address user password mac
+ go run /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go -address redfish+http://192.168.111.1:8000/redfish/v1/Systems/28db03d3-e978-4cf9-833d-3aad09af1181 -password password -user admin -boot-mac 00:a5:43:3a:00:ec -boot-mode legacy node-1
+ read -r name address user password mac
+ go run /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go -address ipmi://192.168.111.1:6232 -password password -user admin -boot-mac 00:a5:43:3a:00:f0 -boot-mode legacy node-2
+ read -r name address user password mac
+ go run /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go -address redfish+http://192.168.111.1:8000/redfish/v1/Systems/0ca8a968-0ff4-4ee2-891a-35ce1bb6080d -password password -user admin -boot-mac 00:a5:43:3a:00:f4 -boot-mode legacy node-3
+ read -r name address user password mac
+ go run /home/azureuser/go/src/github.com/metal3-io/baremetal-operator/cmd/make-bm-worker/main.go -address ipmi://192.168.111.1:6234 -password password -user admin -boot-mac 00:a5:43:3a:00:f8 -boot-mode legacy node-4
+ read -r name address user password mac
++ list_nodes
++ cat /opt/metal3-dev-env/ironic_nodes.json
++ jq '.nodes[] | {
           name,
           driver,
           address:.driver_info.address,
           port:.driver_info.port,
           user:.driver_info.username,
           password:.driver_info.password,
           mac: .ports[0].address
           } |
           .name + " " +
           .address + " " +
           .user + " " + .password + " " + .mac'
++ sed 's/"//g'
+ [[ -n node-0 ipmi://192.168.111.1:6230 admin password 00:a5:43:3a:00:e8
node-1 redfish+http://192.168.111.1:8000/redfish/v1/Systems/28db03d3-e978-4cf9-833d-3aad09af1181 admin password 00:a5:43:3a:00:ec
node-2 ipmi://192.168.111.1:6232 admin password 00:a5:43:3a:00:f0
node-3 redfish+http://192.168.111.1:8000/redfish/v1/Systems/0ca8a968-0ff4-4ee2-891a-35ce1bb6080d admin password 00:a5:43:3a:00:f4
node-4 ipmi://192.168.111.1:6234 admin password 00:a5:43:3a:00:f8 ]]
+ echo 'bmhosts_crs.yaml is applying'
bmhosts_crs.yaml is applying
+ kubectl apply -f /opt/metal3-dev-env/bmhosts_crs.yaml -n metal3
+ echo 'bmhosts_crs.yaml is successfully applied'
bmhosts_crs.yaml is successfully applied
+ popd
~/projects/metal3-dev-env
./04_verify.sh
Logging to ./logs/04_verify-2021-12-21-205801.log
OK - Network provisioning exists
OK - Network baremetal exists
OK - Kubernetes cluster reachable

OK - Fetch CRDs
OK - CRD clusters.cluster.x-k8s.io created
OK - CRD kubeadmconfigs.bootstrap.cluster.x-k8s.io created
OK - CRD kubeadmconfigtemplates.bootstrap.cluster.x-k8s.io created
OK - CRD machinedeployments.cluster.x-k8s.io created
OK - CRD machines.cluster.x-k8s.io created
OK - CRD machinesets.cluster.x-k8s.io created
OK - CRD baremetalhosts.metal3.io created

OK - deployments capm3-system:capm3-controller-manager created
OK - capm3-system:capm3-controller-manager deployments replicas correct
OK - deployments capi-system:capi-controller-manager created
OK - capi-system:capi-controller-manager deployments replicas correct
OK - deployments capi-kubeadm-bootstrap-system:capi-kubeadm-bootstrap-controller-manager created
OK - capi-kubeadm-bootstrap-system:capi-kubeadm-bootstrap-controller-manager deployments replicas correct
OK - deployments capi-kubeadm-control-plane-system:capi-kubeadm-control-plane-controller-manager created
OK - capi-kubeadm-control-plane-system:capi-kubeadm-control-plane-controller-manager deployments replicas correct
OK - deployments baremetal-operator-system:baremetal-operator-controller-manager created
OK - baremetal-operator-system:baremetal-operator-controller-manager deployments replicas correct
OK - Replica sets with label cluster.x-k8s.io/provider=infrastructure-metal3 created
OK - infrastructure-metal3 replicas correct for replica set 0
OK - infrastructure-metal3 replicas correct for replica set 1
OK - Replica sets with label cluster.x-k8s.io/provider=cluster-api created
OK - cluster-api replicas correct for replica set 0
OK - Replica sets with label cluster.x-k8s.io/provider=bootstrap-kubeadm created
OK - bootstrap-kubeadm replicas correct for replica set 0
OK - Replica sets with label cluster.x-k8s.io/provider=control-plane-kubeadm created
OK - control-plane-kubeadm replicas correct for replica set 0
OK - Fetch Baremetalhosts
OK - Fetch Baremetalhosts VMs

   - Waiting for task completion (up to 2400 seconds)  - Command: 'check_bm_hosts node-0 ipmi://192.168.111.1:6230 admin password 00:a5:43:3a:00:e8'
OK - node-0 Baremetalhost exist
OK - node-0 Baremetalhost address correct
OK - node-0 Baremetalhost mac address correct
OK - node-0 Baremetalhost status OK
OK - node-0 Baremetalhost credentials secret exist
OK - node-0 Baremetalhost password correct
OK - node-0 Baremetalhost user correct
OK - node-0 Baremetalhost VM exist
OK - node-0 Baremetalhost VM interface provisioning exist
OK - node-0 Baremetalhost VM interface baremetal exist
OK - node-0 Baremetalhost introspecting completed

OK - node-1 Baremetalhost exist
OK - node-1 Baremetalhost address correct
OK - node-1 Baremetalhost mac address correct
OK - node-1 Baremetalhost status OK
OK - node-1 Baremetalhost credentials secret exist
OK - node-1 Baremetalhost password correct
OK - node-1 Baremetalhost user correct
OK - node-1 Baremetalhost VM exist
OK - node-1 Baremetalhost VM interface provisioning exist
OK - node-1 Baremetalhost VM interface baremetal exist
OK - node-1 Baremetalhost introspecting completed

OK - node-2 Baremetalhost exist
OK - node-2 Baremetalhost address correct
OK - node-2 Baremetalhost mac address correct
OK - node-2 Baremetalhost status OK
OK - node-2 Baremetalhost credentials secret exist
OK - node-2 Baremetalhost password correct
OK - node-2 Baremetalhost user correct
OK - node-2 Baremetalhost VM exist
OK - node-2 Baremetalhost VM interface provisioning exist
OK - node-2 Baremetalhost VM interface baremetal exist
OK - node-2 Baremetalhost introspecting completed

OK - node-3 Baremetalhost exist
OK - node-3 Baremetalhost address correct
OK - node-3 Baremetalhost mac address correct
OK - node-3 Baremetalhost status OK
OK - node-3 Baremetalhost credentials secret exist
OK - node-3 Baremetalhost password correct
OK - node-3 Baremetalhost user correct
OK - node-3 Baremetalhost VM exist
OK - node-3 Baremetalhost VM interface provisioning exist
OK - node-3 Baremetalhost VM interface baremetal exist
OK - node-3 Baremetalhost introspecting completed

OK - node-4 Baremetalhost exist
OK - node-4 Baremetalhost address correct
OK - node-4 Baremetalhost mac address correct
OK - node-4 Baremetalhost status OK
OK - node-4 Baremetalhost credentials secret exist
OK - node-4 Baremetalhost password correct
OK - node-4 Baremetalhost user correct
OK - node-4 Baremetalhost VM exist
OK - node-4 Baremetalhost VM interface provisioning exist
OK - node-4 Baremetalhost VM interface baremetal exist
OK - node-4 Baremetalhost introspecting completed

OK - Container httpd-infra running
OK - Container registry running
OK - Container vbmc running
OK - Container sushy-tools running
OK - Ironic endpoint is secured


Number of failures : 0
azureuser@sidneyshiba-capm3-vm:~/projects/metal3-dev-env$ 
