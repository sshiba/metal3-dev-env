+ source lib/common.sh
++ [[ :/home/capm3/.krew/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
+++ go env
++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/capm3/.cache/go-build"
GOENV="/home/capm3/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/capm3/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/capm3/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build3874757364=/tmp/go-build -gno-record-gcc-switches"'
+++ GO111MODULE=
+++ GOARCH=amd64
+++ GOBIN=
+++ GOCACHE=/home/capm3/.cache/go-build
+++ GOENV=/home/capm3/.config/go/env
+++ GOEXE=
+++ GOFLAGS=
+++ GOHOSTARCH=amd64
+++ GOHOSTOS=linux
+++ GOINSECURE=
+++ GOMODCACHE=/home/capm3/go/pkg/mod
+++ GONOPROXY=
+++ GONOSUMDB=
+++ GOOS=linux
+++ GOPATH=/home/capm3/go
+++ GOPRIVATE=
+++ GOPROXY=https://proxy.golang.org,direct
+++ GOROOT=/usr/local/go
+++ GOSUMDB=sum.golang.org
+++ GOTMPDIR=
+++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
+++ GOVCS=
+++ GOVERSION=go1.16.7
+++ GCCGO=gccgo
+++ AR=ar
+++ CC=gcc
+++ CXX=g++
+++ CGO_ENABLED=1
+++ GOMOD=/dev/null
+++ CGO_CFLAGS='-g -O2'
+++ CGO_CPPFLAGS=
+++ CGO_CXXFLAGS='-g -O2'
+++ CGO_FFLAGS='-g -O2'
+++ CGO_LDFLAGS='-g -O2'
+++ PKG_CONFIG=pkg-config
+++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build3874757364=/tmp/go-build -gno-record-gcc-switches'
++ export GOPATH
++++ dirname lib/common.sh
+++ cd lib/..
+++ pwd
++ SCRIPTDIR=/home/capm3/projects/metal3-dev-env
+++ whoami
++ USER=capm3
++ export USER=capm3
++ USER=capm3
++ '[' -z '' ']'
++ '[' '!' -f /home/capm3/projects/metal3-dev-env/config_capm3.sh ']'
++ CONFIG=/home/capm3/projects/metal3-dev-env/config_capm3.sh
++ source /home/capm3/projects/metal3-dev-env/config_capm3.sh
+++ export KUBECONFIG=/home/capm3/.kube/config
+++ KUBECONFIG=/home/capm3/.kube/config
+++ export K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
+++ K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
+++ export NUM_NODES=7
+++ NUM_NODES=7
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=3
+++ NUM_OF_WORKER_REPLICAS=3
+++ export CAPM3_VERSION=v1beta1
+++ CAPM3_VERSION=v1beta1
+++ export CAPI_VERSION=v1beta1
+++ CAPI_VERSION=v1beta1
+++ export KUBERNETES_VERSION=v1.21.0
+++ KUBERNETES_VERSION=v1.21.0
+++ export UPGRADED_K8S_VERSION=v1.22.2
+++ UPGRADED_K8S_VERSION=v1.22.2
+++ export IMAGE_OS=Ubuntu
+++ IMAGE_OS=Ubuntu
+++ export IMAGE_USERNAME=metal3
+++ IMAGE_USERNAME=metal3
+++ export EPHEMERAL_CLUSTER=minikube
+++ EPHEMERAL_CLUSTER=minikube
++ export MARIADB_HOST=mariaDB
++ MARIADB_HOST=mariaDB
++ export MARIADB_HOST_IP=127.0.0.1
++ MARIADB_HOST_IP=127.0.0.1
++ ADDN_DNS=
++ EXT_IF=
++ PRO_IF=
++ MANAGE_BR_BRIDGE=y
++ MANAGE_PRO_BRIDGE=y
++ MANAGE_INT_BRIDGE=y
++ INT_IF=
++ ROOT_DISK_NAME=/dev/sda
++ NODE_HOSTNAME_FORMAT=node-%d
++ source /etc/os-release
+++ NAME=Ubuntu
+++ VERSION='20.04.3 LTS (Focal Fossa)'
+++ ID=ubuntu
+++ ID_LIKE=debian
+++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
+++ VERSION_ID=20.04
+++ HOME_URL=https://www.ubuntu.com/
+++ SUPPORT_URL=https://help.ubuntu.com/
+++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
+++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
+++ VERSION_CODENAME=focal
+++ UBUNTU_CODENAME=focal
++ export DISTRO=ubuntu20
++ DISTRO=ubuntu20
++ export OS=ubuntu
++ OS=ubuntu
++ export OS_VERSION_ID=20.04
++ OS_VERSION_ID=20.04
++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
++ export SUPPORTED_DISTROS
++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
++ [[ ubuntu == ubuntu ]]
++ export CONTAINER_RUNTIME=docker
++ CONTAINER_RUNTIME=docker
++ [[ docker == \p\o\d\m\a\n ]]
++ export POD_NAME=
++ POD_NAME=
++ export POD_NAME_INFRA=
++ POD_NAME_INFRA=
++ export SSH_KEY=/home/capm3/.ssh/id_rsa
++ SSH_KEY=/home/capm3/.ssh/id_rsa
++ export SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
++ SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
++ '[' '!' -f /home/capm3/.ssh/id_rsa ']'
++ FILESYSTEM=/
++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
++ export CAPM3_VERSION=v1beta1
++ CAPM3_VERSION=v1beta1
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ '[' v1beta1 == v1beta1 ']'
++ export CAPI_VERSION=v1beta1
++ CAPI_VERSION=v1beta1
++ export M3PATH=/home/capm3/go/src/github.com/metal3-io
++ M3PATH=/home/capm3/go/src/github.com/metal3-io
++ export BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
++ BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
++ export RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ export CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ export IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
++ IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
++ export IPAM_BASE_URL=metal3-io/ip-address-manager
++ IPAM_BASE_URL=metal3-io/ip-address-manager
++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ '[' v1beta1 == v1alpha3 ']'
++ '[' v1beta1 == v1alpha4 ']'
++ IPAMBRANCH=main
++ IPA_DOWNLOAD_ENABLED=true
++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ CAPM3BRANCH=main
++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
++ BMOBRANCH=master
++ FORCE_REPO_UPDATE=true
++ BMOCOMMIT=HEAD
++ BMO_RUN_LOCAL=false
++ CAPM3_RUN_LOCAL=false
++ WORKING_DIR=/opt/metal3-dev-env
++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
++ NODES_PLATFORM=libvirt
++ export NAMESPACE=metal3
++ NAMESPACE=metal3
++ export NUM_NODES=7
++ NUM_NODES=7
++ export NUM_OF_MASTER_REPLICAS=3
++ NUM_OF_MASTER_REPLICAS=3
++ export NUM_OF_WORKER_REPLICAS=3
++ NUM_OF_WORKER_REPLICAS=3
++ export VM_EXTRADISKS=false
++ VM_EXTRADISKS=false
++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
++ VM_EXTRADISKS_FILE_SYSTEM=ext4
++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ export NODE_DRAIN_TIMEOUT=0s
++ NODE_DRAIN_TIMEOUT=0s
++ export MAX_SURGE_VALUE=0
++ MAX_SURGE_VALUE=0
++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ export CONTAINER_REGISTRY=quay.io
++ CONTAINER_REGISTRY=quay.io
++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
++ VBMC_IMAGE=quay.io/metal3-io/vbmc
++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ export IRONIC_TLS_SETUP=true
++ IRONIC_TLS_SETUP=true
++ export IRONIC_BASIC_AUTH=true
++ IRONIC_BASIC_AUTH=true
++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
++ IRONIC_IMAGE=quay.io/metal3-io/ironic
++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ '[' v1beta1 == v1alpha4 ']'
++ export IRONIC_NAMESPACE=baremetal-operator-system
++ IRONIC_NAMESPACE=baremetal-operator-system
++ export NAMEPREFIX=baremetal-operator
++ NAMEPREFIX=baremetal-operator
++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ export OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
++ OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
++ '[' v1beta1 == v1alpha4 ']'
++ '[' v1beta1 == v1alpha5 ']'
++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
++ export DEFAULT_HOSTS_MEMORY=4096
++ DEFAULT_HOSTS_MEMORY=4096
++ export CLUSTER_NAME=test1
++ CLUSTER_NAME=test1
++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
++ CLUSTER_APIENDPOINT_IP=192.168.111.249
++ export KUBERNETES_VERSION=v1.21.0
++ KUBERNETES_VERSION=v1.21.0
++ export KUBERNETES_BINARIES_VERSION=v1.21.0
++ KUBERNETES_BINARIES_VERSION=v1.21.0
++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ '[' docker == docker ']'
++ export EPHEMERAL_CLUSTER=minikube
++ EPHEMERAL_CLUSTER=minikube
++ export KUSTOMIZE_VERSION=v4.1.3
++ KUSTOMIZE_VERSION=v4.1.3
++ export KIND_VERSION=v0.11.1
++ KIND_VERSION=v0.11.1
++ '[' v1.21.0 == v1.21.2 ']'
++ export KIND_NODE_IMAGE_VERSION=v1.22.2
++ KIND_NODE_IMAGE_VERSION=v1.22.2
++ export MINIKUBE_VERSION=v1.23.2
++ MINIKUBE_VERSION=v1.23.2
++ export ANSIBLE_VERSION=4.8.0
++ ANSIBLE_VERSION=4.8.0
++ SKIP_RETRIES=false
++ TEST_TIME_INTERVAL=10
++ TEST_MAX_TIME=240
++ FAILS=0
++ RESULT_STR=
++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ '[' 7 -lt 6 ']'
++ export LIBVIRT_DEFAULT_URI=qemu:///system
++ LIBVIRT_DEFAULT_URI=qemu:///system
++ '[' capm3 '!=' root ']'
++ '[' /run/user/1000 == /run/user/0 ']'
++ sudo -n uptime
++ export USE_FIREWALLD=False
++ USE_FIREWALLD=False
++ [[ ubuntu20 == \r\h\e\l\8 ]]
++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
+++ df / --output=fstype
+++ tail -n 1
++ FSTYPE=ext4
++ case ${FSTYPE} in
++ '[' '!' -d /opt/metal3-dev-env ']'
+ source lib/network.sh
++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ export POD_CIDR=192.168.0.0/18
++ POD_CIDR=192.168.0.0/18
++ PROVISIONING_IPV6=false
++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
++ [[ false == \t\r\u\e ]]
++ export BOOT_MODE=legacy
++ BOOT_MODE=legacy
++ export PROVISIONING_NETWORK=172.22.0.0/24
++ PROVISIONING_NETWORK=172.22.0.0/24
++ [[ legacy == \l\e\g\a\c\y ]]
++ export LIBVIRT_FIRMWARE=bios
++ LIBVIRT_FIRMWARE=bios
++ export LIBVIRT_SECURE_BOOT=false
++ LIBVIRT_SECURE_BOOT=false
++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
++ resultvar=PROVISIONING_CIDR
++ network=172.22.0.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
++ result=24
++ eval PROVISIONING_CIDR=24
+++ PROVISIONING_CIDR=24
++ export PROVISIONING_CIDR
++ export PROVISIONING_CIDR
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").netmask)'
++ export PROVISIONING_NETMASK=255.255.255.0
++ PROVISIONING_NETMASK=255.255.255.0
++ network_address PROVISIONING_IP 172.22.0.0/24 1
++ resultvar=PROVISIONING_IP
++ network=172.22.0.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
++ result=172.22.0.1
++ eval PROVISIONING_IP=172.22.0.1
+++ PROVISIONING_IP=172.22.0.1
++ export PROVISIONING_IP
++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
++ resultvar=CLUSTER_PROVISIONING_IP
++ network=172.22.0.0/24
++ record=2
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
++ result=172.22.0.2
++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
+++ CLUSTER_PROVISIONING_IP=172.22.0.2
++ export CLUSTER_PROVISIONING_IP
++ export PROVISIONING_IP
++ export CLUSTER_PROVISIONING_IP
++ [[ 172.22.0.1 == *\:* ]]
++ export PROVISIONING_URL_HOST=172.22.0.1
++ PROVISIONING_URL_HOST=172.22.0.1
++ export CLUSTER_URL_HOST=172.22.0.2
++ CLUSTER_URL_HOST=172.22.0.2
++ [[ 192.168.111.249 == *\:* ]]
++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ export CLUSTER_APIENDPOINT_PORT=6443
++ CLUSTER_APIENDPOINT_PORT=6443
++ network_address dhcp_range_start 172.22.0.0/24 10
++ resultvar=dhcp_range_start
++ network=172.22.0.0/24
++ record=10
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
++ result=172.22.0.10
++ eval dhcp_range_start=172.22.0.10
+++ dhcp_range_start=172.22.0.10
++ export dhcp_range_start
++ network_address dhcp_range_end 172.22.0.0/24 100
++ resultvar=dhcp_range_end
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval dhcp_range_end=172.22.0.100
+++ dhcp_range_end=172.22.0.100
++ export dhcp_range_end
++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
++ resultvar=PROVISIONING_POOL_RANGE_START
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ PROVISIONING_POOL_RANGE_START=172.22.0.100
++ export PROVISIONING_POOL_RANGE_START
++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
++ resultvar=PROVISIONING_POOL_RANGE_END
++ network=172.22.0.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
++ result=172.22.0.200
++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ PROVISIONING_POOL_RANGE_END=172.22.0.200
++ export PROVISIONING_POOL_RANGE_END
++ export PROVISIONING_POOL_RANGE_START
++ export PROVISIONING_POOL_RANGE_END
++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ EXTERNAL_SUBNET=
++ [[ -n '' ]]
++ export IP_STACK=v4
++ IP_STACK=v4
++ [[ v4 == \v\4 ]]
++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
++ EXTERNAL_SUBNET_V4=192.168.111.0/24
++ export EXTERNAL_SUBNET_V6=
++ EXTERNAL_SUBNET_V6=
++ [[ minikube == \m\i\n\i\k\u\b\e ]]
++ [[ -n '' ]]
++ [[ -n 192.168.111.0/24 ]]
++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
++ network=192.168.111.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
++ result=24
++ eval EXTERNAL_SUBNET_V4_PREFIX=24
+++ EXTERNAL_SUBNET_V4_PREFIX=24
++ export EXTERNAL_SUBNET_V4_PREFIX
++ export EXTERNAL_SUBNET_V4_PREFIX
++ [[ -z '' ]]
++ network_address EXTERNAL_SUBNET_V4_HOST 192.168.111.0/24 1
++ resultvar=EXTERNAL_SUBNET_V4_HOST
++ network=192.168.111.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 1 - 1, None)))'
++ result=192.168.111.1
++ eval EXTERNAL_SUBNET_V4_HOST=192.168.111.1
+++ EXTERNAL_SUBNET_V4_HOST=192.168.111.1
++ export EXTERNAL_SUBNET_V4_HOST
++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
++ resultvar=VIRSH_DHCP_V4_START
++ network=192.168.111.0/24
++ record=20
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
++ result=192.168.111.20
++ eval VIRSH_DHCP_V4_START=192.168.111.20
+++ VIRSH_DHCP_V4_START=192.168.111.20
++ export VIRSH_DHCP_V4_START
++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
++ resultvar=VIRSH_DHCP_V4_END
++ network=192.168.111.0/24
++ record=60
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
++ result=192.168.111.60
++ eval VIRSH_DHCP_V4_END=192.168.111.60
+++ VIRSH_DHCP_V4_END=192.168.111.60
++ export VIRSH_DHCP_V4_END
++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
++ resultvar=BAREMETALV4_POOL_RANGE_START
++ network=192.168.111.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
++ result=192.168.111.100
++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
++ export BAREMETALV4_POOL_RANGE_START
++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
++ resultvar=BAREMETALV4_POOL_RANGE_END
++ network=192.168.111.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
++ result=192.168.111.200
++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
++ export BAREMETALV4_POOL_RANGE_END
++ export VIRSH_DHCP_V4_START
++ export VIRSH_DHCP_V4_END
++ export BAREMETALV4_POOL_RANGE_START
++ export BAREMETALV4_POOL_RANGE_END
++ [[ -n '' ]]
++ export EXTERNAL_SUBNET_V6_HOST=
++ EXTERNAL_SUBNET_V6_HOST=
++ export EXTERNAL_SUBNET_V6_PREFIX=
++ EXTERNAL_SUBNET_V6_PREFIX=
++ export BAREMETALV6_POOL_RANGE_START=
++ BAREMETALV6_POOL_RANGE_START=
++ export BAREMETALV6_POOL_RANGE_END=
++ BAREMETALV6_POOL_RANGE_END=
++ export REGISTRY_PORT=5000
++ REGISTRY_PORT=5000
++ export HTTP_PORT=6180
++ HTTP_PORT=6180
++ export IRONIC_INSPECTOR_PORT=5050
++ IRONIC_INSPECTOR_PORT=5050
++ export IRONIC_API_PORT=6385
++ IRONIC_API_PORT=6385
++ [[ -n 192.168.111.1 ]]
++ export REGISTRY=192.168.111.1:5000
++ REGISTRY=192.168.111.1:5000
++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
++ resultvar=INITIAL_IRONICBRIDGE_IP
++ network=172.22.0.0/24
++ record=9
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
++ result=172.22.0.9
++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
++ export INITIAL_IRONICBRIDGE_IP
++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ '[' true == true ']'
++ export IRONIC_URL=https://172.22.0.2:6385/v1/
++ IRONIC_URL=https://172.22.0.2:6385/v1/
++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+ sudo '[' '!' -f /root/.ssh/id_rsa_virt_power ']'
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e working_dir=/opt/metal3-dev-env -e num_nodes=7 -e extradisks=false -e virthost=capm3 -e platform=libvirt -e libvirt_firmware=bios -e libvirt_secure_boot=false -e default_memory=4096 -e manage_baremetal=y -e provisioning_url_host=172.22.0.1 -e nodes_file=/opt/metal3-dev-env/ironic_nodes.json -e node_hostname_format=node-%d -i vm-setup/inventory.ini -b vm-setup/setup-playbook.yml
[0;35m[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names [0m
[0;35mto new standard, use callbacks_enabled instead. This feature will be removed [0m
[0;35mfrom ansible-core in version 2.15. Deprecation warnings can be disabled by [0m
[0;35msetting deprecation_warnings=False in ansible.cfg.[0m

PLAY [Setup dummy baremetal VMs] ***********************************************
Tuesday 09 November 2021  21:47:34 +0000 (0:00:00.012)       0:00:00.012 ****** 

TASK [Gathering Facts] *********************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:01.302)       0:00:01.315 ****** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:00.061)       0:00:01.376 ****** 

TASK [common : Set an empty default for vm_nodes if not already defined] *******
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:00.040)       0:00:01.417 ****** 

TASK [common : Populate vm_nodes if not already defined] ***********************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/common/tasks/vm_nodes_tasks.yml for localhost => (item={'key': 'node', 'value': {'memory': '4096', 'disk': '50', 'vcpu': '2', 'extradisks': False}})[0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:00.087)       0:00:01.505 ****** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:00.059)       0:00:01.564 ****** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost] => (item=0)[0m
[0;32mok: [localhost] => (item=1)[0m
[0;32mok: [localhost] => (item=2)[0m
[0;32mok: [localhost] => (item=3)[0m
[0;32mok: [localhost] => (item=4)[0m
[0;32mok: [localhost] => (item=5)[0m
[0;32mok: [localhost] => (item=6)[0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:00.132)       0:00:01.696 ****** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:35 +0000 (0:00:00.061)       0:00:01.757 ****** 

TASK [common : debug] **********************************************************
[0;32mok: [localhost] => {[0m
[0;32m    "vm_nodes": [[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_0",[0m
[0;32m            "virtualbmc_port": 6230[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_1",[0m
[0;32m            "virtualbmc_port": 6231[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_2",[0m
[0;32m            "virtualbmc_port": 6232[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_3",[0m
[0;32m            "virtualbmc_port": 6233[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_4",[0m
[0;32m            "virtualbmc_port": 6234[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_5",[0m
[0;32m            "virtualbmc_port": 6235[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_6",[0m
[0;32m            "virtualbmc_port": 6236[0m
[0;32m        }[0m
[0;32m    ][0m
[0;32m}[0m
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.069)       0:00:01.827 ****** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.061)       0:00:01.888 ****** 
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.036)       0:00:01.925 ****** 
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.078)       0:00:02.003 ****** 
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.042)       0:00:02.046 ****** 

TASK [libvirt : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/install_setup_tasks.yml for localhost[0m
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.047)       0:00:02.094 ****** 

TASK [libvirt : Start libvirtd] ************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:36 +0000 (0:00:00.613)       0:00:02.707 ****** 

TASK [libvirt : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/network_setup_tasks.yml for localhost[0m
Tuesday 09 November 2021  21:47:37 +0000 (0:00:00.062)       0:00:02.770 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Tuesday 09 November 2021  21:47:37 +0000 (0:00:00.106)       0:00:02.876 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Tuesday 09 November 2021  21:47:37 +0000 (0:00:00.167)       0:00:03.043 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : get a list of MACs to use] *************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:37 +0000 (0:00:00.379)       0:00:03.423 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Create libvirt networks] ***************************************
[0;33mchanged: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;33mchanged: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Tuesday 09 November 2021  21:47:38 +0000 (0:00:00.827)       0:00:04.251 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Start libvirt networks] ****************************************
[0;33mchanged: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;33mchanged: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Tuesday 09 November 2021  21:47:39 +0000 (0:00:00.673)       0:00:04.925 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Mark  libvirt networks as autostarted] *************************
[0;33mchanged: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;33mchanged: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Tuesday 09 November 2021  21:47:39 +0000 (0:00:00.590)       0:00:05.515 ****** 
Tuesday 09 November 2021  21:47:39 +0000 (0:00:00.040)       0:00:05.555 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Tuesday 09 November 2021  21:47:39 +0000 (0:00:00.155)       0:00:05.711 ****** 
Tuesday 09 November 2021  21:47:40 +0000 (0:00:00.111)       0:00:05.822 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Tuesday 09 November 2021  21:47:40 +0000 (0:00:00.132)       0:00:05.955 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Tuesday 09 November 2021  21:47:40 +0000 (0:00:00.133)       0:00:06.088 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Tuesday 09 November 2021  21:47:40 +0000 (0:00:00.131)       0:00:06.219 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Whitelist bridges for unprivileged access on Ubuntu or Fedora] ***
[0;32mok: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;32mok: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Tuesday 09 November 2021  21:47:41 +0000 (0:00:00.588)       0:00:06.808 ****** 

TASK [libvirt : Ensure remote working dir exists] ******************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:41 +0000 (0:00:00.398)       0:00:07.206 ****** 

TASK [libvirt : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/vm_setup_tasks.yml for localhost[0m
Tuesday 09 November 2021  21:47:41 +0000 (0:00:00.064)       0:00:07.271 ****** 

TASK [libvirt : ensure libvirt volume path exists] *****************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:41 +0000 (0:00:00.237)       0:00:07.508 ****** 

TASK [libvirt : Check volume pool] *********************************************
[1;30mtask path: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/vm_setup_tasks.yml:12[0m
[0;31mfatal: [localhost]: FAILED! => {"changed": false, "cmd": ["virsh", "pool-uuid", "oooq_pool"], "delta": "0:00:00.019467", "end": "2021-11-09 21:47:42.077737", "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:42.058270", "stderr": "error: failed to get pool 'oooq_pool'\nerror: Storage pool not found: no storage pool with matching name 'oooq_pool'", "stderr_lines": ["error: failed to get pool 'oooq_pool'", "error: Storage pool not found: no storage pool with matching name 'oooq_pool'"], "stdout": "", "stdout_lines": []}[0m
[0;36m...ignoring[0m
Tuesday 09 November 2021  21:47:42 +0000 (0:00:00.372)       0:00:07.881 ****** 

TASK [libvirt : create the volume pool xml file] *******************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:42 +0000 (0:00:00.629)       0:00:08.511 ****** 

TASK [libvirt : Define volume pool] ********************************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:43 +0000 (0:00:00.291)       0:00:08.802 ****** 

TASK [libvirt : Start volume pool] *********************************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:43 +0000 (0:00:00.432)       0:00:09.234 ****** 

TASK [libvirt : ensure tripleo-quickstart volume pool is defined] **************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:43 +0000 (0:00:00.323)       0:00:09.557 ****** 

TASK [libvirt : Mark volume pool for autostart] ********************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:44 +0000 (0:00:00.272)       0:00:09.830 ****** 

TASK [libvirt : Check if vm volumes exist] *************************************
[0;31mfailed: [localhost] (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_0.qcow2"], "delta": "0:00:00.020926", "end": "2021-11-09 21:47:44.307462", "item": {"flavor": "node", "name": "node_0", "virtualbmc_port": 6230}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:44.286536", "stderr": "error: failed to get vol 'node_0.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_0.qcow2'", "stderr_lines": ["error: failed to get vol 'node_0.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_0.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_1.qcow2"], "delta": "0:00:00.020396", "end": "2021-11-09 21:47:44.521667", "item": {"flavor": "node", "name": "node_1", "virtualbmc_port": 6231}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:44.501271", "stderr": "error: failed to get vol 'node_1.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_1.qcow2'", "stderr_lines": ["error: failed to get vol 'node_1.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_1.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_2.qcow2"], "delta": "0:00:00.019611", "end": "2021-11-09 21:47:44.759422", "item": {"flavor": "node", "name": "node_2", "virtualbmc_port": 6232}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:44.739811", "stderr": "error: failed to get vol 'node_2.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_2.qcow2'", "stderr_lines": ["error: failed to get vol 'node_2.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_2.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_3.qcow2"], "delta": "0:00:00.019931", "end": "2021-11-09 21:47:44.968773", "item": {"flavor": "node", "name": "node_3", "virtualbmc_port": 6233}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:44.948842", "stderr": "error: failed to get vol 'node_3.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_3.qcow2'", "stderr_lines": ["error: failed to get vol 'node_3.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_3.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_4.qcow2"], "delta": "0:00:00.020515", "end": "2021-11-09 21:47:45.181901", "item": {"flavor": "node", "name": "node_4", "virtualbmc_port": 6234}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:45.161386", "stderr": "error: failed to get vol 'node_4.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_4.qcow2'", "stderr_lines": ["error: failed to get vol 'node_4.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_4.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_5.qcow2"], "delta": "0:00:00.020471", "end": "2021-11-09 21:47:45.392842", "item": {"flavor": "node", "name": "node_5", "virtualbmc_port": 6235}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:45.372371", "stderr": "error: failed to get vol 'node_5.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_5.qcow2'", "stderr_lines": ["error: failed to get vol 'node_5.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_5.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_6.qcow2"], "delta": "0:00:00.019768", "end": "2021-11-09 21:47:45.600331", "item": {"flavor": "node", "name": "node_6", "virtualbmc_port": 6236}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-09 21:47:45.580563", "stderr": "error: failed to get vol 'node_6.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_6.qcow2'", "stderr_lines": ["error: failed to get vol 'node_6.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_6.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;36m...ignoring[0m
Tuesday 09 November 2021  21:47:45 +0000 (0:00:01.581)       0:00:11.412 ****** 

TASK [libvirt : Create vm vm storage] ******************************************
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_0.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_0.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_0.qcow2'], 'start': '2021-11-09 21:47:44.286536', 'end': '2021-11-09 21:47:44.307462', 'delta': '0:00:00.020926', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_0.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_0.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_0.qcow2'"], 'item': {'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_1.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_1.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_1.qcow2'], 'start': '2021-11-09 21:47:44.501271', 'end': '2021-11-09 21:47:44.521667', 'delta': '0:00:00.020396', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_1.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_1.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_1.qcow2'"], 'item': {'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_2.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_2.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_2.qcow2'], 'start': '2021-11-09 21:47:44.739811', 'end': '2021-11-09 21:47:44.759422', 'delta': '0:00:00.019611', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_2.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_2.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_2.qcow2'"], 'item': {'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_3.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_3.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_3.qcow2'], 'start': '2021-11-09 21:47:44.948842', 'end': '2021-11-09 21:47:44.968773', 'delta': '0:00:00.019931', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_3.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_3.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_3.qcow2'"], 'item': {'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_4.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_4.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_4.qcow2'], 'start': '2021-11-09 21:47:45.161386', 'end': '2021-11-09 21:47:45.181901', 'delta': '0:00:00.020515', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_4.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_4.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_4.qcow2'"], 'item': {'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_5.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_5.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_5.qcow2'], 'start': '2021-11-09 21:47:45.372371', 'end': '2021-11-09 21:47:45.392842', 'delta': '0:00:00.020471', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_5.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_5.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_5.qcow2'"], 'item': {'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_6.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_6.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_6.qcow2'], 'start': '2021-11-09 21:47:45.580563', 'end': '2021-11-09 21:47:45.600331', 'delta': '0:00:00.019768', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_6.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_6.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_6.qcow2'"], 'item': {'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236}, 'ansible_loop_var': 'item'})[0m
Tuesday 09 November 2021  21:47:47 +0000 (0:00:01.965)       0:00:13.378 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Define vm vms] *************************************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236})[0m
[1;35m[WARNING]: 'xml' is given - ignoring 'name'[0m
Tuesday 09 November 2021  21:47:52 +0000 (0:00:04.956)       0:00:18.334 ****** 
Tuesday 09 November 2021  21:47:52 +0000 (0:00:00.152)       0:00:18.487 ****** 
Tuesday 09 November 2021  21:47:52 +0000 (0:00:00.153)       0:00:18.640 ****** 
Tuesday 09 November 2021  21:47:53 +0000 (0:00:00.134)       0:00:18.774 ****** 

TASK [libvirt : Get vm uuid] ***************************************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236})[0m
Tuesday 09 November 2021  21:47:54 +0000 (0:00:01.908)       0:00:20.683 ****** 

TASK [libvirt : set_fact] ******************************************************
[0;32mok: [localhost] => (item={'changed': True, 'stdout': '581661b6-d70e-4388-a18b-901f03da4937', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_0'], 'start': '2021-11-09 21:47:53.228482', 'end': '2021-11-09 21:47:53.248214', 'delta': '0:00:00.019732', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_0"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['581661b6-d70e-4388-a18b-901f03da4937'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': '659d2f25-02e2-4569-ba33-0d0130ccfe9f', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_1'], 'start': '2021-11-09 21:47:53.763430', 'end': '2021-11-09 21:47:53.783601', 'delta': '0:00:00.020171', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_1"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['659d2f25-02e2-4569-ba33-0d0130ccfe9f'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': 'c8d3b74e-4947-4aa9-9050-1ec0423d02f0', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_2'], 'start': '2021-11-09 21:47:53.975972', 'end': '2021-11-09 21:47:53.995935', 'delta': '0:00:00.019963', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_2"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['c8d3b74e-4947-4aa9-9050-1ec0423d02f0'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': '4144d82a-3c80-40e2-86fc-be0104de6ad8', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_3'], 'start': '2021-11-09 21:47:54.189487', 'end': '2021-11-09 21:47:54.209859', 'delta': '0:00:00.020372', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_3"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['4144d82a-3c80-40e2-86fc-be0104de6ad8'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': 'd9673b35-6902-4474-9e35-78878ec522dc', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_4'], 'start': '2021-11-09 21:47:54.402087', 'end': '2021-11-09 21:47:54.422483', 'delta': '0:00:00.020396', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_4"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['d9673b35-6902-4474-9e35-78878ec522dc'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': 'ca910380-c27b-48df-8740-400f066b2bdc', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_5'], 'start': '2021-11-09 21:47:54.612031', 'end': '2021-11-09 21:47:54.631743', 'delta': '0:00:00.019712', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_5"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['ca910380-c27b-48df-8740-400f066b2bdc'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': 'b6ca7042-9c08-4e9c-821d-4882cae2cc87', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_6'], 'start': '2021-11-09 21:47:54.849959', 'end': '2021-11-09 21:47:54.869437', 'delta': '0:00:00.019478', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_6"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['b6ca7042-9c08-4e9c-821d-4882cae2cc87'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236}, 'ansible_loop_var': 'item'})[0m
Tuesday 09 November 2021  21:47:55 +0000 (0:00:00.151)       0:00:20.835 ****** 

TASK [libvirt : set_fact BMC Driver] *******************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:55 +0000 (0:00:00.112)       0:00:20.947 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Write ironic node json files] **********************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:55 +0000 (0:00:00.592)       0:00:21.539 ****** 
Tuesday 09 November 2021  21:47:55 +0000 (0:00:00.039)       0:00:21.579 ****** 
Tuesday 09 November 2021  21:47:55 +0000 (0:00:00.045)       0:00:21.625 ****** 

TASK [virtbmc : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/virtbmc/tasks/setup_tasks.yml for localhost[0m
Tuesday 09 November 2021  21:47:55 +0000 (0:00:00.077)       0:00:21.702 ****** 

TASK [virtbmc : Create VirtualBMC directories] *********************************
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc/conf)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc/log)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/sushy-tools)[0m
Tuesday 09 November 2021  21:47:56 +0000 (0:00:01.052)       0:00:22.754 ****** 

TASK [virtbmc : Create VirtualBMC configuration file] **************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:57 +0000 (0:00:00.418)       0:00:23.173 ****** 

TASK [virtbmc : get virthost non_root_user userid] *****************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:47:57 +0000 (0:00:00.288)       0:00:23.462 ****** 

TASK [virtbmc : set fact on non_root_user_uid] *********************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:57 +0000 (0:00:00.064)       0:00:23.526 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [virtbmc : set vbmc address (v4) if there is a (nat) network defined with an address] ***
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:57 +0000 (0:00:00.122)       0:00:23.649 ****** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [virtbmc : set vbmc address (v6) if there is a (nat) network defined with an address] ***
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:58 +0000 (0:00:00.124)       0:00:23.774 ****** 

TASK [virtbmc : set vbmc address from IPv4 networks if possible, otherwise IPv6] ***
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:58 +0000 (0:00:00.084)       0:00:23.859 ****** 

TASK [virtbmc : set qemu uri for qemu:///system usage] *************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:47:58 +0000 (0:00:00.088)       0:00:23.947 ****** 
Tuesday 09 November 2021  21:47:58 +0000 (0:00:00.065)       0:00:24.012 ****** 

TASK [virtbmc : Create VirtualBMC directories] *********************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236})[0m
Tuesday 09 November 2021  21:47:59 +0000 (0:00:01.484)       0:00:25.497 ****** 

TASK [virtbmc : Create the Virtual BMCs] ***************************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_4', 'flavor': 'node', 'virtualbmc_port': 6234})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_5', 'flavor': 'node', 'virtualbmc_port': 6235})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_6', 'flavor': 'node', 'virtualbmc_port': 6236})[0m
Tuesday 09 November 2021  21:48:02 +0000 (0:00:02.609)       0:00:28.107 ****** 

TASK [virtbmc : Create a password file for Redfish Virtual BMCs] ***************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:48:03 +0000 (0:00:00.683)       0:00:28.791 ****** 

TASK [virtbmc : Create the Redfish Virtual BMCs] *******************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:48:03 +0000 (0:00:00.416)       0:00:29.207 ****** 

PLAY RECAP *********************************************************************
[0;33mlocalhost[0m                  : [0;32mok=46  [0m [0;33mchanged=19  [0m unreachable=0    failed=0    [0;36mskipped=18  [0m rescued=0    [1;35mignored=2   [0m

Tuesday 09 November 2021  21:48:03 +0000 (0:00:00.076)       0:00:29.283 ****** 
=============================================================================== 
libvirt : Define vm vms ------------------------------------------------- 4.96s
virtbmc : Create the Virtual BMCs --------------------------------------- 2.61s
libvirt : Create vm vm storage ------------------------------------------ 1.97s
+ sudo virsh pool-uuid default
+ [[ ubuntu == ubuntu ]]
+ source ubuntu_bridge_network_configuration.sh
++ set -xe
++ source lib/logging.sh
++++ dirname ./02_configure_host.sh
+++ LOGDIR=./logs
+++ '[' '!' -d ./logs ']'
++++ basename ./02_configure_host.sh .sh
++++ date +%F-%H%M%S
+++ LOGFILE=./logs/02_configure_host-2021-11-09-214803.log
+++ echo 'Logging to ./logs/02_configure_host-2021-11-09-214803.log'
Logging to ./logs/02_configure_host-2021-11-09-214803.log
+++ exec
++++ tee ./logs/02_configure_host-2021-11-09-214803.log
++ source lib/common.sh
+++ [[ :/home/capm3/.krew/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
++++ go env
+++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/capm3/.cache/go-build"
GOENV="/home/capm3/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/capm3/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/capm3/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build897008526=/tmp/go-build -gno-record-gcc-switches"'
++++ GO111MODULE=
++++ GOARCH=amd64
++++ GOBIN=
++++ GOCACHE=/home/capm3/.cache/go-build
++++ GOENV=/home/capm3/.config/go/env
++++ GOEXE=
++++ GOFLAGS=
++++ GOHOSTARCH=amd64
++++ GOHOSTOS=linux
++++ GOINSECURE=
++++ GOMODCACHE=/home/capm3/go/pkg/mod
++++ GONOPROXY=
++++ GONOSUMDB=
++++ GOOS=linux
++++ GOPATH=/home/capm3/go
++++ GOPRIVATE=
++++ GOPROXY=https://proxy.golang.org,direct
++++ GOROOT=/usr/local/go
++++ GOSUMDB=sum.golang.org
++++ GOTMPDIR=
++++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
++++ GOVCS=
++++ GOVERSION=go1.16.7
++++ GCCGO=gccgo
++++ AR=ar
++++ CC=gcc
++++ CXX=g++
++++ CGO_ENABLED=1
++++ GOMOD=/dev/null
++++ CGO_CFLAGS='-g -O2'
++++ CGO_CPPFLAGS=
++++ CGO_CXXFLAGS='-g -O2'
++++ CGO_FFLAGS='-g -O2'
++++ CGO_LDFLAGS='-g -O2'
++++ PKG_CONFIG=pkg-config
++++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build897008526=/tmp/go-build -gno-record-gcc-switches'
+++ export GOPATH
+++++ dirname lib/common.sh
++++ cd lib/..
++++ pwd
+++ SCRIPTDIR=/home/capm3/projects/metal3-dev-env
++++ whoami
+++ USER=capm3
+++ export USER=capm3
+++ USER=capm3
+++ '[' -z /home/capm3/projects/metal3-dev-env/config_capm3.sh ']'
+++ source /home/capm3/projects/metal3-dev-env/config_capm3.sh
++++ export KUBECONFIG=/home/capm3/.kube/config
++++ KUBECONFIG=/home/capm3/.kube/config
++++ export K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
++++ K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
++++ export NUM_NODES=7
++++ NUM_NODES=7
++++ export NUM_OF_MASTER_REPLICAS=3
++++ NUM_OF_MASTER_REPLICAS=3
++++ export NUM_OF_WORKER_REPLICAS=3
++++ NUM_OF_WORKER_REPLICAS=3
++++ export CAPM3_VERSION=v1beta1
++++ CAPM3_VERSION=v1beta1
++++ export CAPI_VERSION=v1beta1
++++ CAPI_VERSION=v1beta1
++++ export KUBERNETES_VERSION=v1.21.0
++++ KUBERNETES_VERSION=v1.21.0
++++ export UPGRADED_K8S_VERSION=v1.22.2
++++ UPGRADED_K8S_VERSION=v1.22.2
++++ export IMAGE_OS=Ubuntu
++++ IMAGE_OS=Ubuntu
++++ export IMAGE_USERNAME=metal3
++++ IMAGE_USERNAME=metal3
++++ export EPHEMERAL_CLUSTER=minikube
++++ EPHEMERAL_CLUSTER=minikube
+++ export MARIADB_HOST=mariaDB
+++ MARIADB_HOST=mariaDB
+++ export MARIADB_HOST_IP=127.0.0.1
+++ MARIADB_HOST_IP=127.0.0.1
+++ ADDN_DNS=
+++ EXT_IF=
+++ PRO_IF=
+++ MANAGE_BR_BRIDGE=y
+++ MANAGE_PRO_BRIDGE=y
+++ MANAGE_INT_BRIDGE=y
+++ INT_IF=
+++ ROOT_DISK_NAME=/dev/sda
+++ NODE_HOSTNAME_FORMAT=node-%d
+++ source /etc/os-release
++++ NAME=Ubuntu
++++ VERSION='20.04.3 LTS (Focal Fossa)'
++++ ID=ubuntu
++++ ID_LIKE=debian
++++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
++++ VERSION_ID=20.04
++++ HOME_URL=https://www.ubuntu.com/
++++ SUPPORT_URL=https://help.ubuntu.com/
++++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
++++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
++++ VERSION_CODENAME=focal
++++ UBUNTU_CODENAME=focal
+++ export DISTRO=ubuntu20
+++ DISTRO=ubuntu20
+++ export OS=ubuntu
+++ OS=ubuntu
+++ export OS_VERSION_ID=20.04
+++ OS_VERSION_ID=20.04
+++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
+++ export SUPPORTED_DISTROS
+++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
+++ [[ ubuntu == ubuntu ]]
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ [[ docker == \p\o\d\m\a\n ]]
+++ export POD_NAME=
+++ POD_NAME=
+++ export POD_NAME_INFRA=
+++ POD_NAME_INFRA=
+++ export SSH_KEY=/home/capm3/.ssh/id_rsa
+++ SSH_KEY=/home/capm3/.ssh/id_rsa
+++ export SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
+++ SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
+++ '[' '!' -f /home/capm3/.ssh/id_rsa ']'
+++ FILESYSTEM=/
+++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
+++ export CAPM3_VERSION=v1beta1
+++ CAPM3_VERSION=v1beta1
+++ '[' v1beta1 == v1alpha4 ']'
+++ '[' v1beta1 == v1alpha5 ']'
+++ '[' v1beta1 == v1beta1 ']'
+++ export CAPI_VERSION=v1beta1
+++ CAPI_VERSION=v1beta1
+++ export M3PATH=/home/capm3/go/src/github.com/metal3-io
+++ M3PATH=/home/capm3/go/src/github.com/metal3-io
+++ export BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
+++ BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
+++ export RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ export CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
+++ CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
+++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
+++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
+++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
+++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
+++ export IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
+++ IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
+++ export IPAM_BASE_URL=metal3-io/ip-address-manager
+++ IPAM_BASE_URL=metal3-io/ip-address-manager
+++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
+++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
+++ '[' v1beta1 == v1alpha3 ']'
+++ '[' v1beta1 == v1alpha4 ']'
+++ IPAMBRANCH=main
+++ IPA_DOWNLOAD_ENABLED=true
+++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
+++ '[' v1beta1 == v1alpha4 ']'
+++ '[' v1beta1 == v1alpha5 ']'
+++ CAPM3BRANCH=main
+++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
+++ BMOBRANCH=master
+++ FORCE_REPO_UPDATE=true
+++ BMOCOMMIT=HEAD
+++ BMO_RUN_LOCAL=false
+++ CAPM3_RUN_LOCAL=false
+++ WORKING_DIR=/opt/metal3-dev-env
+++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
+++ NODES_PLATFORM=libvirt
+++ export NAMESPACE=metal3
+++ NAMESPACE=metal3
+++ export NUM_NODES=7
+++ NUM_NODES=7
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=3
+++ NUM_OF_WORKER_REPLICAS=3
+++ export VM_EXTRADISKS=false
+++ VM_EXTRADISKS=false
+++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
+++ VM_EXTRADISKS_FILE_SYSTEM=ext4
+++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
+++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
+++ export NODE_DRAIN_TIMEOUT=0s
+++ NODE_DRAIN_TIMEOUT=0s
+++ export MAX_SURGE_VALUE=0
+++ MAX_SURGE_VALUE=0
+++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
+++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
+++ export CONTAINER_REGISTRY=quay.io
+++ CONTAINER_REGISTRY=quay.io
+++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
+++ VBMC_IMAGE=quay.io/metal3-io/vbmc
+++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+++ export IRONIC_TLS_SETUP=true
+++ IRONIC_TLS_SETUP=true
+++ export IRONIC_BASIC_AUTH=true
+++ IRONIC_BASIC_AUTH=true
+++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
+++ IRONIC_IMAGE=quay.io/metal3-io/ironic
+++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
+++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
+++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
+++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
+++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
+++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
+++ '[' v1beta1 == v1alpha4 ']'
+++ export IRONIC_NAMESPACE=baremetal-operator-system
+++ IRONIC_NAMESPACE=baremetal-operator-system
+++ export NAMEPREFIX=baremetal-operator
+++ NAMEPREFIX=baremetal-operator
+++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
+++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
+++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
+++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
+++ export OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
+++ OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
+++ '[' v1beta1 == v1alpha4 ']'
+++ '[' v1beta1 == v1alpha5 ']'
+++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
+++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:main
+++ export DEFAULT_HOSTS_MEMORY=4096
+++ DEFAULT_HOSTS_MEMORY=4096
+++ export CLUSTER_NAME=test1
+++ CLUSTER_NAME=test1
+++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
+++ CLUSTER_APIENDPOINT_IP=192.168.111.249
+++ export KUBERNETES_VERSION=v1.21.0
+++ KUBERNETES_VERSION=v1.21.0
+++ export KUBERNETES_BINARIES_VERSION=v1.21.0
+++ KUBERNETES_BINARIES_VERSION=v1.21.0
+++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
+++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
+++ '[' docker == docker ']'
+++ export EPHEMERAL_CLUSTER=minikube
+++ EPHEMERAL_CLUSTER=minikube
+++ export KUSTOMIZE_VERSION=v4.1.3
+++ KUSTOMIZE_VERSION=v4.1.3
+++ export KIND_VERSION=v0.11.1
+++ KIND_VERSION=v0.11.1
+++ '[' v1.21.0 == v1.21.2 ']'
+++ export KIND_NODE_IMAGE_VERSION=v1.22.2
+++ KIND_NODE_IMAGE_VERSION=v1.22.2
+++ export MINIKUBE_VERSION=v1.23.2
+++ MINIKUBE_VERSION=v1.23.2
+++ export ANSIBLE_VERSION=4.8.0
+++ ANSIBLE_VERSION=4.8.0
+++ SKIP_RETRIES=false
+++ TEST_TIME_INTERVAL=10
+++ TEST_MAX_TIME=240
+++ FAILS=0
+++ RESULT_STR=
+++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
+++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
+++ '[' 7 -lt 6 ']'
+++ export LIBVIRT_DEFAULT_URI=qemu:///system
+++ LIBVIRT_DEFAULT_URI=qemu:///system
+++ '[' capm3 '!=' root ']'
+++ '[' /run/user/1000 == /run/user/0 ']'
+++ sudo -n uptime
+++ export USE_FIREWALLD=False
+++ USE_FIREWALLD=False
+++ [[ ubuntu20 == \r\h\e\l\8 ]]
+++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
++++ df / --output=fstype
++++ tail -n 1
+++ FSTYPE=ext4
+++ case ${FSTYPE} in
+++ '[' '!' -d /opt/metal3-dev-env ']'
++ source lib/network.sh
+++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
+++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
+++ export POD_CIDR=192.168.0.0/18
+++ POD_CIDR=192.168.0.0/18
+++ PROVISIONING_IPV6=false
+++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
+++ [[ false == \t\r\u\e ]]
+++ export BOOT_MODE=legacy
+++ BOOT_MODE=legacy
+++ export PROVISIONING_NETWORK=172.22.0.0/24
+++ PROVISIONING_NETWORK=172.22.0.0/24
+++ [[ legacy == \l\e\g\a\c\y ]]
+++ export LIBVIRT_FIRMWARE=bios
+++ LIBVIRT_FIRMWARE=bios
+++ export LIBVIRT_SECURE_BOOT=false
+++ LIBVIRT_SECURE_BOOT=false
+++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
+++ resultvar=PROVISIONING_CIDR
+++ network=172.22.0.0/24
++++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
+++ result=24
+++ eval PROVISIONING_CIDR=24
++++ PROVISIONING_CIDR=24
+++ export PROVISIONING_CIDR
+++ export PROVISIONING_CIDR
+++ export PROVISIONING_NETMASK=255.255.255.0
+++ PROVISIONING_NETMASK=255.255.255.0
+++ network_address PROVISIONING_IP 172.22.0.0/24 1
+++ resultvar=PROVISIONING_IP
+++ network=172.22.0.0/24
+++ record=1
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
+++ result=172.22.0.1
+++ eval PROVISIONING_IP=172.22.0.1
++++ PROVISIONING_IP=172.22.0.1
+++ export PROVISIONING_IP
+++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
+++ resultvar=CLUSTER_PROVISIONING_IP
+++ network=172.22.0.0/24
+++ record=2
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
+++ result=172.22.0.2
+++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
++++ CLUSTER_PROVISIONING_IP=172.22.0.2
+++ export CLUSTER_PROVISIONING_IP
+++ export PROVISIONING_IP
+++ export CLUSTER_PROVISIONING_IP
+++ [[ 172.22.0.1 == *\:* ]]
+++ export PROVISIONING_URL_HOST=172.22.0.1
+++ PROVISIONING_URL_HOST=172.22.0.1
+++ export CLUSTER_URL_HOST=172.22.0.2
+++ CLUSTER_URL_HOST=172.22.0.2
+++ [[ 192.168.111.249 == *\:* ]]
+++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
+++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
+++ export CLUSTER_APIENDPOINT_PORT=6443
+++ CLUSTER_APIENDPOINT_PORT=6443
+++ network_address dhcp_range_start 172.22.0.0/24 10
+++ resultvar=dhcp_range_start
+++ network=172.22.0.0/24
+++ record=10
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
+++ result=172.22.0.10
+++ eval dhcp_range_start=172.22.0.10
++++ dhcp_range_start=172.22.0.10
+++ export dhcp_range_start
+++ network_address dhcp_range_end 172.22.0.0/24 100
+++ resultvar=dhcp_range_end
+++ network=172.22.0.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
+++ result=172.22.0.100
+++ eval dhcp_range_end=172.22.0.100
++++ dhcp_range_end=172.22.0.100
+++ export dhcp_range_end
+++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
+++ resultvar=PROVISIONING_POOL_RANGE_START
+++ network=172.22.0.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
+++ result=172.22.0.100
+++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
++++ PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ export PROVISIONING_POOL_RANGE_START
+++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
+++ resultvar=PROVISIONING_POOL_RANGE_END
+++ network=172.22.0.0/24
+++ record=200
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
+++ result=172.22.0.200
+++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
++++ PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ export PROVISIONING_POOL_RANGE_END
+++ export PROVISIONING_POOL_RANGE_START
+++ export PROVISIONING_POOL_RANGE_END
+++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+++ EXTERNAL_SUBNET=
+++ [[ -n '' ]]
+++ export IP_STACK=v4
+++ IP_STACK=v4
+++ [[ v4 == \v\4 ]]
+++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
+++ EXTERNAL_SUBNET_V4=192.168.111.0/24
+++ export EXTERNAL_SUBNET_V6=
+++ EXTERNAL_SUBNET_V6=
+++ [[ minikube == \m\i\n\i\k\u\b\e ]]
+++ [[ -n '' ]]
+++ [[ -n 192.168.111.0/24 ]]
+++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
+++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
+++ network=192.168.111.0/24
++++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
+++ result=24
+++ eval EXTERNAL_SUBNET_V4_PREFIX=24
++++ EXTERNAL_SUBNET_V4_PREFIX=24
+++ export EXTERNAL_SUBNET_V4_PREFIX
+++ export EXTERNAL_SUBNET_V4_PREFIX
+++ [[ -z 192.168.111.1 ]]
+++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
+++ resultvar=VIRSH_DHCP_V4_START
+++ network=192.168.111.0/24
+++ record=20
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
+++ result=192.168.111.20
+++ eval VIRSH_DHCP_V4_START=192.168.111.20
++++ VIRSH_DHCP_V4_START=192.168.111.20
+++ export VIRSH_DHCP_V4_START
+++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
+++ resultvar=VIRSH_DHCP_V4_END
+++ network=192.168.111.0/24
+++ record=60
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
+++ result=192.168.111.60
+++ eval VIRSH_DHCP_V4_END=192.168.111.60
++++ VIRSH_DHCP_V4_END=192.168.111.60
+++ export VIRSH_DHCP_V4_END
+++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
+++ resultvar=BAREMETALV4_POOL_RANGE_START
+++ network=192.168.111.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
+++ result=192.168.111.100
+++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
++++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ export BAREMETALV4_POOL_RANGE_START
+++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
+++ resultvar=BAREMETALV4_POOL_RANGE_END
+++ network=192.168.111.0/24
+++ record=200
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
+++ result=192.168.111.200
+++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
++++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ export BAREMETALV4_POOL_RANGE_END
+++ export VIRSH_DHCP_V4_START
+++ export VIRSH_DHCP_V4_END
+++ export BAREMETALV4_POOL_RANGE_START
+++ export BAREMETALV4_POOL_RANGE_END
+++ [[ -n '' ]]
+++ export EXTERNAL_SUBNET_V6_HOST=
+++ EXTERNAL_SUBNET_V6_HOST=
+++ export EXTERNAL_SUBNET_V6_PREFIX=
+++ EXTERNAL_SUBNET_V6_PREFIX=
+++ export BAREMETALV6_POOL_RANGE_START=
+++ BAREMETALV6_POOL_RANGE_START=
+++ export BAREMETALV6_POOL_RANGE_END=
+++ BAREMETALV6_POOL_RANGE_END=
+++ export REGISTRY_PORT=5000
+++ REGISTRY_PORT=5000
+++ export HTTP_PORT=6180
+++ HTTP_PORT=6180
+++ export IRONIC_INSPECTOR_PORT=5050
+++ IRONIC_INSPECTOR_PORT=5050
+++ export IRONIC_API_PORT=6385
+++ IRONIC_API_PORT=6385
+++ [[ -n 192.168.111.1 ]]
+++ export REGISTRY=192.168.111.1:5000
+++ REGISTRY=192.168.111.1:5000
+++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
+++ resultvar=INITIAL_IRONICBRIDGE_IP
+++ network=172.22.0.0/24
+++ record=9
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
+++ result=172.22.0.9
+++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
++++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ export INITIAL_IRONICBRIDGE_IP
+++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+++ '[' true == true ']'
+++ export IRONIC_URL=https://172.22.0.2:6385/v1/
+++ IRONIC_URL=https://172.22.0.2:6385/v1/
+++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ '[' y == y ']'
++ sudo ip link add ironicendpoint type veth peer name ironic-peer
++ sudo brctl addbr provisioning
++ sudo ip link set provisioning up
++ [[ false == \t\r\u\e ]]
++ sudo ip addr add dev ironicendpoint 172.22.0.1/24
++ sudo brctl addif provisioning ironic-peer
++ sudo ip link set ironicendpoint up
++ sudo ip link set ironic-peer up
++ '[' '' ']'
++ '[' y == y ']'
+++ ip a show baremetal
++ [[ -n 137: baremetal: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:c1:7d:04 brd ff:ff:ff:ff:ff:ff
    inet 192.168.111.1/24 brd 192.168.111.255 scope global baremetal
       valid_lft forever preferred_lft forever ]]
++ '[' '' ']'
++ '[' y == y ']'
++ sudo virsh net-destroy baremetal
Network baremetal destroyed

++ sudo virsh net-start baremetal
Network baremetal started

++ '[' '' ']'
+ source disable_apparmor_driver_libvirtd.sh
++ selinux='#security_driver = "selinux"'
++ apparmor='security_driver = "apparmor"'
++ none='security_driver = "none"'
++ sudo sed -i 's/#security_driver = "selinux"/security_driver = "none"/g' /etc/libvirt/qemu.conf
++ sudo sed -i 's/security_driver = "apparmor"/security_driver = "none"/g' /etc/libvirt/qemu.conf
++ sudo systemctl restart libvirtd
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e '{use_firewalld: False}' -e 'external_subnet_v4: 192.168.111.0/24' -i vm-setup/inventory.ini -b vm-setup/firewall.yml
[0;35m[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names [0m
[0;35mto new standard, use callbacks_enabled instead. This feature will be removed [0m
[0;35mfrom ansible-core in version 2.15. Deprecation warnings can be disabled by [0m
[0;35msetting deprecation_warnings=False in ansible.cfg.[0m

PLAY [Setup dummy baremetal VMs] ***********************************************
Tuesday 09 November 2021  21:48:05 +0000 (0:00:00.033)       0:00:00.033 ****** 

TASK [Gathering Facts] *********************************************************
[0;32mok: [localhost][0m
Tuesday 09 November 2021  21:48:06 +0000 (0:00:01.384)       0:00:01.417 ****** 
Tuesday 09 November 2021  21:48:06 +0000 (0:00:00.040)       0:00:01.457 ****** 

TASK [firewall : iptables] *****************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/firewall/tasks/iptables.yaml for localhost[0m
Tuesday 09 November 2021  21:48:06 +0000 (0:00:00.056)       0:00:01.514 ****** 

TASK [firewall : iptables: Firewalld service stopped] **************************
[1;30mtask path: /home/capm3/projects/metal3-dev-env/vm-setup/roles/firewall/tasks/iptables.yaml:1[0m
[0;31mfatal: [localhost]: FAILED! => {"changed": false, "msg": "Could not find the requested service firewalld: host"}[0m
[0;36m...ignoring[0m
Tuesday 09 November 2021  21:48:07 +0000 (0:00:00.592)       0:00:02.106 ****** 

TASK [firewall : iptables: VBMC Ports] *****************************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:48:07 +0000 (0:00:00.365)       0:00:02.472 ****** 

TASK [firewall : iptables: sushy Port] *****************************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:48:07 +0000 (0:00:00.245)       0:00:02.718 ****** 

TASK [firewall : iptables: Established and related] ****************************
[0;33mchanged: [localhost][0m
Tuesday 09 November 2021  21:48:08 +0000 (0:00:00.249)       0:00:02.967 ****** 

TASK [firewall : iptables: Ironic Ports] ***************************************
[0;33mchanged: [localhost] => (item=6180)[0m
[0;33mchanged: [localhost] => (item=5050)[0m
[0;33mchanged: [localhost] => (item=6385)[0m
[0;33mchanged: [localhost] => (item=9999)[0m
[0;33mchanged: [localhost] => (item=80)[0m
Tuesday 09 November 2021  21:48:09 +0000 (0:00:01.203)       0:00:04.171 ****** 

TASK [firewall : iptables: Provisioning host ports] ****************************
[0;33mchanged: [localhost] => (item=80)[0m
[0;33mchanged: [localhost] => (item=5000)[0m
[0;33mchanged: [localhost] => (item=53)[0m
Tuesday 09 November 2021  21:48:10 +0000 (0:00:00.703)       0:00:04.874 ****** 

TASK [firewall : iptables: PXE Ports] ******************************************
[0;33mchanged: [localhost] => (item=5353)[0m
[0;33mchanged: [localhost] => (item=67)[0m
[0;33mchanged: [localhost] => (item=68)[0m
[0;33mchanged: [localhost] => (item=546)[0m
[0;33mchanged: [localhost] => (item=547)[0m
[0;33mchanged: [localhost] => (item=69)[0m
Tuesday 09 November 2021  21:48:11 +0000 (0:00:01.348)       0:00:06.222 ****** 

TASK [firewall : iptables: Ironic Endpoint Keepalived] *************************
[0;33mchanged: [localhost] => (item=112)[0m
[0;33mchanged: [localhost] => (item=icmp)[0m
Tuesday 09 November 2021  21:48:11 +0000 (0:00:00.475)       0:00:06.698 ****** 

PLAY RECAP *********************************************************************
[0;33mlocalhost[0m                  : [0;32mok=10  [0m [0;33mchanged=7   [0m unreachable=0    failed=0    [0;36mskipped=2   [0m rescued=0    [1;35mignored=1   [0m

Tuesday 09 November 2021  21:48:11 +0000 (0:00:00.081)       0:00:06.780 ****** 
=============================================================================== 
Gathering Facts --------------------------------------------------------- 1.38s
firewall : iptables: PXE Ports ------------------------------------------ 1.35s
firewall : iptables: Ironic Ports --------------------------------------- 1.20s
+ '[' False == True ']'
+ '[' '' ']'
++ sudo docker inspect registry --format '{{.State.Status}}'
+ reg_state=exited
+ [[ exited == \e\x\i\t\e\d ]]
+ sudo docker start registry
registry
+ sleep 5
++ env
++ grep -v _LOCAL_IMAGE=
++ grep _IMAGE=
++ grep -o '^[^=]*'
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=registry:2.7.1
+ IMAGE_NAME=registry:2.7.1
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
+ sudo docker tag registry:2.7.1 192.168.111.1:5000/localimages/registry:2.7.1
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/registry:2.7.1
The push refers to repository [192.168.111.1:5000/localimages/registry]
6da1e15d5d7f: Preparing
d385a2515a0f: Preparing
d661c8a70d1e: Preparing
02ada6f7a843: Preparing
39982b2a789a: Preparing
02ada6f7a843: Layer already exists
d661c8a70d1e: Layer already exists
39982b2a789a: Layer already exists
d385a2515a0f: Layer already exists
6da1e15d5d7f: Layer already exists
2.7.1: digest: sha256:b0b8dd398630cbb819d9a9c2fbd50561370856874b5d5d935be2e0af07c0ff4c size: 1363
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/baremetal-operator
+ IMAGE_NAME=baremetal-operator
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
+ sudo docker tag quay.io/metal3-io/baremetal-operator 192.168.111.1:5000/localimages/baremetal-operator
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/baremetal-operator
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/baremetal-operator]
996e919e8270: Preparing
0b3d0512394d: Preparing
6d75f23be3dd: Preparing
996e919e8270: Layer already exists
6d75f23be3dd: Layer already exists
0b3d0512394d: Layer already exists
latest: digest: sha256:d1323f075c3f519da71d744f1e0f6c9d5c4b55d971361639442fb0d145fce6ae size: 950
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-client
+ IMAGE_NAME=ironic-client
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-client
+ sudo docker tag quay.io/metal3-io/ironic-client 192.168.111.1:5000/localimages/ironic-client
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic-client
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic-client]
297a1a71e453: Preparing
1591f6d87eef: Preparing
2653d992f4ef: Preparing
1591f6d87eef: Layer already exists
297a1a71e453: Layer already exists
2653d992f4ef: Layer already exists
latest: digest: sha256:0abe9a3de15449f9cb7c8ec3daa5dceaf124c2e1705c51ef73dfc54f92dacec4 size: 948
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic
+ IMAGE_NAME=ironic
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic
+ sudo docker tag quay.io/metal3-io/ironic 192.168.111.1:5000/localimages/ironic
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic]
9f9b0a238484: Preparing
294f636c5f28: Preparing
868b6506fd62: Preparing
8ce05429fcf5: Preparing
4e849da23513: Preparing
9eeef8c842f0: Preparing
9f52c6f111be: Preparing
a6e3f33d3978: Preparing
953d7a124685: Preparing
550ce4d22da2: Preparing
b3a2e4c023a3: Preparing
0b15ddafdd3b: Preparing
b63f83aa0e18: Preparing
8d4d55d47f62: Preparing
ce2c12cad2fc: Preparing
58334e793c2c: Preparing
69980d38ee13: Preparing
784f11623567: Preparing
95c51ae9ba96: Preparing
e7a4bda8f16d: Preparing
525ed45dbdb1: Preparing
5bc03dec6239: Preparing
9eeef8c842f0: Waiting
9f52c6f111be: Waiting
a6e3f33d3978: Waiting
953d7a124685: Waiting
550ce4d22da2: Waiting
b3a2e4c023a3: Waiting
0b15ddafdd3b: Waiting
b63f83aa0e18: Waiting
8d4d55d47f62: Waiting
ce2c12cad2fc: Waiting
58334e793c2c: Waiting
69980d38ee13: Waiting
784f11623567: Waiting
95c51ae9ba96: Waiting
e7a4bda8f16d: Waiting
525ed45dbdb1: Waiting
5bc03dec6239: Waiting
4e849da23513: Layer already exists
8ce05429fcf5: Layer already exists
9f9b0a238484: Layer already exists
868b6506fd62: Layer already exists
294f636c5f28: Layer already exists
9eeef8c842f0: Layer already exists
550ce4d22da2: Layer already exists
a6e3f33d3978: Layer already exists
9f52c6f111be: Layer already exists
953d7a124685: Layer already exists
b3a2e4c023a3: Layer already exists
0b15ddafdd3b: Layer already exists
8d4d55d47f62: Layer already exists
b63f83aa0e18: Layer already exists
ce2c12cad2fc: Layer already exists
58334e793c2c: Layer already exists
69980d38ee13: Layer already exists
784f11623567: Layer already exists
95c51ae9ba96: Layer already exists
525ed45dbdb1: Layer already exists
5bc03dec6239: Layer already exists
e7a4bda8f16d: Layer already exists
latest: digest: sha256:b0f388457682993d6ef203dc5f77ef3f6effc5eef2afd4aff3b817970881f911 size: 4910
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ IMAGE_NAME=ironic-ipa-downloader
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ sudo docker tag quay.io/metal3-io/ironic-ipa-downloader 192.168.111.1:5000/localimages/ironic-ipa-downloader
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic-ipa-downloader
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic-ipa-downloader]
ec89db9873bd: Preparing
17be93712683: Preparing
2653d992f4ef: Preparing
2653d992f4ef: Layer already exists
ec89db9873bd: Layer already exists
17be93712683: Layer already exists
latest: digest: sha256:d2d871675b629bf66514ccda2e2616c50670f7fff9d95b983a216f3a7fdaa1aa size: 948
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/sushy-tools
+ IMAGE_NAME=sushy-tools
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/sushy-tools
+ sudo docker tag quay.io/metal3-io/sushy-tools 192.168.111.1:5000/localimages/sushy-tools
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/sushy-tools
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/sushy-tools]
877458e4c06b: Preparing
f77bec5d5162: Preparing
7b656b8058c4: Preparing
02a38a00d553: Preparing
7fcd2600f5ad: Preparing
8f56c3340629: Preparing
ba6e5ff31f23: Preparing
9f9f651e9303: Preparing
0b3c02b5d746: Preparing
62a747bf1719: Preparing
8f56c3340629: Waiting
ba6e5ff31f23: Waiting
9f9f651e9303: Waiting
0b3c02b5d746: Waiting
7b656b8058c4: Layer already exists
7fcd2600f5ad: Layer already exists
02a38a00d553: Layer already exists
f77bec5d5162: Layer already exists
877458e4c06b: Layer already exists
ba6e5ff31f23: Layer already exists
62a747bf1719: Layer already exists
0b3c02b5d746: Layer already exists
8f56c3340629: Layer already exists
9f9f651e9303: Layer already exists
latest: digest: sha256:03a9f79dcab145cb5f550a65068a38c303e0decf226400775f30bcd48a734315 size: 2430
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/keepalived
+ IMAGE_NAME=keepalived
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/keepalived
+ sudo docker tag quay.io/metal3-io/keepalived 192.168.111.1:5000/localimages/keepalived
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/keepalived
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/keepalived]
366f541fef14: Preparing
c94b2040d772: Preparing
309a780d3dd8: Preparing
9f54eef41275: Preparing
c94b2040d772: Layer already exists
366f541fef14: Layer already exists
9f54eef41275: Layer already exists
309a780d3dd8: Layer already exists
latest: digest: sha256:4d2d44db445e898a08b072a29af18c325f92a06508b720ea9c95ecddc09c942c size: 1155
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/vbmc
+ IMAGE_NAME=vbmc
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/vbmc
+ sudo docker tag quay.io/metal3-io/vbmc 192.168.111.1:5000/localimages/vbmc
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/vbmc
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/vbmc]
091a214fa651: Preparing
95c51ae9ba96: Preparing
e7a4bda8f16d: Preparing
525ed45dbdb1: Preparing
5bc03dec6239: Preparing
95c51ae9ba96: Layer already exists
091a214fa651: Layer already exists
525ed45dbdb1: Layer already exists
e7a4bda8f16d: Layer already exists
5bc03dec6239: Layer already exists
latest: digest: sha256:859a7038a299476b7f0c402869b34c1be58262c0bfbfdb6dda6c062c0af63b36 size: 1373
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ip-address-manager:main
+ IMAGE_NAME=ip-address-manager:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:main
+ sudo docker tag quay.io/metal3-io/ip-address-manager:main 192.168.111.1:5000/localimages/ip-address-manager:main
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ip-address-manager:main
The push refers to repository [192.168.111.1:5000/localimages/ip-address-manager]
09554b5f7a1a: Preparing
6d75f23be3dd: Preparing
09554b5f7a1a: Layer already exists
6d75f23be3dd: Layer already exists
main: digest: sha256:b43e2c4b0954e6f4c878316016123c1d4a9ac8dfba6e185f87ab2fa0f1f0e3f9 size: 739
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:main
+ IMAGE_NAME=cluster-api-provider-metal3:main
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ sudo docker tag quay.io/metal3-io/cluster-api-provider-metal3:main 192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/cluster-api-provider-metal3:main
The push refers to repository [192.168.111.1:5000/localimages/cluster-api-provider-metal3]
260f58b8f5f5: Preparing
6d75f23be3dd: Preparing
260f58b8f5f5: Layer already exists
6d75f23be3dd: Layer already exists
main: digest: sha256:496e0e44e4c3a82755aeeba4ad79b6ffb83724aae51ad17e1d2155f2ed0e9c4d size: 739
++ env
++ grep _LOCAL_IMAGE=
++ grep -o '^[^=]*'
+ IRONIC_IMAGE=quay.io/metal3-io/ironic
+ VBMC_IMAGE=quay.io/metal3-io/vbmc
+ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+ [[ ubuntu == ubuntu ]]
+ sudo docker run -d --net host --privileged --name httpd-infra -v /opt/metal3-dev-env/ironic:/shared --entrypoint /bin/runhttpd --env PROVISIONING_INTERFACE=ironicendpoint quay.io/metal3-io/ironic
7a135a65dce2279a090551891a503432e794d51df21f8103bc7837927b988c6a
+ sudo docker run -d --net host --name vbmc -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/.vbmc -v /root/.ssh:/root/ssh quay.io/metal3-io/vbmc
2ddd1f9c7edb2c50e2972be8a63a253b1aa9b9df3c088312b85d85c765f3d9ea
+ sudo docker run -d --net host --name sushy-tools -v /opt/metal3-dev-env/virtualbmc/sushy-tools:/root/sushy -v /root/.ssh:/root/ssh quay.io/metal3-io/sushy-tools
6be9d5c4cc7eaaebb1e98e1c83bdd187d72ff4a018a5f5abf2404bea6662555d
+ OPENSTACKCLIENT_PATH=/usr/local/bin/openstack
+ command -v openstack
+ grep -v /usr/local/bin/openstack
+ sudo ln -sf /home/capm3/projects/metal3-dev-env/openstackclient.sh /usr/local/bin/openstack
++ dirname /usr/local/bin/openstack
+ sudo ln -sf /home/capm3/projects/metal3-dev-env/openstackclient.sh /usr/local/bin/baremetal
+ VBMC_PATH=/usr/local/bin/vbmc
+ command -v vbmc
+ grep -v /usr/local/bin/vbmc
+ sudo ln -sf /home/capm3/projects/metal3-dev-env/vbmc.sh /usr/local/bin/vbmc
