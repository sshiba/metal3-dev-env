+ source lib/common.sh
++ [[ :/home/capm3/.krew/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
+++ go env
++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/capm3/.cache/go-build"
GOENV="/home/capm3/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/capm3/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/capm3/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build3271174165=/tmp/go-build -gno-record-gcc-switches"'
+++ GO111MODULE=
+++ GOARCH=amd64
+++ GOBIN=
+++ GOCACHE=/home/capm3/.cache/go-build
+++ GOENV=/home/capm3/.config/go/env
+++ GOEXE=
+++ GOFLAGS=
+++ GOHOSTARCH=amd64
+++ GOHOSTOS=linux
+++ GOINSECURE=
+++ GOMODCACHE=/home/capm3/go/pkg/mod
+++ GONOPROXY=
+++ GONOSUMDB=
+++ GOOS=linux
+++ GOPATH=/home/capm3/go
+++ GOPRIVATE=
+++ GOPROXY=https://proxy.golang.org,direct
+++ GOROOT=/usr/local/go
+++ GOSUMDB=sum.golang.org
+++ GOTMPDIR=
+++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
+++ GOVCS=
+++ GOVERSION=go1.16.7
+++ GCCGO=gccgo
+++ AR=ar
+++ CC=gcc
+++ CXX=g++
+++ CGO_ENABLED=1
+++ GOMOD=/dev/null
+++ CGO_CFLAGS='-g -O2'
+++ CGO_CPPFLAGS=
+++ CGO_CXXFLAGS='-g -O2'
+++ CGO_FFLAGS='-g -O2'
+++ CGO_LDFLAGS='-g -O2'
+++ PKG_CONFIG=pkg-config
+++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build3271174165=/tmp/go-build -gno-record-gcc-switches'
++ export GOPATH
++++ dirname lib/common.sh
+++ cd lib/..
+++ pwd
++ SCRIPTDIR=/home/capm3/projects/metal3-dev-env
+++ whoami
++ USER=capm3
++ export USER=capm3
++ USER=capm3
++ '[' -z '' ']'
++ '[' '!' -f /home/capm3/projects/metal3-dev-env/config_capm3.sh ']'
++ CONFIG=/home/capm3/projects/metal3-dev-env/config_capm3.sh
++ source /home/capm3/projects/metal3-dev-env/config_capm3.sh
+++ export KUBECONFIG=/home/capm3/.kube/config
+++ KUBECONFIG=/home/capm3/.kube/config
+++ export K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
+++ K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
+++ export IMAGE_OS=Ubuntu
+++ IMAGE_OS=Ubuntu
+++ export EPHEMERAL_CLUSTER=kind
+++ EPHEMERAL_CLUSTER=kind
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ export NUM_NODES=4
+++ NUM_NODES=4
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=1
+++ NUM_OF_WORKER_REPLICAS=1
+++ export CAPM3_VERSION=v1alpha5
+++ CAPM3_VERSION=v1alpha5
+++ export CAPI_VERSION=v1alpha4
+++ CAPI_VERSION=v1alpha4
+++ export KUBERNETES_VERSION=v1.21.1
+++ KUBERNETES_VERSION=v1.21.1
+++ export UPGRADED_K8S_VERSION=v1.22.2
+++ UPGRADED_K8S_VERSION=v1.22.2
+++ export IMAGE_USERNAME=metal3
+++ IMAGE_USERNAME=metal3
++ export MARIADB_HOST=mariaDB
++ MARIADB_HOST=mariaDB
++ export MARIADB_HOST_IP=127.0.0.1
++ MARIADB_HOST_IP=127.0.0.1
++ ADDN_DNS=
++ EXT_IF=
++ PRO_IF=
++ MANAGE_BR_BRIDGE=y
++ MANAGE_PRO_BRIDGE=y
++ MANAGE_INT_BRIDGE=y
++ INT_IF=
++ ROOT_DISK_NAME=/dev/sda
++ NODE_HOSTNAME_FORMAT=node-%d
++ source /etc/os-release
+++ NAME=Ubuntu
+++ VERSION='20.04.3 LTS (Focal Fossa)'
+++ ID=ubuntu
+++ ID_LIKE=debian
+++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
+++ VERSION_ID=20.04
+++ HOME_URL=https://www.ubuntu.com/
+++ SUPPORT_URL=https://help.ubuntu.com/
+++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
+++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
+++ VERSION_CODENAME=focal
+++ UBUNTU_CODENAME=focal
++ export DISTRO=ubuntu20
++ DISTRO=ubuntu20
++ export OS=ubuntu
++ OS=ubuntu
++ export OS_VERSION_ID=20.04
++ OS_VERSION_ID=20.04
++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
++ export SUPPORTED_DISTROS
++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
++ [[ ubuntu == ubuntu ]]
++ export CONTAINER_RUNTIME=docker
++ CONTAINER_RUNTIME=docker
++ [[ docker == \p\o\d\m\a\n ]]
++ export POD_NAME=
++ POD_NAME=
++ export POD_NAME_INFRA=
++ POD_NAME_INFRA=
++ export SSH_KEY=/home/capm3/.ssh/id_rsa
++ SSH_KEY=/home/capm3/.ssh/id_rsa
++ export SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
++ SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
++ '[' '!' -f /home/capm3/.ssh/id_rsa ']'
++ FILESYSTEM=/
++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
++ export CAPM3_VERSION=v1alpha5
++ CAPM3_VERSION=v1alpha5
++ '[' v1alpha5 == v1alpha4 ']'
++ '[' v1alpha5 == v1alpha5 ']'
++ export CAPI_VERSION=v1alpha4
++ CAPI_VERSION=v1alpha4
++ export M3PATH=/home/capm3/go/src/github.com/metal3-io
++ M3PATH=/home/capm3/go/src/github.com/metal3-io
++ export BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
++ BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
++ export RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
++ export CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
++ export IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
++ IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
++ export IPAM_BASE_URL=metal3-io/ip-address-manager
++ IPAM_BASE_URL=metal3-io/ip-address-manager
++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
++ '[' v1alpha4 == v1alpha3 ']'
++ '[' v1alpha4 == v1alpha4 ']'
++ IPAMBRANCH=release-0.1
++ IPA_DOWNLOAD_ENABLED=true
++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
++ '[' v1alpha5 == v1alpha4 ']'
++ '[' v1alpha5 == v1alpha5 ']'
++ CAPM3BRANCH=release-0.5
++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
++ BMOBRANCH=master
++ FORCE_REPO_UPDATE=true
++ BMOCOMMIT=HEAD
++ BMO_RUN_LOCAL=false
++ CAPM3_RUN_LOCAL=false
++ WORKING_DIR=/opt/metal3-dev-env
++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
++ NODES_PLATFORM=libvirt
++ export NAMESPACE=metal3
++ NAMESPACE=metal3
++ export NUM_NODES=4
++ NUM_NODES=4
++ export NUM_OF_MASTER_REPLICAS=3
++ NUM_OF_MASTER_REPLICAS=3
++ export NUM_OF_WORKER_REPLICAS=1
++ NUM_OF_WORKER_REPLICAS=1
++ export VM_EXTRADISKS=false
++ VM_EXTRADISKS=false
++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
++ VM_EXTRADISKS_FILE_SYSTEM=ext4
++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
++ export NODE_DRAIN_TIMEOUT=0s
++ NODE_DRAIN_TIMEOUT=0s
++ export MAX_SURGE_VALUE=1
++ MAX_SURGE_VALUE=1
++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
++ export CONTAINER_REGISTRY=quay.io
++ CONTAINER_REGISTRY=quay.io
++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
++ VBMC_IMAGE=quay.io/metal3-io/vbmc
++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
++ export IRONIC_TLS_SETUP=true
++ IRONIC_TLS_SETUP=true
++ export IRONIC_BASIC_AUTH=true
++ IRONIC_BASIC_AUTH=true
++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
++ IRONIC_IMAGE=quay.io/metal3-io/ironic
++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
++ '[' v1alpha5 == v1alpha4 ']'
++ export IRONIC_NAMESPACE=baremetal-operator-system
++ IRONIC_NAMESPACE=baremetal-operator-system
++ export NAMEPREFIX=baremetal-operator
++ NAMEPREFIX=baremetal-operator
++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
++ export OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
++ OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
++ '[' v1alpha5 == v1alpha4 ']'
++ '[' v1alpha5 == v1alpha5 ']'
++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:release-0.5
++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:release-0.5
++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:release-0.1
++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:release-0.1
++ export DEFAULT_HOSTS_MEMORY=4096
++ DEFAULT_HOSTS_MEMORY=4096
++ export CLUSTER_NAME=test1
++ CLUSTER_NAME=test1
++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
++ CLUSTER_APIENDPOINT_IP=192.168.111.249
++ export KUBERNETES_VERSION=v1.21.1
++ KUBERNETES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_VERSION=v1.21.1
++ KUBERNETES_BINARIES_VERSION=v1.21.1
++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
++ '[' docker == docker ']'
++ export EPHEMERAL_CLUSTER=kind
++ EPHEMERAL_CLUSTER=kind
++ export KUSTOMIZE_VERSION=v4.1.3
++ KUSTOMIZE_VERSION=v4.1.3
++ export KIND_VERSION=v0.11.1
++ KIND_VERSION=v0.11.1
++ '[' v1.21.1 == v1.21.2 ']'
++ export KIND_NODE_IMAGE_VERSION=v1.22.2
++ KIND_NODE_IMAGE_VERSION=v1.22.2
++ export MINIKUBE_VERSION=v1.23.2
++ MINIKUBE_VERSION=v1.23.2
++ export ANSIBLE_VERSION=4.8.0
++ ANSIBLE_VERSION=4.8.0
++ SKIP_RETRIES=false
++ TEST_TIME_INTERVAL=10
++ TEST_MAX_TIME=240
++ FAILS=0
++ RESULT_STR=
++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
++ '[' 4 -lt 4 ']'
++ export LIBVIRT_DEFAULT_URI=qemu:///system
++ LIBVIRT_DEFAULT_URI=qemu:///system
++ '[' capm3 '!=' root ']'
++ '[' /run/user/1000 == /run/user/0 ']'
++ sudo -n uptime
++ export USE_FIREWALLD=False
++ USE_FIREWALLD=False
++ [[ ubuntu20 == \r\h\e\l\8 ]]
++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
+++ df / --output=fstype
+++ tail -n 1
++ FSTYPE=ext4
++ case ${FSTYPE} in
++ '[' '!' -d /opt/metal3-dev-env ']'
+ source lib/network.sh
++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
++ export POD_CIDR=192.168.0.0/18
++ POD_CIDR=192.168.0.0/18
++ PROVISIONING_IPV6=false
++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
++ [[ false == \t\r\u\e ]]
++ export BOOT_MODE=legacy
++ BOOT_MODE=legacy
++ export PROVISIONING_NETWORK=172.22.0.0/24
++ PROVISIONING_NETWORK=172.22.0.0/24
++ [[ legacy == \l\e\g\a\c\y ]]
++ export LIBVIRT_FIRMWARE=bios
++ LIBVIRT_FIRMWARE=bios
++ export LIBVIRT_SECURE_BOOT=false
++ LIBVIRT_SECURE_BOOT=false
++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
++ resultvar=PROVISIONING_CIDR
++ network=172.22.0.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
++ result=24
++ eval PROVISIONING_CIDR=24
+++ PROVISIONING_CIDR=24
++ export PROVISIONING_CIDR
++ export PROVISIONING_CIDR
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").netmask)'
++ export PROVISIONING_NETMASK=255.255.255.0
++ PROVISIONING_NETMASK=255.255.255.0
++ network_address PROVISIONING_IP 172.22.0.0/24 1
++ resultvar=PROVISIONING_IP
++ network=172.22.0.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
++ result=172.22.0.1
++ eval PROVISIONING_IP=172.22.0.1
+++ PROVISIONING_IP=172.22.0.1
++ export PROVISIONING_IP
++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
++ resultvar=CLUSTER_PROVISIONING_IP
++ network=172.22.0.0/24
++ record=2
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
++ result=172.22.0.2
++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
+++ CLUSTER_PROVISIONING_IP=172.22.0.2
++ export CLUSTER_PROVISIONING_IP
++ export PROVISIONING_IP
++ export CLUSTER_PROVISIONING_IP
++ [[ 172.22.0.1 == *\:* ]]
++ export PROVISIONING_URL_HOST=172.22.0.1
++ PROVISIONING_URL_HOST=172.22.0.1
++ export CLUSTER_URL_HOST=172.22.0.2
++ CLUSTER_URL_HOST=172.22.0.2
++ [[ 192.168.111.249 == *\:* ]]
++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
++ export CLUSTER_APIENDPOINT_PORT=6443
++ CLUSTER_APIENDPOINT_PORT=6443
++ network_address dhcp_range_start 172.22.0.0/24 10
++ resultvar=dhcp_range_start
++ network=172.22.0.0/24
++ record=10
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
++ result=172.22.0.10
++ eval dhcp_range_start=172.22.0.10
+++ dhcp_range_start=172.22.0.10
++ export dhcp_range_start
++ network_address dhcp_range_end 172.22.0.0/24 100
++ resultvar=dhcp_range_end
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval dhcp_range_end=172.22.0.100
+++ dhcp_range_end=172.22.0.100
++ export dhcp_range_end
++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
++ resultvar=PROVISIONING_POOL_RANGE_START
++ network=172.22.0.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
++ result=172.22.0.100
++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ PROVISIONING_POOL_RANGE_START=172.22.0.100
++ export PROVISIONING_POOL_RANGE_START
++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
++ resultvar=PROVISIONING_POOL_RANGE_END
++ network=172.22.0.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
++ result=172.22.0.200
++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ PROVISIONING_POOL_RANGE_END=172.22.0.200
++ export PROVISIONING_POOL_RANGE_END
++ export PROVISIONING_POOL_RANGE_START
++ export PROVISIONING_POOL_RANGE_END
++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
++ EXTERNAL_SUBNET=
++ [[ -n '' ]]
++ export IP_STACK=v4
++ IP_STACK=v4
++ [[ v4 == \v\4 ]]
++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
++ EXTERNAL_SUBNET_V4=192.168.111.0/24
++ export EXTERNAL_SUBNET_V6=
++ EXTERNAL_SUBNET_V6=
++ [[ kind == \m\i\n\i\k\u\b\e ]]
++ [[ -n 192.168.111.0/24 ]]
++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
++ network=192.168.111.0/24
+++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
++ result=24
++ eval EXTERNAL_SUBNET_V4_PREFIX=24
+++ EXTERNAL_SUBNET_V4_PREFIX=24
++ export EXTERNAL_SUBNET_V4_PREFIX
++ export EXTERNAL_SUBNET_V4_PREFIX
++ [[ -z '' ]]
++ network_address EXTERNAL_SUBNET_V4_HOST 192.168.111.0/24 1
++ resultvar=EXTERNAL_SUBNET_V4_HOST
++ network=192.168.111.0/24
++ record=1
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 1 - 1, None)))'
++ result=192.168.111.1
++ eval EXTERNAL_SUBNET_V4_HOST=192.168.111.1
+++ EXTERNAL_SUBNET_V4_HOST=192.168.111.1
++ export EXTERNAL_SUBNET_V4_HOST
++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
++ resultvar=VIRSH_DHCP_V4_START
++ network=192.168.111.0/24
++ record=20
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
++ result=192.168.111.20
++ eval VIRSH_DHCP_V4_START=192.168.111.20
+++ VIRSH_DHCP_V4_START=192.168.111.20
++ export VIRSH_DHCP_V4_START
++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
++ resultvar=VIRSH_DHCP_V4_END
++ network=192.168.111.0/24
++ record=60
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
++ result=192.168.111.60
++ eval VIRSH_DHCP_V4_END=192.168.111.60
+++ VIRSH_DHCP_V4_END=192.168.111.60
++ export VIRSH_DHCP_V4_END
++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
++ resultvar=BAREMETALV4_POOL_RANGE_START
++ network=192.168.111.0/24
++ record=100
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
++ result=192.168.111.100
++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
++ export BAREMETALV4_POOL_RANGE_START
++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
++ resultvar=BAREMETALV4_POOL_RANGE_END
++ network=192.168.111.0/24
++ record=200
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
++ result=192.168.111.200
++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
++ export BAREMETALV4_POOL_RANGE_END
++ export VIRSH_DHCP_V4_START
++ export VIRSH_DHCP_V4_END
++ export BAREMETALV4_POOL_RANGE_START
++ export BAREMETALV4_POOL_RANGE_END
++ [[ -n '' ]]
++ export EXTERNAL_SUBNET_V6_HOST=
++ EXTERNAL_SUBNET_V6_HOST=
++ export EXTERNAL_SUBNET_V6_PREFIX=
++ EXTERNAL_SUBNET_V6_PREFIX=
++ export BAREMETALV6_POOL_RANGE_START=
++ BAREMETALV6_POOL_RANGE_START=
++ export BAREMETALV6_POOL_RANGE_END=
++ BAREMETALV6_POOL_RANGE_END=
++ export REGISTRY_PORT=5000
++ REGISTRY_PORT=5000
++ export HTTP_PORT=6180
++ HTTP_PORT=6180
++ export IRONIC_INSPECTOR_PORT=5050
++ IRONIC_INSPECTOR_PORT=5050
++ export IRONIC_API_PORT=6385
++ IRONIC_API_PORT=6385
++ [[ -n 192.168.111.1 ]]
++ export REGISTRY=192.168.111.1:5000
++ REGISTRY=192.168.111.1:5000
++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
++ resultvar=INITIAL_IRONICBRIDGE_IP
++ network=172.22.0.0/24
++ record=9
+++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
++ result=172.22.0.9
++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
++ export INITIAL_IRONICBRIDGE_IP
++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
++ '[' true == true ']'
++ export IRONIC_URL=https://172.22.0.2:6385/v1/
++ IRONIC_URL=https://172.22.0.2:6385/v1/
++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+ sudo '[' '!' -f /root/.ssh/id_rsa_virt_power ']'
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e working_dir=/opt/metal3-dev-env -e num_nodes=4 -e extradisks=false -e virthost=capm3 -e platform=libvirt -e libvirt_firmware=bios -e libvirt_secure_boot=false -e default_memory=4096 -e manage_baremetal=y -e provisioning_url_host=172.22.0.1 -e nodes_file=/opt/metal3-dev-env/ironic_nodes.json -e node_hostname_format=node-%d -i vm-setup/inventory.ini -b vm-setup/setup-playbook.yml
[0;35m[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names [0m
[0;35mto new standard, use callbacks_enabled instead. This feature will be removed [0m
[0;35mfrom ansible-core in version 2.15. Deprecation warnings can be disabled by [0m
[0;35msetting deprecation_warnings=False in ansible.cfg.[0m

PLAY [Setup dummy baremetal VMs] ***********************************************
Wednesday 10 November 2021  22:42:43 +0000 (0:00:00.015)       0:00:00.015 **** 

TASK [Gathering Facts] *********************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:45 +0000 (0:00:02.403)       0:00:02.418 **** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:45 +0000 (0:00:00.069)       0:00:02.487 **** 

TASK [common : Set an empty default for vm_nodes if not already defined] *******
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:45 +0000 (0:00:00.046)       0:00:02.534 **** 

TASK [common : Populate vm_nodes if not already defined] ***********************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/common/tasks/vm_nodes_tasks.yml for localhost => (item={'key': 'node', 'value': {'memory': '4096', 'disk': '50', 'vcpu': '2', 'extradisks': False}})[0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.096)       0:00:02.630 **** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.068)       0:00:02.698 **** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost] => (item=0)[0m
[0;32mok: [localhost] => (item=1)[0m
[0;32mok: [localhost] => (item=2)[0m
[0;32mok: [localhost] => (item=3)[0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.116)       0:00:02.815 **** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.071)       0:00:02.887 **** 

TASK [common : debug] **********************************************************
[0;32mok: [localhost] => {[0m
[0;32m    "vm_nodes": [[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_0",[0m
[0;32m            "virtualbmc_port": 6230[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_1",[0m
[0;32m            "virtualbmc_port": 6231[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_2",[0m
[0;32m            "virtualbmc_port": 6232[0m
[0;32m        },[0m
[0;32m        {[0m
[0;32m            "flavor": "node",[0m
[0;32m            "name": "node_3",[0m
[0;32m            "virtualbmc_port": 6233[0m
[0;32m        }[0m
[0;32m    ][0m
[0;32m}[0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.077)       0:00:02.965 **** 

TASK [common : set_fact] *******************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.069)       0:00:03.034 **** 
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.041)       0:00:03.076 **** 
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.084)       0:00:03.160 **** 
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.050)       0:00:03.211 **** 

TASK [libvirt : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/install_setup_tasks.yml for localhost[0m
Wednesday 10 November 2021  22:42:46 +0000 (0:00:00.054)       0:00:03.265 **** 

TASK [libvirt : Start libvirtd] ************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:47 +0000 (0:00:00.718)       0:00:03.983 **** 

TASK [libvirt : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/network_setup_tasks.yml for localhost[0m
Wednesday 10 November 2021  22:42:47 +0000 (0:00:00.071)       0:00:04.055 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Wednesday 10 November 2021  22:42:47 +0000 (0:00:00.119)       0:00:04.175 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Wednesday 10 November 2021  22:42:47 +0000 (0:00:00.182)       0:00:04.357 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : get a list of MACs to use] *************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:48 +0000 (0:00:00.458)       0:00:04.815 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Create libvirt networks] ***************************************
[0;33mchanged: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;33mchanged: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Wednesday 10 November 2021  22:42:49 +0000 (0:00:01.002)       0:00:05.817 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Start libvirt networks] ****************************************
[0;33mchanged: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;33mchanged: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Wednesday 10 November 2021  22:42:50 +0000 (0:00:00.865)       0:00:06.683 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Mark  libvirt networks as autostarted] *************************
[0;33mchanged: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;33mchanged: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Wednesday 10 November 2021  22:42:50 +0000 (0:00:00.667)       0:00:07.351 **** 
Wednesday 10 November 2021  22:42:50 +0000 (0:00:00.046)       0:00:07.397 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Wednesday 10 November 2021  22:42:51 +0000 (0:00:00.148)       0:00:07.546 **** 
Wednesday 10 November 2021  22:42:51 +0000 (0:00:00.127)       0:00:07.673 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Wednesday 10 November 2021  22:42:51 +0000 (0:00:00.151)       0:00:07.825 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Wednesday 10 November 2021  22:42:51 +0000 (0:00:00.154)       0:00:07.980 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m
Wednesday 10 November 2021  22:42:51 +0000 (0:00:00.160)       0:00:08.140 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Whitelist bridges for unprivileged access on Ubuntu or Fedora] ***
[0;32mok: [localhost] => (item={'name': 'provisioning', 'bridge': 'provisioning', 'forward_mode': 'bridge'})[0m
[0;32mok: [localhost] => (item={'name': 'baremetal', 'bridge': 'baremetal', 'forward_mode': 'nat', 'address_v4': '192.168.111.1', 'netmask_v4': '255.255.255.0', 'dhcp_range_v4': ['192.168.111.20', '192.168.111.60'], 'address_v6': '', 'prefix_v6': False, 'dhcp_range_v6': ['', ''], 'lease_expiry': 60, 'nat_port_range': [1024, 65535], 'domain': 'ostest.test.metalkube.org', 'dns': {'hosts': [], 'forwarders': [{'domain': 'apps.ostest.test.metalkube.org', 'addr': '127.0.0.1'}], 'srvs': []}})[0m
Wednesday 10 November 2021  22:42:52 +0000 (0:00:00.688)       0:00:08.829 **** 

TASK [libvirt : Ensure remote working dir exists] ******************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:52 +0000 (0:00:00.429)       0:00:09.258 **** 

TASK [libvirt : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/vm_setup_tasks.yml for localhost[0m
Wednesday 10 November 2021  22:42:52 +0000 (0:00:00.075)       0:00:09.333 **** 

TASK [libvirt : ensure libvirt volume path exists] *****************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:53 +0000 (0:00:00.268)       0:00:09.602 **** 

TASK [libvirt : Check volume pool] *********************************************
[1;30mtask path: /home/capm3/projects/metal3-dev-env/vm-setup/roles/libvirt/tasks/vm_setup_tasks.yml:12[0m
[0;31mfatal: [localhost]: FAILED! => {"changed": false, "cmd": ["virsh", "pool-uuid", "oooq_pool"], "delta": "0:00:00.024578", "end": "2021-11-10 22:42:53.442215", "msg": "non-zero return code", "rc": 1, "start": "2021-11-10 22:42:53.417637", "stderr": "error: failed to get pool 'oooq_pool'\nerror: Storage pool not found: no storage pool with matching name 'oooq_pool'", "stderr_lines": ["error: failed to get pool 'oooq_pool'", "error: Storage pool not found: no storage pool with matching name 'oooq_pool'"], "stdout": "", "stdout_lines": []}[0m
[0;36m...ignoring[0m
Wednesday 10 November 2021  22:42:53 +0000 (0:00:00.432)       0:00:10.034 **** 

TASK [libvirt : create the volume pool xml file] *******************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:42:54 +0000 (0:00:00.714)       0:00:10.748 **** 

TASK [libvirt : Define volume pool] ********************************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:42:54 +0000 (0:00:00.360)       0:00:11.109 **** 

TASK [libvirt : Start volume pool] *********************************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:42:55 +0000 (0:00:00.458)       0:00:11.567 **** 

TASK [libvirt : ensure tripleo-quickstart volume pool is defined] **************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:42:55 +0000 (0:00:00.306)       0:00:11.874 **** 

TASK [libvirt : Mark volume pool for autostart] ********************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:42:55 +0000 (0:00:00.312)       0:00:12.186 **** 

TASK [libvirt : Check if vm volumes exist] *************************************
[0;31mfailed: [localhost] (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_0.qcow2"], "delta": "0:00:00.025940", "end": "2021-11-10 22:42:55.920327", "item": {"flavor": "node", "name": "node_0", "virtualbmc_port": 6230}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-10 22:42:55.894387", "stderr": "error: failed to get vol 'node_0.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_0.qcow2'", "stderr_lines": ["error: failed to get vol 'node_0.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_0.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_1.qcow2"], "delta": "0:00:00.025786", "end": "2021-11-10 22:42:56.162507", "item": {"flavor": "node", "name": "node_1", "virtualbmc_port": 6231}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-10 22:42:56.136721", "stderr": "error: failed to get vol 'node_1.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_1.qcow2'", "stderr_lines": ["error: failed to get vol 'node_1.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_1.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_2.qcow2"], "delta": "0:00:00.026137", "end": "2021-11-10 22:42:56.411501", "item": {"flavor": "node", "name": "node_2", "virtualbmc_port": 6232}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-10 22:42:56.385364", "stderr": "error: failed to get vol 'node_2.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_2.qcow2'", "stderr_lines": ["error: failed to get vol 'node_2.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_2.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;31mfailed: [localhost] (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}) => {"ansible_loop_var": "item", "changed": true, "cmd": ["virsh", "vol-info", "--pool", "oooq_pool", "node_3.qcow2"], "delta": "0:00:01.026589", "end": "2021-11-10 22:42:57.686580", "item": {"flavor": "node", "name": "node_3", "virtualbmc_port": 6233}, "msg": "non-zero return code", "rc": 1, "start": "2021-11-10 22:42:56.659991", "stderr": "error: failed to get vol 'node_3.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_3.qcow2'", "stderr_lines": ["error: failed to get vol 'node_3.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_3.qcow2'"], "stdout": "", "stdout_lines": []}[0m
[0;36m...ignoring[0m
Wednesday 10 November 2021  22:42:57 +0000 (0:00:02.104)       0:00:14.290 **** 

TASK [libvirt : Create vm vm storage] ******************************************
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_0.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_0.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_0.qcow2'], 'start': '2021-11-10 22:42:55.894387', 'end': '2021-11-10 22:42:55.920327', 'delta': '0:00:00.025940', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_0.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_0.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_0.qcow2'"], 'item': {'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_1.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_1.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_1.qcow2'], 'start': '2021-11-10 22:42:56.136721', 'end': '2021-11-10 22:42:56.162507', 'delta': '0:00:00.025786', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_1.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_1.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_1.qcow2'"], 'item': {'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_2.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_2.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_2.qcow2'], 'start': '2021-11-10 22:42:56.385364', 'end': '2021-11-10 22:42:56.411501', 'delta': '0:00:00.026137', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_2.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_2.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_2.qcow2'"], 'item': {'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}, 'ansible_loop_var': 'item'})[0m
[0;33mchanged: [localhost] => (item={'changed': True, 'stdout': '', 'stderr': "error: failed to get vol 'node_3.qcow2'\nerror: Storage volume not found: no storage vol with matching path 'node_3.qcow2'", 'rc': 1, 'cmd': ['virsh', 'vol-info', '--pool', 'oooq_pool', 'node_3.qcow2'], 'start': '2021-11-10 22:42:56.659991', 'end': '2021-11-10 22:42:57.686580', 'delta': '0:00:01.026589', 'failed': True, 'msg': 'non-zero return code', 'invocation': {'module_args': {'_raw_params': "virsh vol-info --pool 'oooq_pool' 'node_3.qcow2'\n", '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': [], 'stderr_lines': ["error: failed to get vol 'node_3.qcow2'", "error: Storage volume not found: no storage vol with matching path 'node_3.qcow2'"], 'item': {'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}, 'ansible_loop_var': 'item'})[0m
Wednesday 10 November 2021  22:42:59 +0000 (0:00:01.308)       0:00:15.599 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Define vm vms] *************************************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
[1;35m[WARNING]: 'xml' is given - ignoring 'name'[0m
Wednesday 10 November 2021  22:43:01 +0000 (0:00:02.667)       0:00:18.267 **** 
Wednesday 10 November 2021  22:43:01 +0000 (0:00:00.131)       0:00:18.398 **** 
Wednesday 10 November 2021  22:43:02 +0000 (0:00:00.190)       0:00:18.588 **** 
Wednesday 10 November 2021  22:43:02 +0000 (0:00:00.128)       0:00:18.717 **** 

TASK [libvirt : Get vm uuid] ***************************************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
Wednesday 10 November 2021  22:43:04 +0000 (0:00:02.136)       0:00:20.853 **** 

TASK [libvirt : set_fact] ******************************************************
[0;32mok: [localhost] => (item={'changed': True, 'stdout': '19351546-412e-405f-8b2c-79c5dbd88001', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_0'], 'start': '2021-11-10 22:43:02.451879', 'end': '2021-11-10 22:43:02.476277', 'delta': '0:00:00.024398', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_0"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['19351546-412e-405f-8b2c-79c5dbd88001'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': 'f55e6a75-ccc8-457d-a5a0-f5c246301189', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_1'], 'start': '2021-11-10 22:43:02.705115', 'end': '2021-11-10 22:43:03.729301', 'delta': '0:00:01.024186', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_1"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['f55e6a75-ccc8-457d-a5a0-f5c246301189'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': 'cb0103d8-da02-4e02-a5c9-62c1923d04ad', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_2'], 'start': '2021-11-10 22:43:03.978638', 'end': '2021-11-10 22:43:04.002990', 'delta': '0:00:00.024352', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_2"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['cb0103d8-da02-4e02-a5c9-62c1923d04ad'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232}, 'ansible_loop_var': 'item'})[0m
[0;32mok: [localhost] => (item={'changed': True, 'stdout': '1dcff408-8be2-4aa7-9bdf-f5317064b75c', 'stderr': '', 'rc': 0, 'cmd': ['virsh', 'domuuid', 'node_3'], 'start': '2021-11-10 22:43:04.224681', 'end': '2021-11-10 22:43:04.249287', 'delta': '0:00:00.024606', 'msg': '', 'invocation': {'module_args': {'_raw_params': 'virsh domuuid "node_3"\n', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}, 'stdout_lines': ['1dcff408-8be2-4aa7-9bdf-f5317064b75c'], 'stderr_lines': [], 'failed': False, 'item': {'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233}, 'ansible_loop_var': 'item'})[0m
Wednesday 10 November 2021  22:43:04 +0000 (0:00:00.144)       0:00:20.998 **** 

TASK [libvirt : set_fact BMC Driver] *******************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:04 +0000 (0:00:00.089)       0:00:21.087 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [libvirt : Write ironic node json files] **********************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:05 +0000 (0:00:00.758)       0:00:21.845 **** 
Wednesday 10 November 2021  22:43:05 +0000 (0:00:00.044)       0:00:21.890 **** 
Wednesday 10 November 2021  22:43:05 +0000 (0:00:00.049)       0:00:21.939 **** 

TASK [virtbmc : include_tasks] *************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/virtbmc/tasks/setup_tasks.yml for localhost[0m
Wednesday 10 November 2021  22:43:05 +0000 (0:00:00.088)       0:00:22.028 **** 

TASK [virtbmc : Create VirtualBMC directories] *********************************
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc/conf)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/vbmc/log)[0m
[0;33mchanged: [localhost] => (item=/opt/metal3-dev-env/virtualbmc/sushy-tools)[0m
Wednesday 10 November 2021  22:43:06 +0000 (0:00:01.204)       0:00:23.233 **** 

TASK [virtbmc : Create VirtualBMC configuration file] **************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:07 +0000 (0:00:00.505)       0:00:23.738 **** 

TASK [virtbmc : get virthost non_root_user userid] *****************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:07 +0000 (0:00:00.314)       0:00:24.052 **** 

TASK [virtbmc : set fact on non_root_user_uid] *********************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:07 +0000 (0:00:00.076)       0:00:24.129 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [virtbmc : set vbmc address (v4) if there is a (nat) network defined with an address] ***
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:07 +0000 (0:00:00.141)       0:00:24.270 **** 
[1;35m[WARNING]: The value '' is not a valid IP address or network, passing this[0m
[1;35mvalue to ipaddr filter might result in breaking change in future.[0m

TASK [virtbmc : set vbmc address (v6) if there is a (nat) network defined with an address] ***
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:07 +0000 (0:00:00.140)       0:00:24.411 **** 

TASK [virtbmc : set vbmc address from IPv4 networks if possible, otherwise IPv6] ***
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:07 +0000 (0:00:00.096)       0:00:24.507 **** 

TASK [virtbmc : set qemu uri for qemu:///system usage] *************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:08 +0000 (0:00:00.101)       0:00:24.609 **** 
Wednesday 10 November 2021  22:43:08 +0000 (0:00:00.069)       0:00:24.678 **** 

TASK [virtbmc : Create VirtualBMC directories] *********************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
Wednesday 10 November 2021  22:43:09 +0000 (0:00:01.010)       0:00:25.689 **** 

TASK [virtbmc : Create the Virtual BMCs] ***************************************
[0;33mchanged: [localhost] => (item={'name': 'node_0', 'flavor': 'node', 'virtualbmc_port': 6230})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_1', 'flavor': 'node', 'virtualbmc_port': 6231})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_2', 'flavor': 'node', 'virtualbmc_port': 6232})[0m
[0;33mchanged: [localhost] => (item={'name': 'node_3', 'flavor': 'node', 'virtualbmc_port': 6233})[0m
Wednesday 10 November 2021  22:43:10 +0000 (0:00:01.682)       0:00:27.371 **** 

TASK [virtbmc : Create a password file for Redfish Virtual BMCs] ***************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:11 +0000 (0:00:00.772)       0:00:28.144 **** 

TASK [virtbmc : Create the Redfish Virtual BMCs] *******************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:12 +0000 (0:00:00.566)       0:00:28.710 **** 

PLAY RECAP *********************************************************************
[0;33mlocalhost[0m                  : [0;32mok=46  [0m [0;33mchanged=19  [0m unreachable=0    failed=0    [0;36mskipped=18  [0m rescued=0    [1;35mignored=2   [0m

Wednesday 10 November 2021  22:43:12 +0000 (0:00:00.085)       0:00:28.795 **** 
=============================================================================== 
libvirt : Define vm vms ------------------------------------------------- 2.67s
Gathering Facts --------------------------------------------------------- 2.40s
libvirt : Get vm uuid --------------------------------------------------- 2.14s
+ sudo virsh pool-uuid default
+ [[ ubuntu == ubuntu ]]
+ source ubuntu_bridge_network_configuration.sh
++ set -xe
++ source lib/logging.sh
++++ dirname ./02_configure_host.sh
+++ LOGDIR=./logs
+++ '[' '!' -d ./logs ']'
++++ basename ./02_configure_host.sh .sh
++++ date +%F-%H%M%S
+++ LOGFILE=./logs/02_configure_host-2021-11-10-224312.log
+++ echo 'Logging to ./logs/02_configure_host-2021-11-10-224312.log'
Logging to ./logs/02_configure_host-2021-11-10-224312.log
+++ exec
++++ tee ./logs/02_configure_host-2021-11-10-224312.log
++ source lib/common.sh
+++ [[ :/home/capm3/.krew/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/local/go/bin: != *\:\/\u\s\r\/\l\o\c\a\l\/\g\o\/\b\i\n\:* ]]
++++ go env
+++ eval 'GO111MODULE=""
GOARCH="amd64"
GOBIN=""
GOCACHE="/home/capm3/.cache/go-build"
GOENV="/home/capm3/.config/go/env"
GOEXE=""
GOFLAGS=""
GOHOSTARCH="amd64"
GOHOSTOS="linux"
GOINSECURE=""
GOMODCACHE="/home/capm3/go/pkg/mod"
GONOPROXY=""
GONOSUMDB=""
GOOS="linux"
GOPATH="/home/capm3/go"
GOPRIVATE=""
GOPROXY="https://proxy.golang.org,direct"
GOROOT="/usr/local/go"
GOSUMDB="sum.golang.org"
GOTMPDIR=""
GOTOOLDIR="/usr/local/go/pkg/tool/linux_amd64"
GOVCS=""
GOVERSION="go1.16.7"
GCCGO="gccgo"
AR="ar"
CC="gcc"
CXX="g++"
CGO_ENABLED="1"
GOMOD="/dev/null"
CGO_CFLAGS="-g -O2"
CGO_CPPFLAGS=""
CGO_CXXFLAGS="-g -O2"
CGO_FFLAGS="-g -O2"
CGO_LDFLAGS="-g -O2"
PKG_CONFIG="pkg-config"
GOGCCFLAGS="-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build1763983632=/tmp/go-build -gno-record-gcc-switches"'
++++ GO111MODULE=
++++ GOARCH=amd64
++++ GOBIN=
++++ GOCACHE=/home/capm3/.cache/go-build
++++ GOENV=/home/capm3/.config/go/env
++++ GOEXE=
++++ GOFLAGS=
++++ GOHOSTARCH=amd64
++++ GOHOSTOS=linux
++++ GOINSECURE=
++++ GOMODCACHE=/home/capm3/go/pkg/mod
++++ GONOPROXY=
++++ GONOSUMDB=
++++ GOOS=linux
++++ GOPATH=/home/capm3/go
++++ GOPRIVATE=
++++ GOPROXY=https://proxy.golang.org,direct
++++ GOROOT=/usr/local/go
++++ GOSUMDB=sum.golang.org
++++ GOTMPDIR=
++++ GOTOOLDIR=/usr/local/go/pkg/tool/linux_amd64
++++ GOVCS=
++++ GOVERSION=go1.16.7
++++ GCCGO=gccgo
++++ AR=ar
++++ CC=gcc
++++ CXX=g++
++++ CGO_ENABLED=1
++++ GOMOD=/dev/null
++++ CGO_CFLAGS='-g -O2'
++++ CGO_CPPFLAGS=
++++ CGO_CXXFLAGS='-g -O2'
++++ CGO_FFLAGS='-g -O2'
++++ CGO_LDFLAGS='-g -O2'
++++ PKG_CONFIG=pkg-config
++++ GOGCCFLAGS='-fPIC -m64 -pthread -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build1763983632=/tmp/go-build -gno-record-gcc-switches'
+++ export GOPATH
+++++ dirname lib/common.sh
++++ cd lib/..
++++ pwd
+++ SCRIPTDIR=/home/capm3/projects/metal3-dev-env
++++ whoami
+++ USER=capm3
+++ export USER=capm3
+++ USER=capm3
+++ '[' -z /home/capm3/projects/metal3-dev-env/config_capm3.sh ']'
+++ source /home/capm3/projects/metal3-dev-env/config_capm3.sh
++++ export KUBECONFIG=/home/capm3/.kube/config
++++ KUBECONFIG=/home/capm3/.kube/config
++++ export K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
++++ K8S_AUTH_KUBECONFIG=/home/capm3/.kube/config
++++ export IMAGE_OS=Ubuntu
++++ IMAGE_OS=Ubuntu
++++ export EPHEMERAL_CLUSTER=kind
++++ EPHEMERAL_CLUSTER=kind
++++ export CONTAINER_RUNTIME=docker
++++ CONTAINER_RUNTIME=docker
++++ export NUM_NODES=4
++++ NUM_NODES=4
++++ export NUM_OF_MASTER_REPLICAS=3
++++ NUM_OF_MASTER_REPLICAS=3
++++ export NUM_OF_WORKER_REPLICAS=1
++++ NUM_OF_WORKER_REPLICAS=1
++++ export CAPM3_VERSION=v1alpha5
++++ CAPM3_VERSION=v1alpha5
++++ export CAPI_VERSION=v1alpha4
++++ CAPI_VERSION=v1alpha4
++++ export KUBERNETES_VERSION=v1.21.1
++++ KUBERNETES_VERSION=v1.21.1
++++ export UPGRADED_K8S_VERSION=v1.22.2
++++ UPGRADED_K8S_VERSION=v1.22.2
++++ export IMAGE_USERNAME=metal3
++++ IMAGE_USERNAME=metal3
+++ export MARIADB_HOST=mariaDB
+++ MARIADB_HOST=mariaDB
+++ export MARIADB_HOST_IP=127.0.0.1
+++ MARIADB_HOST_IP=127.0.0.1
+++ ADDN_DNS=
+++ EXT_IF=
+++ PRO_IF=
+++ MANAGE_BR_BRIDGE=y
+++ MANAGE_PRO_BRIDGE=y
+++ MANAGE_INT_BRIDGE=y
+++ INT_IF=
+++ ROOT_DISK_NAME=/dev/sda
+++ NODE_HOSTNAME_FORMAT=node-%d
+++ source /etc/os-release
++++ NAME=Ubuntu
++++ VERSION='20.04.3 LTS (Focal Fossa)'
++++ ID=ubuntu
++++ ID_LIKE=debian
++++ PRETTY_NAME='Ubuntu 20.04.3 LTS'
++++ VERSION_ID=20.04
++++ HOME_URL=https://www.ubuntu.com/
++++ SUPPORT_URL=https://help.ubuntu.com/
++++ BUG_REPORT_URL=https://bugs.launchpad.net/ubuntu/
++++ PRIVACY_POLICY_URL=https://www.ubuntu.com/legal/terms-and-policies/privacy-policy
++++ VERSION_CODENAME=focal
++++ UBUNTU_CODENAME=focal
+++ export DISTRO=ubuntu20
+++ DISTRO=ubuntu20
+++ export OS=ubuntu
+++ OS=ubuntu
+++ export OS_VERSION_ID=20.04
+++ OS_VERSION_ID=20.04
+++ SUPPORTED_DISTROS=(centos8 rhel8 ubuntu18 ubuntu20)
+++ export SUPPORTED_DISTROS
+++ [[ ! centos8 rhel8 ubuntu18 ubuntu20 =~ ubuntu20 ]]
+++ [[ ubuntu == ubuntu ]]
+++ export CONTAINER_RUNTIME=docker
+++ CONTAINER_RUNTIME=docker
+++ [[ docker == \p\o\d\m\a\n ]]
+++ export POD_NAME=
+++ POD_NAME=
+++ export POD_NAME_INFRA=
+++ POD_NAME_INFRA=
+++ export SSH_KEY=/home/capm3/.ssh/id_rsa
+++ SSH_KEY=/home/capm3/.ssh/id_rsa
+++ export SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
+++ SSH_PUB_KEY=/home/capm3/.ssh/id_rsa.pub
+++ '[' '!' -f /home/capm3/.ssh/id_rsa ']'
+++ FILESYSTEM=/
+++ CAPM3_VERSION_LIST='v1alpha4 v1alpha5 v1beta1'
+++ export CAPM3_VERSION=v1alpha5
+++ CAPM3_VERSION=v1alpha5
+++ '[' v1alpha5 == v1alpha4 ']'
+++ '[' v1alpha5 == v1alpha5 ']'
+++ export CAPI_VERSION=v1alpha4
+++ CAPI_VERSION=v1alpha4
+++ export M3PATH=/home/capm3/go/src/github.com/metal3-io
+++ M3PATH=/home/capm3/go/src/github.com/metal3-io
+++ export BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
+++ BMOPATH=/home/capm3/go/src/github.com/metal3-io/baremetal-operator
+++ export RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ RUN_LOCAL_IRONIC_SCRIPT=/home/capm3/go/src/github.com/metal3-io/baremetal-operator/tools/run_local_ironic.sh
+++ export CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
+++ CAPM3PATH=/home/capm3/go/src/github.com/metal3-io/cluster-api-provider-metal3
+++ export CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
+++ CAPM3_BASE_URL=metal3-io/cluster-api-provider-metal3
+++ export CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
+++ CAPM3REPO=https://github.com/metal3-io/cluster-api-provider-metal3
+++ export IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
+++ IPAMPATH=/home/capm3/go/src/github.com/metal3-io/ip-address-manager
+++ export IPAM_BASE_URL=metal3-io/ip-address-manager
+++ IPAM_BASE_URL=metal3-io/ip-address-manager
+++ export IPAMREPO=https://github.com/metal3-io/ip-address-manager
+++ IPAMREPO=https://github.com/metal3-io/ip-address-manager
+++ '[' v1alpha4 == v1alpha3 ']'
+++ '[' v1alpha4 == v1alpha4 ']'
+++ IPAMBRANCH=release-0.1
+++ IPA_DOWNLOAD_ENABLED=true
+++ CAPI_BASE_URL=kubernetes-sigs/cluster-api
+++ '[' v1alpha5 == v1alpha4 ']'
+++ '[' v1alpha5 == v1alpha5 ']'
+++ CAPM3BRANCH=release-0.5
+++ BMOREPO=https://github.com/metal3-io/baremetal-operator.git
+++ BMOBRANCH=master
+++ FORCE_REPO_UPDATE=true
+++ BMOCOMMIT=HEAD
+++ BMO_RUN_LOCAL=false
+++ CAPM3_RUN_LOCAL=false
+++ WORKING_DIR=/opt/metal3-dev-env
+++ NODES_FILE=/opt/metal3-dev-env/ironic_nodes.json
+++ NODES_PLATFORM=libvirt
+++ export NAMESPACE=metal3
+++ NAMESPACE=metal3
+++ export NUM_NODES=4
+++ NUM_NODES=4
+++ export NUM_OF_MASTER_REPLICAS=3
+++ NUM_OF_MASTER_REPLICAS=3
+++ export NUM_OF_WORKER_REPLICAS=1
+++ NUM_OF_WORKER_REPLICAS=1
+++ export VM_EXTRADISKS=false
+++ VM_EXTRADISKS=false
+++ export VM_EXTRADISKS_FILE_SYSTEM=ext4
+++ VM_EXTRADISKS_FILE_SYSTEM=ext4
+++ export VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
+++ VM_EXTRADISKS_MOUNT_DIR=/mnt/disk2
+++ export NODE_DRAIN_TIMEOUT=0s
+++ NODE_DRAIN_TIMEOUT=0s
+++ export MAX_SURGE_VALUE=1
+++ MAX_SURGE_VALUE=1
+++ export DOCKER_REGISTRY_IMAGE=registry:2.7.1
+++ DOCKER_REGISTRY_IMAGE=registry:2.7.1
+++ export CONTAINER_REGISTRY=quay.io
+++ CONTAINER_REGISTRY=quay.io
+++ export VBMC_IMAGE=quay.io/metal3-io/vbmc
+++ VBMC_IMAGE=quay.io/metal3-io/vbmc
+++ export SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+++ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+++ export IRONIC_TLS_SETUP=true
+++ IRONIC_TLS_SETUP=true
+++ export IRONIC_BASIC_AUTH=true
+++ IRONIC_BASIC_AUTH=true
+++ export IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+++ IPA_DOWNLOADER_IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+++ export IRONIC_IMAGE=quay.io/metal3-io/ironic
+++ IRONIC_IMAGE=quay.io/metal3-io/ironic
+++ export IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
+++ IRONIC_CLIENT_IMAGE=quay.io/metal3-io/ironic-client
+++ export IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+++ IRONIC_DATA_DIR=/opt/metal3-dev-env/ironic
+++ export IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
+++ IRONIC_IMAGE_DIR=/opt/metal3-dev-env/ironic/html/images
+++ export IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
+++ IRONIC_KEEPALIVED_IMAGE=quay.io/metal3-io/keepalived
+++ '[' v1alpha5 == v1alpha4 ']'
+++ export IRONIC_NAMESPACE=baremetal-operator-system
+++ IRONIC_NAMESPACE=baremetal-operator-system
+++ export NAMEPREFIX=baremetal-operator
+++ NAMEPREFIX=baremetal-operator
+++ export RESTART_CONTAINER_CERTIFICATE_UPDATED=true
+++ RESTART_CONTAINER_CERTIFICATE_UPDATED=true
+++ export BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
+++ BAREMETAL_OPERATOR_IMAGE=quay.io/metal3-io/baremetal-operator
+++ export OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
+++ OPENSTACK_CONFIG=/home/capm3/.config/openstack/clouds.yaml
+++ '[' v1alpha5 == v1alpha4 ']'
+++ '[' v1alpha5 == v1alpha5 ']'
+++ export CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:release-0.5
+++ CAPM3_IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:release-0.5
+++ export IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:release-0.1
+++ IPAM_IMAGE=quay.io/metal3-io/ip-address-manager:release-0.1
+++ export DEFAULT_HOSTS_MEMORY=4096
+++ DEFAULT_HOSTS_MEMORY=4096
+++ export CLUSTER_NAME=test1
+++ CLUSTER_NAME=test1
+++ export CLUSTER_APIENDPOINT_IP=192.168.111.249
+++ CLUSTER_APIENDPOINT_IP=192.168.111.249
+++ export KUBERNETES_VERSION=v1.21.1
+++ KUBERNETES_VERSION=v1.21.1
+++ export KUBERNETES_BINARIES_VERSION=v1.21.1
+++ KUBERNETES_BINARIES_VERSION=v1.21.1
+++ export KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
+++ KUBERNETES_BINARIES_CONFIG_VERSION=v0.2.7
+++ '[' docker == docker ']'
+++ export EPHEMERAL_CLUSTER=kind
+++ EPHEMERAL_CLUSTER=kind
+++ export KUSTOMIZE_VERSION=v4.1.3
+++ KUSTOMIZE_VERSION=v4.1.3
+++ export KIND_VERSION=v0.11.1
+++ KIND_VERSION=v0.11.1
+++ '[' v1.21.1 == v1.21.2 ']'
+++ export KIND_NODE_IMAGE_VERSION=v1.22.2
+++ KIND_NODE_IMAGE_VERSION=v1.22.2
+++ export MINIKUBE_VERSION=v1.23.2
+++ MINIKUBE_VERSION=v1.23.2
+++ export ANSIBLE_VERSION=4.8.0
+++ ANSIBLE_VERSION=4.8.0
+++ SKIP_RETRIES=false
+++ TEST_TIME_INTERVAL=10
+++ TEST_MAX_TIME=240
+++ FAILS=0
+++ RESULT_STR=
+++ export ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
+++ ANSIBLE_DISPLAY_SKIPPED_HOSTS=no
+++ '[' 4 -lt 4 ']'
+++ export LIBVIRT_DEFAULT_URI=qemu:///system
+++ LIBVIRT_DEFAULT_URI=qemu:///system
+++ '[' capm3 '!=' root ']'
+++ '[' /run/user/1000 == /run/user/0 ']'
+++ sudo -n uptime
+++ export USE_FIREWALLD=False
+++ USE_FIREWALLD=False
+++ [[ ubuntu20 == \r\h\e\l\8 ]]
+++ [[ ubuntu20 == \c\e\n\t\o\s\8 ]]
++++ df / --output=fstype
++++ tail -n 1
+++ FSTYPE=ext4
+++ case ${FSTYPE} in
+++ '[' '!' -d /opt/metal3-dev-env ']'
++ source lib/network.sh
+++ export CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
+++ CLUSTER_PROVISIONING_INTERFACE=ironicendpoint
+++ export POD_CIDR=192.168.0.0/18
+++ POD_CIDR=192.168.0.0/18
+++ PROVISIONING_IPV6=false
+++ IPV6_ADDR_PREFIX=fd2e:6f44:5dd8:b856
+++ [[ false == \t\r\u\e ]]
+++ export BOOT_MODE=legacy
+++ BOOT_MODE=legacy
+++ export PROVISIONING_NETWORK=172.22.0.0/24
+++ PROVISIONING_NETWORK=172.22.0.0/24
+++ [[ legacy == \l\e\g\a\c\y ]]
+++ export LIBVIRT_FIRMWARE=bios
+++ LIBVIRT_FIRMWARE=bios
+++ export LIBVIRT_SECURE_BOOT=false
+++ LIBVIRT_SECURE_BOOT=false
+++ prefixlen PROVISIONING_CIDR 172.22.0.0/24
+++ resultvar=PROVISIONING_CIDR
+++ network=172.22.0.0/24
++++ python -c 'import ipaddress; print(ipaddress.ip_network(u"172.22.0.0/24").prefixlen)'
+++ result=24
+++ eval PROVISIONING_CIDR=24
++++ PROVISIONING_CIDR=24
+++ export PROVISIONING_CIDR
+++ export PROVISIONING_CIDR
+++ export PROVISIONING_NETMASK=255.255.255.0
+++ PROVISIONING_NETMASK=255.255.255.0
+++ network_address PROVISIONING_IP 172.22.0.0/24 1
+++ resultvar=PROVISIONING_IP
+++ network=172.22.0.0/24
+++ record=1
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 1 - 1, None)))'
+++ result=172.22.0.1
+++ eval PROVISIONING_IP=172.22.0.1
++++ PROVISIONING_IP=172.22.0.1
+++ export PROVISIONING_IP
+++ network_address CLUSTER_PROVISIONING_IP 172.22.0.0/24 2
+++ resultvar=CLUSTER_PROVISIONING_IP
+++ network=172.22.0.0/24
+++ record=2
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 2 - 1, None)))'
+++ result=172.22.0.2
+++ eval CLUSTER_PROVISIONING_IP=172.22.0.2
++++ CLUSTER_PROVISIONING_IP=172.22.0.2
+++ export CLUSTER_PROVISIONING_IP
+++ export PROVISIONING_IP
+++ export CLUSTER_PROVISIONING_IP
+++ [[ 172.22.0.1 == *\:* ]]
+++ export PROVISIONING_URL_HOST=172.22.0.1
+++ PROVISIONING_URL_HOST=172.22.0.1
+++ export CLUSTER_URL_HOST=172.22.0.2
+++ CLUSTER_URL_HOST=172.22.0.2
+++ [[ 192.168.111.249 == *\:* ]]
+++ export CLUSTER_APIENDPOINT_HOST=192.168.111.249
+++ CLUSTER_APIENDPOINT_HOST=192.168.111.249
+++ export CLUSTER_APIENDPOINT_PORT=6443
+++ CLUSTER_APIENDPOINT_PORT=6443
+++ network_address dhcp_range_start 172.22.0.0/24 10
+++ resultvar=dhcp_range_start
+++ network=172.22.0.0/24
+++ record=10
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 10 - 1, None)))'
+++ result=172.22.0.10
+++ eval dhcp_range_start=172.22.0.10
++++ dhcp_range_start=172.22.0.10
+++ export dhcp_range_start
+++ network_address dhcp_range_end 172.22.0.0/24 100
+++ resultvar=dhcp_range_end
+++ network=172.22.0.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
+++ result=172.22.0.100
+++ eval dhcp_range_end=172.22.0.100
++++ dhcp_range_end=172.22.0.100
+++ export dhcp_range_end
+++ network_address PROVISIONING_POOL_RANGE_START 172.22.0.0/24 100
+++ resultvar=PROVISIONING_POOL_RANGE_START
+++ network=172.22.0.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 100 - 1, None)))'
+++ result=172.22.0.100
+++ eval PROVISIONING_POOL_RANGE_START=172.22.0.100
++++ PROVISIONING_POOL_RANGE_START=172.22.0.100
+++ export PROVISIONING_POOL_RANGE_START
+++ network_address PROVISIONING_POOL_RANGE_END 172.22.0.0/24 200
+++ resultvar=PROVISIONING_POOL_RANGE_END
+++ network=172.22.0.0/24
+++ record=200
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 200 - 1, None)))'
+++ result=172.22.0.200
+++ eval PROVISIONING_POOL_RANGE_END=172.22.0.200
++++ PROVISIONING_POOL_RANGE_END=172.22.0.200
+++ export PROVISIONING_POOL_RANGE_END
+++ export PROVISIONING_POOL_RANGE_START
+++ export PROVISIONING_POOL_RANGE_END
+++ export CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+++ CLUSTER_DHCP_RANGE=172.22.0.10,172.22.0.100
+++ EXTERNAL_SUBNET=
+++ [[ -n '' ]]
+++ export IP_STACK=v4
+++ IP_STACK=v4
+++ [[ v4 == \v\4 ]]
+++ export EXTERNAL_SUBNET_V4=192.168.111.0/24
+++ EXTERNAL_SUBNET_V4=192.168.111.0/24
+++ export EXTERNAL_SUBNET_V6=
+++ EXTERNAL_SUBNET_V6=
+++ [[ kind == \m\i\n\i\k\u\b\e ]]
+++ [[ -n 192.168.111.0/24 ]]
+++ prefixlen EXTERNAL_SUBNET_V4_PREFIX 192.168.111.0/24
+++ resultvar=EXTERNAL_SUBNET_V4_PREFIX
+++ network=192.168.111.0/24
++++ python -c 'import ipaddress; print(ipaddress.ip_network(u"192.168.111.0/24").prefixlen)'
+++ result=24
+++ eval EXTERNAL_SUBNET_V4_PREFIX=24
++++ EXTERNAL_SUBNET_V4_PREFIX=24
+++ export EXTERNAL_SUBNET_V4_PREFIX
+++ export EXTERNAL_SUBNET_V4_PREFIX
+++ [[ -z 192.168.111.1 ]]
+++ network_address VIRSH_DHCP_V4_START 192.168.111.0/24 20
+++ resultvar=VIRSH_DHCP_V4_START
+++ network=192.168.111.0/24
+++ record=20
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 20 - 1, None)))'
+++ result=192.168.111.20
+++ eval VIRSH_DHCP_V4_START=192.168.111.20
++++ VIRSH_DHCP_V4_START=192.168.111.20
+++ export VIRSH_DHCP_V4_START
+++ network_address VIRSH_DHCP_V4_END 192.168.111.0/24 60
+++ resultvar=VIRSH_DHCP_V4_END
+++ network=192.168.111.0/24
+++ record=60
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 60 - 1, None)))'
+++ result=192.168.111.60
+++ eval VIRSH_DHCP_V4_END=192.168.111.60
++++ VIRSH_DHCP_V4_END=192.168.111.60
+++ export VIRSH_DHCP_V4_END
+++ network_address BAREMETALV4_POOL_RANGE_START 192.168.111.0/24 100
+++ resultvar=BAREMETALV4_POOL_RANGE_START
+++ network=192.168.111.0/24
+++ record=100
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 100 - 1, None)))'
+++ result=192.168.111.100
+++ eval BAREMETALV4_POOL_RANGE_START=192.168.111.100
++++ BAREMETALV4_POOL_RANGE_START=192.168.111.100
+++ export BAREMETALV4_POOL_RANGE_START
+++ network_address BAREMETALV4_POOL_RANGE_END 192.168.111.0/24 200
+++ resultvar=BAREMETALV4_POOL_RANGE_END
+++ network=192.168.111.0/24
+++ record=200
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"192.168.111.0/24").hosts(), 200 - 1, None)))'
+++ result=192.168.111.200
+++ eval BAREMETALV4_POOL_RANGE_END=192.168.111.200
++++ BAREMETALV4_POOL_RANGE_END=192.168.111.200
+++ export BAREMETALV4_POOL_RANGE_END
+++ export VIRSH_DHCP_V4_START
+++ export VIRSH_DHCP_V4_END
+++ export BAREMETALV4_POOL_RANGE_START
+++ export BAREMETALV4_POOL_RANGE_END
+++ [[ -n '' ]]
+++ export EXTERNAL_SUBNET_V6_HOST=
+++ EXTERNAL_SUBNET_V6_HOST=
+++ export EXTERNAL_SUBNET_V6_PREFIX=
+++ EXTERNAL_SUBNET_V6_PREFIX=
+++ export BAREMETALV6_POOL_RANGE_START=
+++ BAREMETALV6_POOL_RANGE_START=
+++ export BAREMETALV6_POOL_RANGE_END=
+++ BAREMETALV6_POOL_RANGE_END=
+++ export REGISTRY_PORT=5000
+++ REGISTRY_PORT=5000
+++ export HTTP_PORT=6180
+++ HTTP_PORT=6180
+++ export IRONIC_INSPECTOR_PORT=5050
+++ IRONIC_INSPECTOR_PORT=5050
+++ export IRONIC_API_PORT=6385
+++ IRONIC_API_PORT=6385
+++ [[ -n 192.168.111.1 ]]
+++ export REGISTRY=192.168.111.1:5000
+++ REGISTRY=192.168.111.1:5000
+++ network_address INITIAL_IRONICBRIDGE_IP 172.22.0.0/24 9
+++ resultvar=INITIAL_IRONICBRIDGE_IP
+++ network=172.22.0.0/24
+++ record=9
++++ python -c 'import ipaddress; import itertools; print(next(itertools.islice(ipaddress.ip_network(u"172.22.0.0/24").hosts(), 9 - 1, None)))'
+++ result=172.22.0.9
+++ eval INITIAL_IRONICBRIDGE_IP=172.22.0.9
++++ INITIAL_IRONICBRIDGE_IP=172.22.0.9
+++ export INITIAL_IRONICBRIDGE_IP
+++ export DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+++ DEPLOY_KERNEL_URL=http://172.22.0.2:6180/images/ironic-python-agent.kernel
+++ export DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+++ DEPLOY_RAMDISK_URL=http://172.22.0.2:6180/images/ironic-python-agent.initramfs
+++ '[' true == true ']'
+++ export IRONIC_URL=https://172.22.0.2:6385/v1/
+++ IRONIC_URL=https://172.22.0.2:6385/v1/
+++ export IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
+++ IRONIC_INSPECTOR_URL=https://172.22.0.2:5050/v1/
++ '[' y == y ']'
++ sudo ip link add ironicendpoint type veth peer name ironic-peer
++ sudo brctl addbr provisioning
++ sudo ip link set provisioning up
++ [[ false == \t\r\u\e ]]
++ sudo ip addr add dev ironicendpoint 172.22.0.1/24
++ sudo brctl addif provisioning ironic-peer
++ sudo ip link set ironicendpoint up
++ sudo ip link set ironic-peer up
++ '[' '' ']'
++ '[' y == y ']'
+++ ip a show baremetal
++ [[ -n 41: baremetal: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state DOWN group default qlen 1000
    link/ether 52:54:00:1a:a3:8b brd ff:ff:ff:ff:ff:ff
    inet 192.168.111.1/24 brd 192.168.111.255 scope global baremetal
       valid_lft forever preferred_lft forever ]]
++ '[' '' ']'
++ '[' y == y ']'
++ sudo virsh net-destroy baremetal
Network baremetal destroyed

++ sudo virsh net-start baremetal
Network baremetal started

++ '[' '' ']'
+ source disable_apparmor_driver_libvirtd.sh
++ selinux='#security_driver = "selinux"'
++ apparmor='security_driver = "apparmor"'
++ none='security_driver = "none"'
++ sudo sed -i 's/#security_driver = "selinux"/security_driver = "none"/g' /etc/libvirt/qemu.conf
++ sudo sed -i 's/security_driver = "apparmor"/security_driver = "none"/g' /etc/libvirt/qemu.conf
++ sudo systemctl restart libvirtd
+ ANSIBLE_FORCE_COLOR=true
+ ansible-playbook -e '{use_firewalld: False}' -e 'external_subnet_v4: 192.168.111.0/24' -i vm-setup/inventory.ini -b vm-setup/firewall.yml
[0;35m[DEPRECATION WARNING]: [defaults]callback_whitelist option, normalizing names [0m
[0;35mto new standard, use callbacks_enabled instead. This feature will be removed [0m
[0;35mfrom ansible-core in version 2.15. Deprecation warnings can be disabled by [0m
[0;35msetting deprecation_warnings=False in ansible.cfg.[0m

PLAY [Setup dummy baremetal VMs] ***********************************************
Wednesday 10 November 2021  22:43:14 +0000 (0:00:00.035)       0:00:00.035 **** 

TASK [Gathering Facts] *********************************************************
[0;32mok: [localhost][0m
Wednesday 10 November 2021  22:43:15 +0000 (0:00:01.448)       0:00:01.483 **** 
Wednesday 10 November 2021  22:43:15 +0000 (0:00:00.042)       0:00:01.526 **** 

TASK [firewall : iptables] *****************************************************
[0;36mincluded: /home/capm3/projects/metal3-dev-env/vm-setup/roles/firewall/tasks/iptables.yaml for localhost[0m
Wednesday 10 November 2021  22:43:15 +0000 (0:00:00.059)       0:00:01.585 **** 

TASK [firewall : iptables: Firewalld service stopped] **************************
[1;30mtask path: /home/capm3/projects/metal3-dev-env/vm-setup/roles/firewall/tasks/iptables.yaml:1[0m
[0;31mfatal: [localhost]: FAILED! => {"changed": false, "msg": "Could not find the requested service firewalld: host"}[0m
[0;36m...ignoring[0m
Wednesday 10 November 2021  22:43:16 +0000 (0:00:00.646)       0:00:02.232 **** 

TASK [firewall : iptables: VBMC Ports] *****************************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:16 +0000 (0:00:00.406)       0:00:02.639 **** 

TASK [firewall : iptables: sushy Port] *****************************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:16 +0000 (0:00:00.267)       0:00:02.906 **** 

TASK [firewall : iptables: Established and related] ****************************
[0;33mchanged: [localhost][0m
Wednesday 10 November 2021  22:43:17 +0000 (0:00:00.276)       0:00:03.183 **** 

TASK [firewall : iptables: Ironic Ports] ***************************************
[0;33mchanged: [localhost] => (item=6180)[0m
[0;33mchanged: [localhost] => (item=5050)[0m
[0;33mchanged: [localhost] => (item=6385)[0m
[0;33mchanged: [localhost] => (item=9999)[0m
[0;33mchanged: [localhost] => (item=80)[0m
Wednesday 10 November 2021  22:43:18 +0000 (0:00:01.282)       0:00:04.465 **** 

TASK [firewall : iptables: Provisioning host ports] ****************************
[0;33mchanged: [localhost] => (item=80)[0m
[0;33mchanged: [localhost] => (item=5000)[0m
[0;33mchanged: [localhost] => (item=53)[0m
Wednesday 10 November 2021  22:43:19 +0000 (0:00:00.763)       0:00:05.229 **** 

TASK [firewall : iptables: PXE Ports] ******************************************
[0;33mchanged: [localhost] => (item=5353)[0m
[0;33mchanged: [localhost] => (item=67)[0m
[0;33mchanged: [localhost] => (item=68)[0m
[0;33mchanged: [localhost] => (item=546)[0m
[0;33mchanged: [localhost] => (item=547)[0m
[0;33mchanged: [localhost] => (item=69)[0m
Wednesday 10 November 2021  22:43:20 +0000 (0:00:01.438)       0:00:06.667 **** 

TASK [firewall : iptables: Ironic Endpoint Keepalived] *************************
[0;33mchanged: [localhost] => (item=112)[0m
[0;33mchanged: [localhost] => (item=icmp)[0m
Wednesday 10 November 2021  22:43:21 +0000 (0:00:00.559)       0:00:07.227 **** 

TASK [firewall : iptables: Allow access to baremetal network from kind] ********
[0;33mchanged: [localhost][0m

PLAY RECAP *********************************************************************
[0;33mlocalhost[0m                  : [0;32mok=11  [0m [0;33mchanged=8   [0m unreachable=0    failed=0    [0;36mskipped=1   [0m rescued=0    [1;35mignored=1   [0m

Wednesday 10 November 2021  22:43:21 +0000 (0:00:00.313)       0:00:07.540 **** 
=============================================================================== 
Gathering Facts --------------------------------------------------------- 1.45s
firewall : iptables: PXE Ports ------------------------------------------ 1.44s
firewall : iptables: Ironic Ports --------------------------------------- 1.28s
+ '[' False == True ']'
+ '[' '' ']'
++ sudo docker inspect registry --format '{{.State.Status}}'
+ reg_state=exited
+ [[ exited == \e\x\i\t\e\d ]]
+ sudo docker start registry
registry
+ sleep 5
++ env
++ grep -v _LOCAL_IMAGE=
++ grep _IMAGE=
++ grep -o '^[^=]*'
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=registry:2.7.1
+ IMAGE_NAME=registry:2.7.1
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/registry:2.7.1
+ sudo docker tag registry:2.7.1 192.168.111.1:5000/localimages/registry:2.7.1
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/registry:2.7.1
The push refers to repository [192.168.111.1:5000/localimages/registry]
6da1e15d5d7f: Preparing
d385a2515a0f: Preparing
d661c8a70d1e: Preparing
02ada6f7a843: Preparing
39982b2a789a: Preparing
d385a2515a0f: Layer already exists
d661c8a70d1e: Layer already exists
6da1e15d5d7f: Layer already exists
02ada6f7a843: Layer already exists
39982b2a789a: Layer already exists
2.7.1: digest: sha256:b0b8dd398630cbb819d9a9c2fbd50561370856874b5d5d935be2e0af07c0ff4c size: 1363
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/baremetal-operator
+ IMAGE_NAME=baremetal-operator
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/baremetal-operator
+ sudo docker tag quay.io/metal3-io/baremetal-operator 192.168.111.1:5000/localimages/baremetal-operator
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/baremetal-operator
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/baremetal-operator]
996e919e8270: Preparing
0b3d0512394d: Preparing
6d75f23be3dd: Preparing
996e919e8270: Layer already exists
0b3d0512394d: Layer already exists
6d75f23be3dd: Layer already exists
latest: digest: sha256:d1323f075c3f519da71d744f1e0f6c9d5c4b55d971361639442fb0d145fce6ae size: 950
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-client
+ IMAGE_NAME=ironic-client
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-client
+ sudo docker tag quay.io/metal3-io/ironic-client 192.168.111.1:5000/localimages/ironic-client
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic-client
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic-client]
297a1a71e453: Preparing
1591f6d87eef: Preparing
2653d992f4ef: Preparing
2653d992f4ef: Layer already exists
1591f6d87eef: Layer already exists
297a1a71e453: Layer already exists
latest: digest: sha256:0abe9a3de15449f9cb7c8ec3daa5dceaf124c2e1705c51ef73dfc54f92dacec4 size: 948
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic
+ IMAGE_NAME=ironic
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic
+ sudo docker tag quay.io/metal3-io/ironic 192.168.111.1:5000/localimages/ironic
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic]
9f9b0a238484: Preparing
294f636c5f28: Preparing
868b6506fd62: Preparing
8ce05429fcf5: Preparing
4e849da23513: Preparing
9eeef8c842f0: Preparing
9f52c6f111be: Preparing
a6e3f33d3978: Preparing
953d7a124685: Preparing
550ce4d22da2: Preparing
b3a2e4c023a3: Preparing
0b15ddafdd3b: Preparing
b63f83aa0e18: Preparing
8d4d55d47f62: Preparing
ce2c12cad2fc: Preparing
58334e793c2c: Preparing
69980d38ee13: Preparing
784f11623567: Preparing
95c51ae9ba96: Preparing
e7a4bda8f16d: Preparing
525ed45dbdb1: Preparing
5bc03dec6239: Preparing
9eeef8c842f0: Waiting
9f52c6f111be: Waiting
8d4d55d47f62: Waiting
ce2c12cad2fc: Waiting
95c51ae9ba96: Waiting
b3a2e4c023a3: Waiting
a6e3f33d3978: Waiting
e7a4bda8f16d: Waiting
525ed45dbdb1: Waiting
953d7a124685: Waiting
5bc03dec6239: Waiting
0b15ddafdd3b: Waiting
550ce4d22da2: Waiting
b63f83aa0e18: Waiting
58334e793c2c: Waiting
784f11623567: Waiting
69980d38ee13: Waiting
868b6506fd62: Layer already exists
9f9b0a238484: Layer already exists
4e849da23513: Layer already exists
294f636c5f28: Layer already exists
8ce05429fcf5: Layer already exists
9eeef8c842f0: Layer already exists
953d7a124685: Layer already exists
9f52c6f111be: Layer already exists
a6e3f33d3978: Layer already exists
550ce4d22da2: Layer already exists
b3a2e4c023a3: Layer already exists
0b15ddafdd3b: Layer already exists
b63f83aa0e18: Layer already exists
8d4d55d47f62: Layer already exists
ce2c12cad2fc: Layer already exists
58334e793c2c: Layer already exists
69980d38ee13: Layer already exists
95c51ae9ba96: Layer already exists
e7a4bda8f16d: Layer already exists
784f11623567: Layer already exists
525ed45dbdb1: Layer already exists
5bc03dec6239: Layer already exists
latest: digest: sha256:b0f388457682993d6ef203dc5f77ef3f6effc5eef2afd4aff3b817970881f911 size: 4910
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ironic-ipa-downloader
+ IMAGE_NAME=ironic-ipa-downloader
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ironic-ipa-downloader
+ sudo docker tag quay.io/metal3-io/ironic-ipa-downloader 192.168.111.1:5000/localimages/ironic-ipa-downloader
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ironic-ipa-downloader
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/ironic-ipa-downloader]
ec89db9873bd: Preparing
17be93712683: Preparing
2653d992f4ef: Preparing
ec89db9873bd: Layer already exists
2653d992f4ef: Layer already exists
17be93712683: Layer already exists
latest: digest: sha256:d2d871675b629bf66514ccda2e2616c50670f7fff9d95b983a216f3a7fdaa1aa size: 948
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/sushy-tools
+ IMAGE_NAME=sushy-tools
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/sushy-tools
+ sudo docker tag quay.io/metal3-io/sushy-tools 192.168.111.1:5000/localimages/sushy-tools
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/sushy-tools
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/sushy-tools]
877458e4c06b: Preparing
f77bec5d5162: Preparing
7b656b8058c4: Preparing
02a38a00d553: Preparing
7fcd2600f5ad: Preparing
8f56c3340629: Preparing
ba6e5ff31f23: Preparing
9f9f651e9303: Preparing
0b3c02b5d746: Preparing
62a747bf1719: Preparing
0b3c02b5d746: Waiting
8f56c3340629: Waiting
62a747bf1719: Waiting
ba6e5ff31f23: Waiting
9f9f651e9303: Waiting
7fcd2600f5ad: Layer already exists
877458e4c06b: Layer already exists
7b656b8058c4: Layer already exists
02a38a00d553: Layer already exists
f77bec5d5162: Layer already exists
0b3c02b5d746: Layer already exists
9f9f651e9303: Layer already exists
62a747bf1719: Layer already exists
8f56c3340629: Layer already exists
ba6e5ff31f23: Layer already exists
latest: digest: sha256:03a9f79dcab145cb5f550a65068a38c303e0decf226400775f30bcd48a734315 size: 2430
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/keepalived
+ IMAGE_NAME=keepalived
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/keepalived
+ sudo docker tag quay.io/metal3-io/keepalived 192.168.111.1:5000/localimages/keepalived
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/keepalived
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/keepalived]
366f541fef14: Preparing
c94b2040d772: Preparing
309a780d3dd8: Preparing
9f54eef41275: Preparing
9f54eef41275: Layer already exists
c94b2040d772: Layer already exists
309a780d3dd8: Layer already exists
366f541fef14: Layer already exists
latest: digest: sha256:4d2d44db445e898a08b072a29af18c325f92a06508b720ea9c95ecddc09c942c size: 1155
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/vbmc
+ IMAGE_NAME=vbmc
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/vbmc
+ sudo docker tag quay.io/metal3-io/vbmc 192.168.111.1:5000/localimages/vbmc
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/vbmc
Using default tag: latest
The push refers to repository [192.168.111.1:5000/localimages/vbmc]
091a214fa651: Preparing
95c51ae9ba96: Preparing
e7a4bda8f16d: Preparing
525ed45dbdb1: Preparing
5bc03dec6239: Preparing
95c51ae9ba96: Layer already exists
e7a4bda8f16d: Layer already exists
5bc03dec6239: Layer already exists
525ed45dbdb1: Layer already exists
091a214fa651: Layer already exists
latest: digest: sha256:859a7038a299476b7f0c402869b34c1be58262c0bfbfdb6dda6c062c0af63b36 size: 1373
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/ip-address-manager:release-0.1
+ IMAGE_NAME=ip-address-manager:release-0.1
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/ip-address-manager:release-0.1
+ sudo docker tag quay.io/metal3-io/ip-address-manager:release-0.1 192.168.111.1:5000/localimages/ip-address-manager:release-0.1
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/ip-address-manager:release-0.1
The push refers to repository [192.168.111.1:5000/localimages/ip-address-manager]
9373af21b8a1: Preparing
6d75f23be3dd: Preparing
9373af21b8a1: Layer already exists
6d75f23be3dd: Layer already exists
release-0.1: digest: sha256:2e29f8aca13b7641fa967187a68162e5f0cb8787a9ed08ea999f12d8a907940b size: 739
+ for IMAGE_VAR in $(env | grep -v "_LOCAL_IMAGE=" | grep "_IMAGE=" | grep -o "^[^=]*")
+ IMAGE=quay.io/metal3-io/cluster-api-provider-metal3:release-0.5
+ IMAGE_NAME=cluster-api-provider-metal3:release-0.5
+ LOCAL_IMAGE=192.168.111.1:5000/localimages/cluster-api-provider-metal3:release-0.5
+ sudo docker tag quay.io/metal3-io/cluster-api-provider-metal3:release-0.5 192.168.111.1:5000/localimages/cluster-api-provider-metal3:release-0.5
+ [[ docker == \p\o\d\m\a\n ]]
+ sudo docker push 192.168.111.1:5000/localimages/cluster-api-provider-metal3:release-0.5
The push refers to repository [192.168.111.1:5000/localimages/cluster-api-provider-metal3]
38151a4c0c44: Preparing
6d75f23be3dd: Preparing
6d75f23be3dd: Layer already exists
38151a4c0c44: Layer already exists
release-0.5: digest: sha256:73ea2421ab36557f3e942ed9e47044f253ee585931f2f3e9b025a56f9d33d213 size: 739
++ env
++ grep _LOCAL_IMAGE=
++ grep -o '^[^=]*'
+ IRONIC_IMAGE=quay.io/metal3-io/ironic
+ VBMC_IMAGE=quay.io/metal3-io/vbmc
+ SUSHY_TOOLS_IMAGE=quay.io/metal3-io/sushy-tools
+ [[ ubuntu == ubuntu ]]
+ sudo docker run -d --net host --privileged --name httpd-infra -v /opt/metal3-dev-env/ironic:/shared --entrypoint /bin/runhttpd --env PROVISIONING_INTERFACE=ironicendpoint quay.io/metal3-io/ironic
e9f9e256abb03188bb72321ae8ea672d81489d2e32e6d79a78d5cbef10a2496a
+ sudo docker run -d --net host --name vbmc -v /opt/metal3-dev-env/virtualbmc/vbmc:/root/.vbmc -v /root/.ssh:/root/ssh quay.io/metal3-io/vbmc
9c63a69b3d07854b2eea7c629e15d05b3364645213930929353ba419352098f2
+ sudo docker run -d --net host --name sushy-tools -v /opt/metal3-dev-env/virtualbmc/sushy-tools:/root/sushy -v /root/.ssh:/root/ssh quay.io/metal3-io/sushy-tools
5c44dfc7144f2dbcb6d0f8ef0bd22ebab3084dd5d00fea7da894e74bdc02c188
+ OPENSTACKCLIENT_PATH=/usr/local/bin/openstack
+ command -v openstack
+ grep -v /usr/local/bin/openstack
+ sudo ln -sf /home/capm3/projects/metal3-dev-env/openstackclient.sh /usr/local/bin/openstack
++ dirname /usr/local/bin/openstack
+ sudo ln -sf /home/capm3/projects/metal3-dev-env/openstackclient.sh /usr/local/bin/baremetal
+ VBMC_PATH=/usr/local/bin/vbmc
+ command -v vbmc
+ grep -v /usr/local/bin/vbmc
+ sudo ln -sf /home/capm3/projects/metal3-dev-env/vbmc.sh /usr/local/bin/vbmc
